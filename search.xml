<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis AOF 持久化学习笔记]]></title>
    <url>%2FRedis-AOF-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[文章 Redis 快照持久化学习笔记 介绍 Redis 快照持久化的功能，除了快照持久化外，Redis 还提供了 AOF（Append Only File）持久化功能。与快照持久化通过直接保存 Redis 的键值对数据不同，AOF 持久化是通过保存 Redis 执行的写命令来记录 Redis 的内存数据。 AOF 持久化的原理理论上说，只要我们保存了所有可能修改 Redis 内存数据的命令（也就是写命令），那么根据这些保存的写命令，我们可以重新恢复 Redis 的内存状态。AOF 持久化正是利用这个原理来实现数据的持久化与数据的恢复的。Redis AOF 持久化原理如图 1 与图 2 所示。 图1：Redis 将写命令保存到 AOF 文件中 图2：Redis 利用 AOF 文件还原内存数据 AOF 配置为了打开 AOF 持久化的功能，我们只需要将 redis.conf 配置文件中的appendonly配置选项设置为yes即可。涉及 AOF 持久化的几个常用配置如下所示： 123appendonly yesappendfilename &quot;appendonly.aof&quot;appendfsync everysec appendonly：是否打开 AOF 持久化功能 appendfilename：AOF 文件名称 appendfsync：同步频率，可选配置如表1所示 appendfsync 持久化选项的取值如表 1 所示。 表 1：appendfsync 选项及同步频率 选项 同步频率 always 每个 Redis 命令都要同步写入硬盘。这样会严重降低 Redis 的性能 everysec 每秒执行一次同步，显式地将多个写命令同步到硬盘 no 让操作系统来决定应该何时进行同步 AOF 文件的生成过程具体包括命令追加，文件写入，文件同步三个步骤。Redis 打开 AOF 持久化功能后，Redis 在执行完一个写命令后，都会将执行的写命令追回到 Redis 内部的缓冲区的末尾。这个过程是命令的追加过程。接下来，缓冲区的写命令会被写入到 AOF 文件，这一过程是文件写入过程。对于操作系统来说，调用write函数并不会立刻将数据写入到硬盘，为了将数据真正写入硬盘，还需要调用fsync函数，调用fsync函数即是文件同步的过程。只有经过文件同步过程，AOF 文件才在硬盘中真正保存了 Redis 的写命令。appendfsync 配置选项正是用来配置将写命令同步到文件的频率的，各个选项的值的含义如表 1 所示。 AOF 文件前面我们提到，AOF 持久化是通过 AOF 文件来实现的，那么 AOF 里面是如何保存 Redis 的写命令的呢？假设我们对一个空的 Redis 数据库执行以下命令： 12set msg helloset site leehao.me 执行后，Redis 数据库保存了键 msg 与 site 的数据。打开保存的 AOF 文件appendonly.aof（AOF 文件的路径和名称由 redis.conf 配置，具体可以参考上面 AOF 配置的描述），可以看到其内容为： 12345678910111213141516171819*2$6SELECT$10*3$3set$3msg$5hello*3$3set$4site$9leehao.me appendonly.aof以 Redis 协议格式 RESP 来保存写命令。由于 RESP 协议中包含了换行符，所以上面展示的 AOF 文件时遇到换行符时进行了换行。在 AOF 文件里面，除了用于指定数据库的 SELECT 命令是自动添加的之外，其他都是我们通过客户端发送的命令。appendonly.aof保存的命令会在 Redis 下次重启时使用来还原 Redis 数据库。 重写 / 压缩 AOF 文件由于 Redis 会不断地将被执行的命令记录到 AOF 文件里面，所以随着 Redis 不断运行，AOF 文件的体积会越来越大。另外，如果 AOF 文件的体积很大，那么还原操作所需要的时间也会非常地长。为了解决 AOF 文件越来越大的问题，用户可以向 Redis 发送 BGREWRITEAOF 命令，这个命令会移除 AOF 文件中冗余的命令来重写 AOF 文件，使 AOF 文件的体积变得尽可能地小。BGREWRITEAOF 的工作原理和快照持久化命令 BGSAVE 的工作原理类似，Redis 会创建一个子进程来负责对 AOF 文件进行重写。值得注意的是，进行 AOF 文件重写时，如果原来的 AOF 文件体积已经非常大，那么重写 AOF 并删除旧 AOF 文件的过程将会对 Redis 的性能造成较大的影响。为此，Redis 提供auto-aof-rewrite-percentage和auto-aof-rewrite-min-size两个配置选项来对 AOF 重写进行配置。auto-aof-rewrite-percentage和auto-aof-rewrite-min-size两个配置选项的含义可以参考 redis.conf 配置中的详细说明，具体来说，auto-aof-rewrite-percentage配置当 AOF 文件需要比旧 AOF 文件增大多少时才进行 AOF 重写，而auto-aof-rewrite-min-size则配置当 AOF 文件需要达到多大体积时才进行 AOF 重写。只有这两个配置的条件都达到时，才会进行 AOF 重写。 AOF 的优点 AOF 持久化的方法提供了多种的同步频率，即使使用默认的同步频率每秒同步一次，Redis 最多也就丢失 1 秒的数据而已。 AOF 文件使用 Redis 命令追加的形式来构造，因此，即使 Redis 只能向 AOF 文件写入命令的片断，使用 redis-check-aof 工具也很容易修正 AOF 文件。 AOF 文件的格式可读性较强，这也为使用者提供了更灵活的处理方式。例如，如果我们不小心错用了 FLUSHALL 命令，在重写还没进行时，我们可以手工将最后的 FLUSHALL 命令去掉，然后再使用 AOF 来恢复数据。 AOF 的缺点 对于具有相同数据的的 Redis，AOF 文件通常会比 RDF 文件体积更大。 虽然 AOF 提供了多种同步的频率，默认情况下，每秒同步一次的频率也具有较高的性能。但在 Redis 的负载较高时，RDB 比 AOF 具好更好的性能保证。 RDB 使用快照的形式来持久化整个 Redis 数据，而 AOF 只是将每次执行的命令追加到 AOF 文件中，因此从理论上说，RDB 比 AOF 方式更健壮。官方文档也指出，AOF 的确也存在一些 BUG，这些 BUG 在 RDB 没有存在。 参考资料 Redis 实战，Josiah L. Carlson 著，黄健宏译，人民邮电出版社，2015年 Redis 设计与实现，黄健宏著，机械工业出版社，2015年 https://redis.io/topics/persistence https://redis.io/topics/protocol]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis - RDB - AOF - 持久化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 快照持久化学习笔记]]></title>
    <url>%2FRedis-%E5%BF%AB%E7%85%A7%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Redis 是一种内存数据库，它将数据存储在内存中，所以如果不将数据保存到硬盘中，那么一旦 Redis 进程退出，保存在内存中的数据将会丢失。为此，Redis 提供了两种不同的持久化方法来将数据存储到硬盘里面。一种方法叫做快照（snapshotting），它可以将存在于某一时刻的所有数据写入硬盘里面。另一种方法叫 AOF（append-only file），它会在执行写命令时，将被执行的写命令复制到硬盘里面。这种两持久化方法既可以同时使用，也可以单独使用，当然如果 Redis 单纯只作为缓存系统使用的话，也可以两种持久化方法都不使用。具体选择哪种持久化方法取决于用户的应用场景。快照持久化方法是 Redis 默认开启的持久化方法，本文介绍快照持久化方法，AOF 方法将在另一篇文章中介绍。 RDB 文件Redis 将某一时刻的快照保存成一种称为 RDB 格式的文件中。RDB 文件是一个经过压缩的二进制文件，通过该文件，Redis 可以将内存中的数据恢复成某一时刻的状态。Redis 与 RDB 文件的关系可以通过下图 1 和图 2 来表示。 图 1：Redis 将内存的数据保存为 RDB 文件 图2：Redis 用 RDB 文件还原内存数据 SAVE 命令Redis 提供了两个命令用来生成 RDB 文件，一个是 SAVE，一个是 BGSAVE。SAVE 命令会阻塞 Redis 服务器进程，走到 RDB 文件创建完毕为止，在 Redis 服务器进程阻塞期间，Redis 不能处理任何命令请求。 12redis 127.0.0.1:6379&gt; saveOK 在生产环境，我们一般不会直接使用 SAVE 命令，原因是由于它会阻塞 Redis 进程。但是，如果机器已没有足够的内存去执行 BGSAVE 命令，或者即使等待持久化操作完毕也无所谓，我们也可以使用 SAVE 命令来生成 RDB 文件。 BGSAVE 命令BGSAVE 命令会派生出一个子进程，然后由子进程创建 RDB 文件，因此，BGSAVE 命令不会阻塞 Redis 服务器进程。 12redis 127.0.0.1:6379&gt; bgsaveBackground saving started 可以使用 LASTSAVE 命令来检查保存 RDB 文件的操作是否执行成功。 自动保存 RDB 文件除了手动执行 SAVE 和 BGSAVE 命令来生成 RDB 文件外，Redis 还提供了自动保存 RDB 文件的功能。由于 BGSAVE 命令可以在不阻塞服务器进程的情况下执行，所以 Redis 允许用户通过设置配置来让 Redis 服务器每隔一段时间自动执行一次 BGSAVE 命令。下面是 Redis 配置文件 redis.conf 中有关自动保存 RDB 文件的有关配置内容： 123save 900 1save 300 10save 60 10000 上面配置的含义是，Redis 服务器只要满足以下三个条件中的任意一个， BGSAVE 命令就会被执行： Redis 服务器在 900 秒之内，对数据库进行了至少一次修改 Redis 服务器在 300 秒之内，对数据库进行了至少 10 次修改 Redis 服务器在 60 秒之内，对数据库进行了至少 10000 次修改 涉及 RDB 文件的配置选项还包括： 12345dbfilename dump.rdbdir ./stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yes dbfilename：配置 RDB 文件名称 dir：配置 RDB 文件存放的路径 stop-writes-on-bgsave-error：当生成 RDB 文件出错时是否继续处理 Redis 写命令，默认为不处理 rdbcompression：是否对 RDB 文件进行压缩 rdbchecksum：是否对 RDB 文件进行校验和校验 快照持久化的优点快照持久化的方法采用创建一个子进程的方法来将 Redis 的内存数据保存到硬盘中，因此，它并不会对 Redis 服务器性能造成多大的影响，这可以说是快照持久化方法最大的一个优点。快照持久化使用的 RDB 文件是一种经过压缩的二进制文件，可以方便地在网络中传输与保存。通过恰当的配置，可以让我们方便快捷地将 Redis 内存数据恢复到某一历史状态。这对于提高数据的安全性，应对服务器宕机等意外的发生很有帮助。 快照持久化的缺点尽管快照持久化允许 Redis 恢复到快照文件的状态，但如果 RDB 文件生成后，Redis 服务器继续处理了写命令导致 Redis 内存数据有更新，这时恰好 Redis 崩溃了而来不及保存新的 RDB 文件，那么 Redis 将会丢失这部分新的数据。也就是说，如果系统真的发生崩溃，那么我们将会丢失最近一次生成 RDB 文件之后更改的所有数据。因此，快照持久化方法只适用于那些即使丢失一部分数据也不会造成问题的应用场景。另外，快照持久化方法需要调用fork()方法创建子进程。当 Redis 内存的数据量较大时，创建子进程和生成 RDB 文件得占用较多的系统资源和处理时间，这会对 Redis 正常处理客户端命令的性能造成较大的影响。当然，如果我们可以妥善处理快照持久化方法可能带来的数据丢失，那么快照持久化仍然是一个不错的选择。 参考资料 Redis 实战，Josiah L. Carlson 著，黄健宏译，人民邮电出版社，2015年 Redis 设计与实现，黄健宏著，机械工业出版社，2015年 https://redis.io/topics/persistence https://redis.io/commands/save https://redis.io/commands/bgsave Redis RDB 文件格式介绍，https://github.com/sripathikrishnan/redis-rdb-tools/wiki/Redis-RDB-Dump-File-Format Redis RDB 版本的历史，https://github.com/sripathikrishnan/redis-rdb-tools/blob/master/docs/RDB_Version_History.textile]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>RDB</tag>
        <tag>AOF</tag>
        <tag>持久化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实基础：Redis 的数据结构介绍]]></title>
    <url>%2F%E5%A4%AF%E5%AE%9E%E5%9F%BA%E7%A1%80%EF%BC%9ARedis-%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[使用 Redis 这么久，发现自己还没写过一篇有关 Redis 数据结构的文章，从构造 Redis 整个知识体系来说，显示是不完整的。故这篇文章再次让自己回归到 Redis 的五种基本数据结构，除了描述这些数据结构的特点，也介绍如何使用 Redis 命令来操作这些数据结构。Redis 支持的数据结构包括： 字符串 列表 集合 有序集合 哈希表 需要指出的是，这些数据结构不是指 Redis 内部实现所采用的数据结构，而是指 Redis 提供给用户使用的数据结构。例如，对于集合这种数据结构，其内部可以使用整数集合（intset）来实现，也可以使用哈希表（hashtable）来实现。一般情况下，对于用户来说，我们并不需要关心集合内部的实现，只需要知道集合这种数据结构的应用场景以及集合为我们提供的操作命令就可以了。接下来，本文依次对 Redis 这些数据结构进行介绍。 字符串字符串是 Redis 最简单的数据结构。熟悉 Memcached 的朋友对于字符串这种数据结构一定感到很亲切，因为 Memcached 也提供了这种数据结构。跟 Memcached 不同的是，Redis 除了提供字符串这种数据结构外，还提供了其他的数据结构，而 Memcached 则只提供字符串数据结构而已。Redis 字符串使用 SDS 来实现。SDS 是 Redis 内部使用的一种数据结构，用作 Redis 字符串的表示。由于 Redis 的键都是字符串，这里我们说字符串是指 Redis 值的数据结构类型，例如客户端执行下面命令： 1SET website leehao.me 那么，Redis 将创建一个新的键值对，其中， 键是一个字符串，字符串内部使用 SDS 来实现，用来保存“website” 值也是一个字符串，字符串内部也是使用 SDS 来实现，用来保存“leehao.me” 下文我们所说的数据结构也是针对键值对的值的数据结构类型来说的，且值的数据结构是什么，我们就称之什么类型的键，例如字符串键，即键值对的值的数据结构为字符串。使用命令 SET 可以设置字符串的值，如果需要取出字符串，可以使用命令 GET。 1GET website 输出： leehao.me 正常情况，客户端发送一个命令，Redis 执行这个命令并将处理结果返回给客户端。如果客户端需要同时设置多个键的值，采用每次只设置一个键值对的方法显然会导致较大的往返时延。Redis 为了解决这个问题引入了 MSET 和 MGET 命令，用来支持一次同时设置或者获取多个键的值。 1MSET website leehao.me name leo sex male 上面的命令同时设置键 website，name，sex 的值分别为 leehao.me，leo 以及 male。MGET 则可以同时获取多个键的值： 1MGET website name sex MGET 的输出为一个列表，其元素为每个键的值： 1) “leehao.me”2) “leo”3) “male” 如果我们需要删除某个键，可以使用命令 DEL： 1DEL name 这样，键 name 以及对应的值 leo 都会从 Redis 中删除。DEL 命令不单可以用在字符串键上，也可以用在其他类型的键上面。 列表Redis 的列表在内部使用链表来实现，这意味着在列表的头部或尾部添加一个元素的时间复杂度都为O(1)。由于列表使用链表来实现，故列表并不支持随机存取元素，如果需要访问列表中指定位置的元素，其时间复杂度为O(n)。 可以使用 LPUSH 或者 RPUSH 命令创建一个列表，LPUSH是指在列表的头部添加一个元素，RPUSH 是指在列表的尾部添加一个元素。例如： 123RPUSH mylist leoRPUSH mylist leeLPUSH mylist first 在列表还没存在时，LPUSH 或者 RPUSH 命令都会创建一个新的列表，并将第一个元素添加进去。可以使用 LRANGE 来查看列表中的元素： 1LRANGE mylist 0 -1 输出： 1) “first”2) “leo”3) “lee” LRANGE 命令的第一个参数为键，第二个参数指定列表的开始位置，第三个参数则指定列表的结束位置。上面的 LRANGE 命令的第三个参数为 -1，表明最后一个元素，故整个命令的含义是指取出列表的所有元素。如果需要删除列表的元素，可以使用 LPOP 或者 RPOP 命令，也可以使用阻塞版本的 BLPOP 或者 BRPOP 命令。 集合Redis 提供集合这种数据结构类型。对于集合来说，它里面的元素都是唯一的，并不存在相同的元素。使用 SADD 可以创建一个 Redis 集合： 1SADD myset leehao leo me 1 这样就创建了一个包含4个元素的集合 myset。可以使用 SMEMBERS 来获取集合中所有的元素： 1SMEMBERS myset 输出： 1) “1”2) “leo”3) “me”4) “leehao” 如果往集合里面添加相同的元素，则这个元素并不会被添加到集合里面： 12SADD myset 1 3SMEMBERS myset 输出： 1) “me”2) “leehao”3) “1”4) “3”5) “leo” 可见，“1” 并不会再次被添加到 myset 集合当中。 使用 SPOP 可以从集合中删除一个或多个元素： 1SPOP myset 2 上面的命令从集合中移除两个元素，且被移除的元素是随机的。 可以使用 SINTER 来获取几个集合的交集，使用 SUNIONSTORE 来获取几个集合的并集。 有序集合有序集合可以看作集合与哈希表的混合体。有序集合里面的元素都是唯一的，这点跟上面我们提到的集合的特点很相似，但跟集合不同的是，有序集合的元素都有一个关联的浮点数值（称为元素的分数），这一点则跟哈希表相似。有序集合之所以称为“有序”是由于集合里面的任意两个元素都可以比较大小，且不存在两个元素的大小相等。Redis 使用以下规则来决定两个有序集合元素的大小： 如果有序集合元素 A 和 B 拥有不同的分数，那么，如果 A.score &gt; B.score，则 A &gt; B 如果有序集合元素 A 和 B 拥有相同的分数，那么，如果 A 键字符串 &gt; B 键字符串，则 A &gt; B 由于有序集合中不存在两个相同的元素，故有序集合中的任意两个元素都不会出现相等的情况。 使用 ZADD 来创建一个有序集合： 1234del leehaoZADD leehao 1 C++ZADD leehao 2 PythonZADD leehao 0 C 我们先把原有的 leehao 键删除，然后构造了一个有序集合 leehao，其中，1，2，0分别为元素 C++，Python，C 的分数。使用命令 ZRANG 可以查看有序集合的元素： 1ZRANGE leehao 0 -1 上面的输出有序集合内所有元素，可以看到，输出的元素已经按元素的分数从小到大进行了排列： 1) “C”2) “C++”3) “Python” 如果需要逆序输出元素，则可以使用命令： 1ZREVRANGE leehao 0 -1 则元素会按元素的分数逆序输出： 1) “Python”2) “C++”3) “C” 如果需要输出元素的分数，则可以在 ZRANGE 命令最后添加上 WITHSCORES： 1ZRANGE leehao 0 -1 WITHSCORES 哈希表如果我们对 Python 的字典或者 C++ 的 std::map 这些数据结构熟悉的话，那么也一定会很快掌握 Redis 的哈希表数据结构，Redis 的哈希表同样是一种映射关系的数据结构。可以使用 HSET 或者 HMSET 来创建或者设置一个哈希表： 1hmset leehao name leo sex male location gz 上面的命令创建了一个哈希表，它的键为 leehao，该哈希表包含3个 域 - 值 对，分别为 name -&gt; leo，sex -&gt; male 以及 location -&gt; gz。 可以使用 HGET 命令获取哈希表某个 域 的值： 1hget leehao name 输出域 name 的值： “leo” 或者使用 HGETALL 命令获取哈希表所有 域 的值： 1hgetall leehao 输出为每个域 - 值对的列表： 1) “name”2) “leo”3) “sex”4) “male”5) “location”6) “gz” 参考资料 https://redis.io/topics/data-types-intro https://redis.io/topics/internals-sds Redis 设计与实现，黄健宏著，机械工业出版社，2015年1月]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
        <tag>集合</tag>
        <tag>列表</tag>
        <tag>有序集合</tag>
        <tag>哈希表</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiredis 实现 Redis 流水线]]></title>
    <url>%2FHiredis-%E5%AE%9E%E7%8E%B0-Redis-%E6%B5%81%E6%B0%B4%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[Pipelining（流水线）允许 Redis 客户端一次向 Redis 发送多个命令，Redis 在接收到这些命令后，按顺序进行处理，然后将请求的处理结果一次性返回给客户端。流水线可以减少客户端与 Redis 之间的网络通信次数来提升 Redis 客户端在发送多个命令时的性能，可谓提升客户端性能的一个利器。我们熟悉的 Python 版本的 Redis 客户端 redis-py 提供了 StrictPipeline 对象来实现流水线，使用起来很是方便，具体用法可以参考文章《Redis 事务学习笔记》。作为 C/C++ 版本的 Redis 客户端，hiredis 实现流水线稍显有点复杂，不过通过使用 hiredis 来实现流水线却可以更深刻了解流水线的内部实现原理。 Hiredis 提供redisCommand()函数来向 Redis 服务端发送命令，redisCommand()函数的原型如下： 1void *redisCommand(redisContext *c, const char *format, ...); redisCommand()执行后，返回一个redisReply *指针，指向redisReply结构体，该结构体包含了返回的结果信息。redisCommand()函数是阻塞的（是指使用阻塞版的redisContext对象，下文我们同样有这个假定），每调用一次，都会等待 Redis 服务端的返回，然后再继续执行程序下面的逻辑。redisCommand()函数的使用示例如下所示，完整的代码和编译可以参考文章《Redis C 语言客户端 hiredis 的使用》。 1234567redisReply *reply;reply = redisCommand(conn, "SET %s %s", "foo", "bar");freeReplyObject(reply);reply = redisCommand(conn, "GET %s", "foo");printf("%s\n", reply-&gt;str);freeReplyObject(reply); 如果我们需要向 Redis 服务端发送多次命令，如果都是使用redisCommand()函数来发送，那么每次发送后都得等待返回结果后才能继续下一次发送，这性能显然不是我们能接受的。Hiredis 提供了redisAppendCommand()函数来实现流水线的命令发送方案。 1int redisAppendCommand(redisContext *c, const char *format, ...); redisAppendCommand()函数执行成功时返回REDIS_OK，失败时返回REDIS_ERR。12#define REDIS_ERR -1#define REDIS_OK 0 跟redisCommand()函数一样，redisAppendCommand()函数在 hiredis 中也有其他变体，这里为了描述的简便，仅以redisCommand()函数为例说明。redisAppendCommand()函数执行后，并没有立刻将命令发送到 Redis 执行，而是先将命令缓存到redisContext对象中。那么，redisContext对象中被缓存起来的命令什么时候会被发送出去呢？Hiredis 提供了redisGetReply()函数来将缓存的命令发送出去的功能。redisGetReply()函数的处理过程如下： 查看结果缓冲区是否还有结果没被取出，如果有，则取出结果后直接返回；如果没有，则执行步骤2 将命令缓冲区的所有命令发送到 Redis 处理，然后一直等待，直到有一个 Redis 的处理结果返回 上面我们提到的redisCommand()函数执行后可以直接获取 Redis 的返回结果，这是由于其内部先调用redisAppendCommand()函数，然后再调用redisGetReply()函数实现的。 说到这里，hiredis 实现流水线的过程就很清晰了。无论redisCommand()函数还是redisAppendCommand()函数，都会先将命令缓存起来，然后再发送到 Redis 执行。不同的是 redisCommand()函数会马上发送命令然后取得返回结果，而redisAppendCommand()函数则在调用redisGetReply()函数才将所有命令一次性发送，并取得第一个命令的返回结果。 下面是使用redisAppendCommand()函数实现流水线方案的示例。 1234567redisReply *reply;redisAppendCommand(context,"SET foo bar");redisAppendCommand(context,"GET foo");redisGetReply(context,&amp;reply); // SET命令的返回freeReplyObject(reply);redisGetReply(context,&amp;reply); // GET命令的返回freeReplyObject(reply); 值得注意的是，调用redisAppendCommand()函数的次数需要与调用redisGetReply()的次数要一致，否则会出现获取的 Redis 处理结果跟预期不一致的情况。 12345// 测试 redisGetReply 与 redisAppendCommand 调用次数不一致的情况redisAppendCommand(conn, "get t");// 本来想取得 set a ddd 的返回，却获取了 get t 的返回reply = redisCommand(conn, "set a ddd");printf("set a res: %s\n", reply-&gt;str); 输出的结果将会是get t命令的返回，而不是set a ddd命令的返回。 参考资料 https://github.com/redis/hiredis https://gist.github.com/dspezia/1893378 http://www.leoox.com/?p=316 http://www.redis.cn/topics/pipelining.html 附：示例程序 testhiredis.c编译： gcc -o testhiredis testhiredis.c -L/usr/local/lib -lhiredis 执行： ./testhiredis 输出： barres: OKres: bwatch res: OKres: OK, num: 0, type: 5res: QUEUED, num: 0, type: 5res: QUEUED, num: 0, type: 5res: QUEUED, num: 0, type: 5res: (null), num: 3, type: 2set a res: tt 源程序：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;stdio.h&gt;#include &lt;hiredis/hiredis.h&gt;int main() &#123; // 阻塞 redisContext redisContext *conn = redisConnect("127.0.0.1", 6379); if (conn != NULL &amp;&amp; conn-&gt;err) &#123; printf("connection error: %s\n", conn-&gt;errstr); return 0; &#125; // 使用 redisCommand 发送命令并获取返回 redisReply *reply; reply = redisCommand(conn, "SET %s %s", "foo", "bar"); freeReplyObject(reply); reply = redisCommand(conn, "GET %s", "foo"); printf("%s\n", reply-&gt;str); freeReplyObject(reply); // 使用 redisAppendCommand 实现流水线 redisAppendCommand(conn, "set a b"); redisAppendCommand(conn,"get a"); int r = redisGetReply(conn, (void **)&amp;reply); if (r == REDIS_ERR) &#123; printf("ERROR\n"); &#125; printf("res: %s\n", reply-&gt;str); freeReplyObject(reply); r = redisGetReply(conn, (void **)&amp;reply); if (r == REDIS_ERR) &#123; printf("ERROR\n"); &#125; printf("res: %s\n", reply-&gt;str); freeReplyObject(reply); // 使用 watch 命令监控键 a reply = redisCommand(conn, "watch a"); printf("watch res: %s\n", reply-&gt;str); freeReplyObject(reply); // 事务流水线，总共5个命令 redisAppendCommand(conn, "multi"); redisAppendCommand(conn, "get foo"); redisAppendCommand(conn, "set t tt"); redisAppendCommand(conn, "set a aa"); redisAppendCommand(conn, "exec"); for (int i = 0; i &lt; 5; ++i) &#123; r = redisGetReply(conn, (void **)&amp;reply); if (r == REDIS_ERR) &#123; printf("ERROR\n"); &#125; printf("res: %s, num: %zu, type: %d\n", reply-&gt;str, reply-&gt;elements, reply-&gt;type); freeReplyObject(reply); &#125; // 测试 redisGetReply 与 redisAppendCommand 调用次数不一致的情况 redisAppendCommand(conn, "get t"); // 本来想取得 set a ddd 的返回，却获取了 get t 的返回 reply = redisCommand(conn, "set a ddd"); printf("set a res: %s\n", reply-&gt;str); redisFree(conn); return 0;&#125;]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>hiredis</tag>
        <tag>pipeline</tag>
        <tag>流水线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 使用 ElementTree 模块来处理 XML]]></title>
    <url>%2FPython-%E4%BD%BF%E7%94%A8-ElementTree-%E6%A8%A1%E5%9D%97%E6%9D%A5%E5%A4%84%E7%90%86-XML%2F</url>
    <content type="text"><![CDATA[最近使用 Python 来发送 SOAP 请求以测试 Web Service 的性能，由于 SOAP 是基于 XML 的，故免不了需要使用 Python 来处理 XML 数据。在对比了几种方案后，最后选定使用 xml.etree.ElementTree 模块来实现。这篇文章记录了使用 xml.etree.ElementTree 模块常用的几个操作，也算是总结一下，免得以后忘记了。 概述对比其他 Python 处理 XML 的方案，xml.etree.ElementTree 模块（下文我们以 ET 来表示）相对来说比较简单，接口也较友好。官方文档 里面对 ET 模块进行了较为详细的描述，总的来说，ET 模块可以归纳为三个部分：ElementTree类，Element类以及一些操作 XML 的函数。XML 可以看成是一种树状结构，ET 使用ElementTree类来表示整个 XML 文档，使用Element类来表示 XML 的一个结点。对整 XML 文档的操作一般是对ElementTree对象进行，而对 XML 结点的操作一般是对Element对象进行。 解析 XML 文件ET 模块支持从一个 XML 文件构造ElementTree对象，例如我们的 XML 文件example.xml内容如下（下文会继续使用这个 XML 文档）： 12345678910111213141516&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;data&gt; &lt;country name="Liechtenstein"&gt; &lt;rank&gt;1&lt;/rank&gt; &lt;year&gt;2008&lt;/year&gt; &lt;gdppc&gt;141100&lt;/gdppc&gt; &lt;neighbor name="Austria" direction="E"/&gt; &lt;neighbor name="Switzerland" direction="W"/&gt; &lt;/country&gt; &lt;country name="Singapore"&gt; &lt;rank&gt;4&lt;/rank&gt; &lt;year&gt;2011&lt;/year&gt; &lt;gdppc&gt;59900&lt;/gdppc&gt; &lt;neighbor name="Malaysia" direction="N"/&gt; &lt;/country&gt;&lt;/data&gt; 可以使用 ET 模块的parse()函数来从指定的 XML 文件构造一个ElementTree对象： 12345678import xml.etree.ElementTree as ET# 获取 XML 文档对象 ElementTreetree = ET.parse('example.xml')# 获取 XML 文档对象的根结点 Elementroot = tree.getroot()# 打印根结点的名称print root.tag 从 XML 文件构造好ElementTree对象后，还可以获取其结点，或者再继续对结点进行进一步的操作。 解析 XML 字符串ET 模块的fromstring()函数提供从 XML 字符串构造一个Element对象的功能。 1234xml_str = ET.tostring(root)print xml_strroot = ET.fromstring(xml_str)print root.tag 接着上面的代码，我们使用 ET 模块的tostring()函数来将上面我们构造的root对象转化为字符串，然后使用fromstring()函数重新构造一个Element对象，并赋值给root变量，这时root代表整个 XML 文档的根结点。 构造 XML如果我们需要构造 XML 文档，可以使用 ET 模块的 Element类以及SubElement()函数。可以使用Element类来生成一个Element对象作为根结点，然后使用ET.SubElement()函数生成子结点。 123456789a = ET.Element('a')b = ET.SubElement(a, 'b')b.text = 'leehao.me'c = ET.SubElement(a, 'c')c.attrib['greeting'] = 'hello'd = ET.SubElement(a, 'd')d.text = 'www.leehao.me'xml_str = ET.tostring(a, encoding='UTF-8')print xml_str 输出： 12&lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;a&gt;&lt;b&gt;leehao.me&lt;/b&gt;&lt;c greeting="hello" /&gt;&lt;d&gt;www.leehao.me&lt;/d&gt;&lt;/a&gt; 如果需要输出到文件中，可以继续使用ElementTree.write()方法来处理： 123# 先构造一个 ElementTree 以便使用其 write 方法tree = ET.ElementTree(a)tree.write('a.xml', encoding='UTF-8') 执行后，便会生成一个 XML 文件a.xml: 12&lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;a&gt;&lt;b&gt;leehao.me&lt;/b&gt;&lt;c greeting="hello" /&gt;&lt;d&gt;www.leehao.me&lt;/d&gt;&lt;/a&gt; XML 结点的查找与更新1. 查找 XML 结点Element类提供了Element.iter()方法来查找指定的结点。Element.iter()会递归查找所有的子结点，以便查找到所有符合条件的结点。 1234567# 获取 XML 文档对象 ElementTreetree = ET.parse('example.xml')# 获取 XML 文档对象的根结点 Elementroot = tree.getroot()# 递归查找所有的 neighbor 子结点for neighbor in root.iter('neighbor'): print neighbor.attrib 输出： 123&#123;&apos;direction&apos;: &apos;E&apos;, &apos;name&apos;: &apos;Austria&apos;&#125;&#123;&apos;direction&apos;: &apos;W&apos;, &apos;name&apos;: &apos;Switzerland&apos;&#125;&#123;&apos;direction&apos;: &apos;N&apos;, &apos;name&apos;: &apos;Malaysia&apos;&#125; 如果使用Element.findall()或者Element.find()方法，则只会从结点的直接子结点中查找，并不会递归查找。 1234for country in root.findall('country'): rank = country.find('rank').text name = country.get('name') print name, rank 输出： 12Liechtenstein 1Singapore 4 2. 更新结点如果需要更新结点的文本，可以通过直接修改Element.text来实现。如果需要更新结点的属性，可以通过直接修改Element.attrib来实现。对结点进行更新后，可以使用ElementTree.write()方法将更新后的 XML 文档写入文件中。 123456789# 获取 XML 文档对象 ElementTreetree = ET.parse('example.xml')# 获取 XML 文档对象的根结点 Elementroot = tree.getroot()for rank in root.iter('rank'): new_rank = int(rank.text) + 1 rank.text = str(new_rank) rank.attrib['updated'] = 'yes'tree.write('output.xml', encoding='UTF-8') 新生成的output.xml文件以下： 12345678910111213141516&lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;data&gt; &lt;country name="Liechtenstein"&gt; &lt;rank updated="yes"&gt;2&lt;/rank&gt; &lt;year&gt;2008&lt;/year&gt; &lt;gdppc&gt;141100&lt;/gdppc&gt; &lt;neighbor direction="E" name="Austria" /&gt; &lt;neighbor direction="W" name="Switzerland" /&gt; &lt;/country&gt; &lt;country name="Singapore"&gt; &lt;rank updated="yes"&gt;5&lt;/rank&gt; &lt;year&gt;2011&lt;/year&gt; &lt;gdppc&gt;59900&lt;/gdppc&gt; &lt;neighbor direction="N" name="Malaysia" /&gt; &lt;/country&gt;&lt;/data&gt; 对比example.xml文件，可以看到output.xml文件已更新。 参考资料 https://docs.python.org/2/library/xml.html#xml-vulnerabilities https://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>ELementTree</tag>
        <tag>XML</tag>
        <tag>Element</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOAP Web Services 简介]]></title>
    <url>%2FSOAP-Web-Services-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[SOAP（Simple Object Access Protocol）是一种基于 XML 的 web 服务协议。SOAP 是平台独立的，不依赖于特定的语言，例如，我们可以使用 C#，C/C++，或者 Java 语言来实现 SOAP web 服务。 SOAP 的优点SOAP 的优点有： WS Security：SOAP 使用 WS Security 作为其安全的标准，安全性较高。 语言与平台独立：可以使用多种语言来实现 SOAP web 服务，且可以运行在多种平台上面。 SOAP 的缺点SOAP 的缺点有： 速度较慢：SOAP 使用 XML 作为数据传输的格式，web 服务每次读取数据时都需要对 XML 进行解析，速度较慢。另外，SOAP 规定了 web 服务需要遵循的许多规范，这导致在传输过程中消耗较多的网络带宽。 依赖于 WSDL：除了使用 WSDL 外，SOAP 并不提供其他的机制来让其他应用程序发现服务。 参考资料 https://www.javatpoint.com/soap-web-services https://en.wikipedia.org/wiki/SOAP https://en.wikipedia.org/wiki/WS-Security]]></content>
      <categories>
        <category>后台</category>
      </categories>
      <tags>
        <tag>SOAP</tag>
        <tag>Web Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 图形化监控方案 RedisLive 介绍]]></title>
    <url>%2FRedis-%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88-RedisLive-%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[作为一款开源的 Redis 图形化监控工具，RedisLive 提供对 Redis 实例的内存使用情况，接收的客户端命令，接收的请求数量以及键进行监控。RedisLive 的工作原理基于 Redis 的 INFO 和 MONITOR 命令，通过向 Redis 实例发送 INFO 和 MONITOR 命令来获取 Redis 实例当前的运行数据。 RedisLive 提供的图形化展示界面如下图所示： 安装RedisLive 使用 Python 实现，使用 Tornado 作为自己的 Web 服务器。运行 RedisLive 并不需要额外的编译过程。下载 RedisLive 的代码后，只须安装好依赖的相关 Python 扩展包就可以直接运行。可以通过 Git 下载最新的 RedisLive 源代码： 1git clone https://github.com/kumarnitin/RedisLive.git 进下下载后的 RedisLive 目录，可以看下 RedisLive 依赖的 Python 扩展包都已经写在 requirements.txt 文件中了。requirements.txt 的内容如下： 1234argparse==1.2.1python-dateutil==1.5redistornado==2.1.1 熟悉 Python 的朋友对于 requirements.txt 文件也一定感觉很亲切了，使用下面的命令可以安装里面的扩展包（指定豆瓣源来安装速度更快）： 1pip install -r requirements.txt -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com 运行安装好依赖后，接下来就可以运行 RedisLive 了。进入 RedisLive/src 目录，可以看到 redis-live.conf.example文件，这个文件是 RedisLive 的示例配置文件，内容如下： 12345678910111213141516171819202122232425262728&#123; "RedisServers": [ &#123; "server": "154.17.59.99", "port" : 6379 &#125;, &#123; "server": "localhost", "port" : 6380, "password" : "some-password" &#125; ], "DataStoreType" : "redis", "RedisStatsServer": &#123; "server" : "ec2-184-72-166-144.compute-1.amazonaws.com", "port" : 6385 &#125;, "SqliteStatsStore" : &#123; "path": "to your sql lite file" &#125;&#125; RedisServers：监控的 Redis 实例列表，RedisLive 支持同时监控多个 Redis 实例 RedisStatsServer：用来存储监控数据的 Redis 实例，此配置不同于 RedisServers，RedisLive 并不监控 RedisStatsServer，RedisStatsServer 只是用作存储监控数据使用 DataStoreType：监控数据的存储方案，可以配置为redis或者sqlite SqliteStatsStore：存储监控数据的 sqlite 配置 我们实例使用的redis-live.conf（需要去除.example后缀）配置如下所示： 12345678910111213141516&#123; "RedisServers": [ &#123; "server": "127.0.0.1", "port" : 6379 &#125; ], "DataStoreType" : "sqlite", "SqliteStatsStore" : &#123; "path": "db/redislive.sqlite" &#125;&#125; 即监控的 Redis 实例为 127.0.0.1:6379 ，使用 sqlite 作用存储监控数据方案，sqlite 数据库路径为db/redislive.sqlite。 配置完成后，便可以将 RedisLive 运行起来。RedisLive 的运行包括两个部分（在 RedisLive/src 目录），redis-monitor.py用于向 Redis 实例发送 INFO 和 MONITOR 命令并获取其返回，redis-live.py 用于运行 Web 服务器。 我们首先启动redis-monitor.py脚本，并将duration参数设置为 120 秒。duration参数指定了监控脚本的运行持续时间，例如设置为 120 秒，即经过 120 秒后，监控脚本会自动退出，并在终端打印 shutting down… 的提示。 1./redis-monitor.py --duration=120 接下来启动 Web 服务器：1./redis-live.py 打开浏览器，在地址栏输入 http://localhost:8888/index.html，按下回车后，便可以看到 Redis 实例的监控数据。 需要指出的是，由于redis-monitor.py脚本采用向 Redis 实例发送 MONITOR 命令和 INFO 命令的方式来取得监控数据，而 MONITOR 命令对于 Redis 实例的性能有较大影响，因此，对于生产环境下的redis-monitor.py的部署，需要设置一个较适宜的duration参数，并使用 crontab 来定时执行该脚本。 参考资料 https://github.com/nkrode/RedisLive http://www.nkrode.com/article/real-time-dashboard-for-redis http://wxmimperio.tk/2016/02/25/Redis-Monitor-Tools/ 深入理解Redis，Jeremy Nelson 著，汪佳南译，电子工业出版社，2017年4月 http://redis.io/commands/monitor]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>RedisLive</tag>
        <tag>Redis监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 事务学习笔记]]></title>
    <url>%2FRedis-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Redis 为了支持事务，提供了 5 个相关的命令，他们分别是 MULTI，EXEC， WATCH，UNWATCH 和 DISCARD。我们先介绍 MULTI 和 EXEC 的用法，MULTI 和 EXEC 支持了 Redis 的基本事务的用法。接下来介绍 WATCH，UNWATCH 和 DISCARD，这3个命令则支持更高级的 Redis 事务的用法。 事务允许一次执行多个命令，并且带有以下两个重要的保证： 事务是一个单独的隔离的操作：事务中的所有命令会按顺序执行，事务在执行过程中，不会被其他客户端发来的命令请求所打断 事务是一个原子的操作：事务中的命令要么全部被执行，要么全部都不执行 Redis 的基本事务Redis 的基本事务需要用到 MULTI 和 EXEC 命令，这种事务可以让一个客户端在不被其他客户端打断的情况下执行多个命令。被 MULTI 和 EXEC 命令包裹的所有命令会一个接一个执行，直到所有命令都执行完毕，当一个事务执行完毕之后，Redis 才会处理其他客户端的命令。 MULTI 命令用于开启一个事务，它总是返回 OK。MULTI 执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当 EXEC 命令被调用时，队列中的所有命令才会被执行。以下是一个基本事务的例子，它原子地增加了 foo 和 bar 两个健的值： redis 127.0.0.1:6379&gt; MULTIOKredis 127.0.0.1:6379&gt; INCR fooQUEUEDredis 127.0.0.1:6379&gt; INCR barQUEUEDredis 127.0.0.1:6379&gt; EXEC1) (integer) 12) (integer) 1 EXEC 命令的返回是一个数组，数组中的每个元素都是执行事务中命令所产生的回复，且返回回复的先后顺序与命令发送的先后顺序一致。当客户端处于事务状态时，所有传入的命令都会返回一个内容为 QUEUED 的状态回复（status reply），这些被入队的命令将在 EXEC 命令被调用时执行。 上面的情况是，客户端的每个命令都会发送到 Redis，Redis 对于每个命令都返回结果给客户端。为了减少 Redis 与客户端之间的通信往返次数，提升执行多个命令时的性能，也可以在客户端先存储起事务包含的多个命令，然后在事务执行时一次性地将所有命令都发送给 Redis。Redis 的 Python 客户端 redis-py 正是这样实现的。 Redis-py 实现事务我们以 redis-py 为例，说明如何实现 Redis 事务。Redis-py 提供了Pipeline对象来实现事务，对 Redis 客户端对象（StrictRedis或者Redis对象）调用pipeline()方法将返回事务型流水线对象Pipeline。Pipeline对象会自动地使用 MULTI 和 EXEC 包裹起用户输入的多个命令，在事务执行时一次性将所有命令发送给 Redis 服务端执行。Pipeline对象还提供非事务型流水线的功能，只需要transaction参数设置为False即可。根据 redis-py 官方文档的说明，Pipeline对象不是线程安全的，不应该在多线程环境中使用同一个Pipeline对象。Pipeline的使用比较简单，以下是一个简单的例子，更详细的说明可以参考 redis-py 的官方文档。 12345678910111213import redisr = redis.Redis(host='127.0.0.1', port=6379)r.set('lee', 'leo')# 使用 pipeline() 方法创建一个事务型流水线对象 Pipelinepipe = r.pipeline()# 以下的 SET 和 GET 命令会被在客户端缓存起来pipe.set('foo', 'bar')pipe.get('lee')# execute() 会将上面缓存起来的所有命令被发送到服务端，# 并返回所有命令回复的列表res = pipe.execute()print(res) 输出为上述两个命令回复的列表： [True, ‘leo’] 更高级的 Redis 事务操作在执行 EXEC 命令之前，Redis 不会执行任何实际的操作，所以用户没办法根据读取到的数据来做决定。Redis 提供了 WATCH 和 UNWATCH 命令来实现更高级的 Redis 事务操作。 使用 WATCH 实现乐观锁WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。被 WATCH 的键会被监视，并且 Redis 会发觉这些键是否被改动过。在用户使用 WATCH 命令对键进行监视之后，直到用户执行 EXEC 命令的这段时间里面，如果有其他客户端抢先对任何被监视的键进行了替换，更新或者删除操作，那么当用户尝试执行 EXEC 命令的时候，事务将失败并返回一个错误（之后用户可以选择重试事务或者放弃事务）。通过使用 WATCH、MULTI/EXEC、UNWATCH/DISCARD 等命令，程序可以在执行某些重要操作的时候，通过确保自己正在使用的数据没有发生变化来避免数据出错。为了说明 WATCH 的作用，假设我们不使用 Redis 的 INCR 命令而去实现原子性地为某个键的值增加 1 的操作。我们最先想到的方案可能如下所示： 123456789import redisr = redis.Redis(host='127.0.0.1', port=6379)val = r.get('mykey')if val: val = int(val) + 1else: val = 1r.set('mykey', val) 首先读出一个键的值，然后对这个值加 1，如果键不存在，则使用默认值 1。上面的实现在只有一个客户端的情况下可以执行得很好，但是，当多个客户端同时对同一个键进行这样的操作时，就会产生竞争条件。例如，如果客户端 A 和 B 都读取了键原来的值，比如 1，那个两个客户端都会将键设置为 2，但正确的结果应该是 3 才对。 接下来我们想到的方案是使用 MULTI 和 EXEC 封装 Redis 来实现对键增加 1 的原子性操作。但是由于增加 1 的操作依赖于前面读取键值命令的结果，所以单纯使用 MULTI 和 EXEC 也没法实现我们想要的结果。 有了 WATCH，我们就可以轻松地解决这类问题了，我们编写的 Python 的代码如下： 1234567891011121314151617181920212223242526import redisr = redis.Redis(host='127.0.0.1', port=6379)with r.pipeline() as pipe: while True: try: # 监视我们需要修改的键 res = pipe.watch('mykey') # 由于调用了 WATCH 命令，接下来的 pipe 对象的命令都会立马 # 发送到 Redis 服务端执行，故我们可以正常拿到执行的结果 val = pipe.get('mykey') print('val: %s' % val) if val: val = int(val) + 1 else: val = 1 # 现在，我们重新将 pipe 对象设置为将命令包裹起来执行的形式 pipe.multi() pipe.set('mykey', val) # 最后，将包裹起来的命令一起发送到 Redis 服务端以事务形式执行 pipe.execute() # 如果没抛 WatchError 异常，说明事务被成功执行 break except redis.WatchError: # 如果其他客户端在我们 WATCH 和 事务执行期间，则重试 continue 我们继续使用 redis-py 中的Pipeline对象来向 Redis 服务端发送命令。读上面的代码大家可能有个疑问：Pipeline对象不是执行execute()时才一次性将所有命令发送到 Redis 服务端执行么，怎么在代码的中间就可以拿到命令执行的结果了？说实话，一开始我也对上面的代码不理解。为了弄清楚个究竟，我们将上面的代码在 PyCharm 执行，利用 PyCharm 的断点功能，可以看到代码的详细执行流程。 我们来分析 redis-py 内部的实现代码。pipe.watch(&#39;mykey&#39;)代码如下所示： 1234def watch(self, *names): "Watches the values at keys ``names``" ... return self.execute_command('WATCH', *names) 由于执行的是 WATCH 命令，接下来的execute_command()方法会调用immediate_execute_command()方法。进入immediate_execute_command()方法查看其代码，可以看到其对于每次调用的命令都会立马发送给 Redis 服务端执行，其获取返回结果。也就是说，通过pipe.watch()的调用，程序可以顺利拿到执行 Redis 命令中间结果的值。 12345def execute_command(self, *args, **kwargs): if (self.watching or args[0] == 'WATCH') and \ not self.explicit_transaction: return self.immediate_execute_command(*args, **kwargs) return self.pipeline_execute_command(*args, **kwargs) 接下来调用pipe.multi()设置self.explicit_transaction为真： 1234567def multi(self): """ Start a transactional block of the pipeline after WATCH commands are issued. End the transactional block with `execute`. """ ... self.explicit_transaction = True 这样 redis-py 会重新使用 MULTI 和 EXEC 来包裹起接下来需要执行的命令，以达到命令以事务形式执行的目的。有兴趣的同学可以通过断点运行的方式来查看程序的接下来的执行过程，也可以结合 redis-py 对于事务型流水线对象Pipeline的介绍来理解，链接：官方文档:)再回到原来前面我们所说的，如果键mykey在 EXEC 执行之前被修改了，那么整个事务就会被取消。如果事务被取消了，接着继续重试的过程，直到事务被正常执行。 取消监视 UNWATCHUNWATCH 命令用来取消对所有键的监视。如果在执行 WATCH 命令之后，EXEC 命令已经执行了的话（无论事务是否成功执行），那么就不需要再执行 UNWATCH 了。这是因为当 EXEC 被调用时，Redis 对所有键的监视都会被取消。另外，由于 DISCARD 命令在取消事务的同时也会取消对所有对键的监视，所以 DISCARD 命令执行以后，也没必要执行 UNWATCH 了。 放弃事务 DISCARD当执行 DISCARD 命令时，事务会被放弃，事务队列会被清空。例如： redis 127.0.0.1:6379&gt; SET foo 1OKredis 127.0.0.1:6379&gt; MULTIOKredis 127.0.0.1:6379&gt; INCR fooQUEUEDredis 127.0.0.1:6379&gt; DISCARDOKredis 127.0.0.1:6379&gt; get foo“1” 上面的例子中，DISCARD 命令执行后，事务被取消，前面的 INCR 并没有生效，’foo’键的值仍然为1。 参考资料 http://redisdoc.com/topic/transaction.html https://www.tutorialspoint.com/redis/redis_transactions.htm Redis 实战，Josiah L. Carlson 著，黄健宏译，人民邮电出版社，2015年 Redis-py的源码，https://github.com/andymccurdy/redis-py]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>pipeline</tag>
        <tag>事务</tag>
        <tag>WATCH</tag>
        <tag>MULTI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[I/O多路复用之 epoll 系统调用]]></title>
    <url>%2FI-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E4%B9%8B-epoll-%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[I/O多路复用除了之前我们提到的select和poll外，epoll 也可以检查多个文件描述符的就绪状态，以达到I/O多路复用的目的。epoll 系统调用是 Linux 系统专有的，在 Linux 内核 2.6 版本新增，epoll 的主要优点有： 当检查大量的文件描述符时，epoll 的性能比select和poll高很多 epoll 既支持水平触发也支持边缘触发，select和poll只支持水平触发 epoll 编程接口的核心数据结构为 epoll 实例，它和一个打开的文件描述符相关联。这个文件描述符是内核数据结构的句柄，该内核数据结构的作用主要有两个： 记录在进程中声明过的感兴趣的文件描述符列表，即 interest list 维护处于I/O就绪状态中文件描述符列表，即 ready list 其中，ready list 是 interest list 的子集。 epoll 编程接口由以下3个系统调用组成： epoll_create创建一个 epoll 实例，返回代码该实例的文件描述符 epoll_ctl增删改 epoll 实例的 interest list epoll_wait返回与 epoll 实例相关联的就绪列表中的成员 创建 epoll 实例: epoll_create系统调用epoll_create创建一个新的 epoll 实例，其对应的 interest list 初始化为空。 12#include &lt;sys/epoll.h&gt;int epoll_create(int size); 参数size指定了我们想要通过 epoll 实例来检查的文件描述符个数，该参数并不是一个上限，而是告诉内核应该如何为内部数据结构划分初始大小。epoll_create返回新创建 epoll 实例的文件描述符，这个文件描述符在其他几个 epoll 系统调用中会被用来表示 epoll 实例。当这个文件描述符不再使用时，应该通过close来关闭。 从 Linux 2.6.27 版内核以来，Linux 支持了一个新的系统调用 epoll_create1。该系统调用执行的任务同epoll_create，但是去掉了无用的参数size，并增加了一个可用来修改系统调用行为的flag标志。 修改 epoll 实例: epoll_ctl系统调用epoll_ctl能够修改由文件描述符epfd所代表的 epoll 实例中的 interest list。 12#include &lt;sys/epoll.h&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *ev); 参数epfd指定 epoll 实例的文件描述符，即对哪个 epoll 实例进行操作 参数fd指明要修改 interest list 中的哪一个文件描述符。 参数op用来指定需要执行的操作，下文我们还会对op操作类型进行进一步描述 参数ev是指向结构体epoll_event的指针，关于结构体epoll_event的定义，我们也在下文描述 epoll_ctl中op支持的操作包括以下以种： EPOLL_CTL_ADD将描述符fd添加到 epoll 实例的 interest list 中去。对于fd上我们感兴趣的事件，在ev所指向的结构体中指定。 EPOLL_CTL_MOD修改描述符fd上设定的事件，需用到由ev所指向的结构体中的信息。 EPOLL_CTL_DEL将描述符fd从 epoll 实例的 interest list 中移除，该操作忽略ev参数。 上面我们多处提到了ev，ev是指向结构体epoll_event的指针，该结构体的定义如下： 1234struct epoll_event &#123; uint32_t events; // epoll 事件 epoll_data data; // 用户数据&#125;; 结构体epoll_event中的data字段的类型为epoll_data，其定义以下： 123456typedef union epoll_data &#123; void *ptr; // 用户自定义数据的指针 int fd; // 文件描述符 uint32_t u32; // 32位整型 uint64_t u64; // 64位整型&#125; epoll_data_t; 参数ev为文件描述符fd所做的设置如下： 结构体epoll_event中的events字段是一个位掩码，它指定了 epoll 实例监控的事件集合 data字段是一个联合体，当fd就绪时，联合体的成员可用来指定传回给调用进程的信息 就绪等待: epoll_wait系统调用epoll_wait返回 epoll 实例中处于就绪状态的文件描述符的信息。单个epoll_wait调用能返回多个就绪态文件描述符的信息，这也正是I/O多路复用的体现。 12#include &lt;sys/epoll.h&gt;int epoll_wait(int epfd, struct epoll_event *evlist, int maxevents, int timeout); 参数evlist所指向的结构体数组中返回就绪状态文件描述符的信息。数据evlist的空间由调用者负责申请，所包含的元素个数在参数maxevents中指定。 参数timeout指定epoll_wait的阻塞行为，例如timeout等于-1，调用将一直阻塞，走到 interest list 中的文件描述符上有事件产生。 epoll_wait 调用成功后，返回数据evlist中的元素个数，即就绪的描述符个数。 例子我们以编写一个 TCP 服务器为例子，说明 epoll 的用法，该 TCP 服务器打印出所有接收到的消息。我们先来看创建和绑定 TCP 监听套接字的函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445static intcreate_and_bind (char *port)&#123; struct addrinfo hints; struct addrinfo *result, *rp; int s, sfd; memset (&amp;hints, 0, sizeof (struct addrinfo)); hints.ai_family = AF_UNSPEC; // 支持 IPv4 和 IPv6 hints.ai_socktype = SOCK_STREAM; // TCP socket hints.ai_flags = AI_PASSIVE; // 监听套接字 s = getaddrinfo (NULL, port, &amp;hints, &amp;result); if (s != 0) &#123; fprintf (stderr, "getaddrinfo: %s\n", gai_strerror (s)); return -1; &#125; for (rp = result; rp != NULL; rp = rp-&gt;ai_next) &#123; sfd = socket (rp-&gt;ai_family, rp-&gt;ai_socktype, rp-&gt;ai_protocol); if (sfd == -1) continue; s = bind (sfd, rp-&gt;ai_addr, rp-&gt;ai_addrlen); if (s == 0) &#123; // 已成功绑定套接字 break; &#125; close (sfd); &#125; if (rp == NULL) &#123; fprintf (stderr, "Could not bind\n"); return -1; &#125; freeaddrinfo (result); return sfd;&#125; create_and_bind接受port参数（表示监听的端口），其作用是创建并绑定监听套接字。getaddrinfo函数既可以用于IPv4，也可以用于IPv6，能够处理名字到地址以及服务到端口这两种转换，它返回addrinfo结构体数组的指针。关于getaddrinfo详细介绍，可以参考《UNIX网络编程》的有关描述。create_and_bind返回结构体addrinfo数组的指针（保存在reslut指针中）接下来，我们对result进行遍历，直到将监听套接字成功绑定为止。 接下来，我们再来看将一个套接字设置为非阻塞套接字的函数。 12345678910111213141516171819202122static intmake_socket_non_blocking (int sfd)&#123; int flags, s; flags = fcntl (sfd, F_GETFL, 0); if (flags == -1) &#123; perror ("fcntl"); return -1; &#125; flags |= O_NONBLOCK; s = fcntl (sfd, F_SETFL, flags); if (s == -1) &#123; perror ("fcntl"); return -1; &#125; return 0;&#125; 最后我们来看下main函数的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179intmain (int argc, char *argv[])&#123; int sfd, s; int efd; struct epoll_event event; struct epoll_event *events; if (argc != 2) &#123; fprintf (stderr, "Usage: %s [port]\n", argv[0]); exit (EXIT_FAILURE); &#125; sfd = create_and_bind (argv[1]); if (sfd == -1) abort (); s = make_socket_non_blocking (sfd); if (s == -1) abort (); s = listen (sfd, SOMAXCONN); if (s == -1) &#123; perror ("listen"); abort (); &#125; efd = epoll_create1 (0); if (efd == -1) &#123; perror ("epoll_create"); abort (); &#125; event.data.fd = sfd; // ET 模式 event.events = EPOLLIN | EPOLLET; s = epoll_ctl (efd, EPOLL_CTL_ADD, sfd, &amp;event); if (s == -1) &#123; perror ("epoll_ctl"); abort (); &#125; // 用来存储epoll_wait返回的就绪文件描述符列表 events = calloc (MAXEVENTS, sizeof event); // 主循环 while (1) &#123; int n, i; n = epoll_wait (efd, events, MAXEVENTS, -1); for (i = 0; i &lt; n; i++) &#123; if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) || (!(events[i].events &amp; EPOLLIN))) &#123; // 监测的文件描述符出错了 fprintf (stderr, "epoll error\n"); close (events[i].data.fd); continue; &#125; else if (sfd == events[i].data.fd) &#123; // 监听套接字就绪，表明有一个或者多个连接进来 while (1) &#123; struct sockaddr in_addr; socklen_t in_len; int infd; char hbuf[NI_MAXHOST], sbuf[NI_MAXSERV]; in_len = sizeof in_addr; infd = accept (sfd, &amp;in_addr, &amp;in_len); if (infd == -1) &#123; if ((errno == EAGAIN) || (errno == EWOULDBLOCK)) &#123; // 处理完所有的连接 break; &#125; else &#123; perror ("accept"); break; &#125; &#125; s = getnameinfo (&amp;in_addr, in_len, hbuf, sizeof hbuf, sbuf, sizeof sbuf, NI_NUMERICHOST | NI_NUMERICSERV); if (s == 0) &#123; printf("Accepted connection on descriptor %d " "(host=%s, port=%s)\n", infd, hbuf, sbuf); &#125; // 设置已连接套接字为非阻塞，并且加入到 epoll 实例监测中 s = make_socket_non_blocking (infd); if (s == -1) abort (); event.data.fd = infd; // ET 模式 event.events = EPOLLIN | EPOLLET; s = epoll_ctl (efd, EPOLL_CTL_ADD, infd, &amp;event); if (s == -1) &#123; perror ("epoll_ctl"); abort (); &#125; &#125; continue; &#125; else &#123; // 已连接套接字可读，我们读取该套接字所有的数据并打印出来 // 由于使用了 ET 模式，我们必须将所有可读数据读取完毕 int done = 0; while (1) &#123; ssize_t count; char buf[512]; count = read (events[i].data.fd, buf, sizeof buf); if (count == -1) &#123; // 如果 errno == EAGAIN，说明所有数据已读取完毕 // 如果 errno != EAGAIN，说明读取出错 if (errno != EAGAIN) &#123; // 读取出错 perror ("read"); done = 1; &#125; break; &#125; else if (count == 0) &#123; // 客户端断开了连接 done = 1; break; &#125; // 打印到标准输出 s = write (1, buf, count); if (s == -1) &#123; perror ("write"); abort (); &#125; &#125; if (done) &#123; printf ("Closed connection on descriptor %d\n", events[i].data.fd); // 关闭连接 close (events[i].data.fd); &#125; &#125; &#125; &#125; free (events); close (sfd); return EXIT_SUCCESS;&#125; main函数首先调用create_and_bind创建并绑定监听套接字，接下来调用make_socket_non_blocking设置监听套接字为非阻塞模式，并调用listen系统调用监听客户端的连接请求。接下来，我们创建了一个 epoll 实例，并将监听套接字加入到该 epoll 实例的 interest list，当监听套接字可读时，说明有新的客户端请求连接。在主循环中，我们调用epoll_wait等待就绪事件的发生。timeout参数设置为-1说明主线程会一直阻塞到事件就绪。这些就绪事件包括以下类型： 客户端请求到达：当监听套接字可读时，说明一个或者多个客户端连接请求到达，我们设置新的已连接套接字为非阻塞模式并添加到 epoll 实例的 interest list 中。 客户端数据可读：已连接套接字就绪时，说明客户端数据可读。我们使用read每次读出512字节的数据，直接所有的数据读取完毕。这是由于我们使用了 ET 模式，ET 模式对于数据可读只会通知一次。读出的数据通过write系统调用打印到标准输出。 完整的程序可以在这里下载：epoll_example.c 参考资料 https://banu.com/blog/2/how-to-use-epoll-a-complete-example-in-c/ Linux/UNIX 系统编程手册，Michael Kerrisk 著，郭光伟译，人民邮电出版社 UNIX网络编程，卷1：套接字联网API，第3版，人民邮电出版社 http://blog.lucode.net/linux/epoll-tutorial.html]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>epoll</tag>
        <tag>IO多种复用</tag>
        <tag>epoll_create</tag>
        <tag>epoll_ctl</tag>
        <tag>epoll_wait</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 性能测试记录]]></title>
    <url>%2FRedis-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[虽然 Redis 本身也提供redis-benchmark工具来对 Redis 的性能进行测试，但为了对测试维度自由定制，我们还是通过自己编写脚本的方式来测试。 Redis 的官方文档也提到了，简单的起一个循环，然后在循环中向 Redis 发送操作命令，其实不是对 Redis 进行性能测试，而是对网络延迟进行测试。为了真正测试 Redis 的并发性能，需要使用多个 Redis 连接，或者使用 pipelining 来聚合多个命令。当然也可以使用多线程或者多进程。我们使用 Python 脚本来作为 Redis 的测试工具，并采用每个进程一个 Redis 连接的方案。 测试脚本如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/env python# -*- coding:utf-8 -*-import timeimport datetimefrom multiprocessing import Processimport redisdef write_worker(n): print 'in write worker: %d, time: %s' % (n, datetime.datetime.now()) r = redis.StrictRedis(host='10.88.115.114', port=6380) s = 'a' * 1024 * 1024 * 10 t1 = time.time() bigkey = 'bigkey%d' % (n + 100) r.set(bigkey, s) t2 = time.time() print 'write time: %f' % (t2 - t1)def read_worker(n): print 'in read worker: %d, time: %s' % (n, datetime.datetime.now()) r = redis.StrictRedis(host='10.88.115.114', port=6380) t1 = time.time() bigkey = 'bigkey%d' % n r.get(bigkey) t2 = time.time() print 'read time: %f' % (t2 - t1)def test(): read_processes = [] for i in range(0, 100): read = Process(target=read_worker, args=(i,)) read.start() read_processes.append(read) write_processes = [] for i in range(0, 100): write = Process(target=write_worker, args=(i,)) write.start() write_processes.append(write) for write in write_processes: write.join() for read in read_processes: read.join() print 'end press test'if __name__ == '__main__': test() redis-py 的StrictRedis已提供了连接池的功能的，可以在多线程中直接使用，这里我们使用了多进程的方式，并采用每个进程都使用独立的一个StrictRedis实例。在上面的代码中，我们起了大量的读写进程。对 Redis 的写入操作中每次发送一个SET的请求，写入的数据大小为 100 MB，读操作则每次读取GET 100 MB的数据（需要提前准备GET请求的 key 的数据）。测试程序的目的是为了测试 Redis 在处理较高并发时，大数据读写的性能。 测试结果在这里就不贴不出来，有兴趣的读者也可以运行一下这个 Python 脚本，就可以得到测试结果了。从测试结果来看，Redis 的性能还是很优秀的。 这里重点提一下测试过程中 Redis 服务会报错的问题。在写入大量的数据（100个10MB，即1GB）后，Redis 进程开始出现以下日志： [2418] 21 Apr 09:38:24.041 * 10 changes in 300 seconds. Saving…[2418] 21 Apr 09:38:24.042 # Can’t save in background: fork: Cannot allocate memory 客户端执行写入命令会报错： 127.0.0.1:6380&gt; set key value(error) MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error. 从网上查找 Redis 的资料可以知道，这是由于 Redis 开启了 RDB 功能所致。RDB 用于保存 Redis 的数据快照，将数据快照数据持久化到硬盘中。以下是 Redis 的 RDB 默认配置： 123save 900 1 save 300 10 save 60 10000 其含义是900秒内有1次修改，300秒内有10次修改，60秒内有10000秒修改，都会将照数据持久化到磁盘中。 要解决 Redis 报错的问题，可以把 RDB 功能关闭，或者通过优化 Linux 内存配置优化解决。具体可以参考《LINUX下REDIS内存优化》。 1sysctl vm.overcommit_memory=1 以 root 权限执行此命令后，再对 Redis 服务进行重启，然后执行测试程序，不再出现 Redis 报错的问题。 需要指出的是，按照网上很多文章说的，可以通过修改 redis.conf 配置中的stop-writes-on-bgsave-error选项的方式来解决报错的问题，实际上只是把问题掩盖起来而。因为采用下面的配置： stop-writes-on-bgsave-error no RDB 功能虽然不能保存 Redis 数据快照了，但 Redis 仍可继续接受新的 Redis 请求。为了从根本解决 RDB 报错的问题，还是应该通过修改vm.overcommit_memoryLinux 内核参数的方式来解决。 参考资料 https://redis.io/topics/benchmarks http://www.redis.cn/topics/benchmarks.html http://blog.chedushi.com/archives/7719]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>RDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis C 语言客户端 hiredis 的使用]]></title>
    <url>%2FRedis-C-%E8%AF%AD%E8%A8%80%E5%AE%A2%E6%88%B7%E7%AB%AF-hiredis-%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[最近需要使用C++来访问 Redis，查找了一些开源C/C++ 的 Redis 客户端，发现 hiredis 目前的社区活跃度较高，且比较轻量级，就决定选用这款客户端了。 安装以 Mac OS 为例，说明如何安装 hiredis。 下载 hiredis 的代码 1git clone https://github.com/redis/hiredis.git 编译安装 12makemake install 第一个例子下面的例子中，首先创建了一个连接 Redis 的实例，然后通过这个连接向 Redis 发送命令。取得 Redis 的返回后，需要记住将相关的返回对象和连接对象释放，避免资源的泄漏。 1234567891011121314151617181920212223#include &lt;stdio.h&gt;#include &lt;hiredis/hiredis.h&gt;int main()&#123; redisContext *conn = redisConnect("127.0.0.1", 6379); if (conn != NULL &amp;&amp; conn-&gt;err) &#123; printf("connection error: %s\n", conn-&gt;errstr); return 0; &#125; redisReply *reply; reply = redisCommand(conn, "SET %s %s", "foo", "bar"); freeReplyObject(reply); reply = redisCommand(conn, "GET %s", "foo"); printf("%s\n", reply-&gt;str); freeReplyObject(reply); redisFree(conn); return 0;&#125; 写好代码后，我们对上面的代码进行编译。 1gcc -o testhiredis testhiredis.c -L/usr/local/lib -lhiredis 其中的-L参数指定 gcc 的库文件的搜索路径，在这里为/usr/local/lib。/usr/local/lib包含了 hiredis 的库文件。-l参数则指明使用的库文件，-lhiredis的意思即是搜索libhiredis.a的库文件。libhiredis.a实际上已安装在/usr/local/lib目录下了，有兴趣的可以查证一下。 上面的程序执行结果为： bar]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>hiredis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web 服务架构类型]]></title>
    <url>%2FWeb-%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[根据 Web 应用架构设计的风格，可以将 Web 服务划分为以功能为中心的服务以及以资源为中心的服务。 以功能为中心的服务以功能为中心的 Web 服务历史由来已久，它是指能够调用远程机器上的功能或者对象方法，而无须知道这些功能或者对象是如何实现的。我们了解的 CORBA（公共对象请求代理体系架构），XML - RPC（可扩展标记语言——远程过程调用），DCOM（分布式组件对象模型），SOAP（简单对象访问协议）等技术都是关注于如何让客户端代码调用一个在远程机器上实现的功能。本文我们重点讲述 SOAP 协议。 SOAPSOAP 基于 XML 和 HTTP，使用 XML 来实现消息描述，使用 HTTP 实现消息传输。SOAP 最重要的一个特性是允许 Web 服务基于自身的约定描述完成服务发现并生成集成代码。 图1所示为 SOAP 的工作原理。 图1：SOAP 的工作原理 Web 服务提供者暴露一组 XML 资源，诸如 Web 服务定义语言（WSDL）文件，XML 模式定义（XSD）文件。WSDL 描述方法与可用的终端信息，XSD 描述请求与响应中用到的数据结构。这些资源文件就是 Web 服务的约定，客户端代码如何生成，Web 服务如何调用，这些必要的信息全部都在这些资源文件中。 举个例子，如果我们使用 Java 开发客户端，那么我们可以使用专门的工具和库，下载这些 Web 服务定义资源文件，然后生成 Java 本地客户端代码。这些代码就是一些 Java 类，可以被编译并用在应用程序中。在这些代码的背后，会依赖到 SOAP 相关的一些库，这些库封装了数据序列化，身份认证，路由，错误处理等各种实现细节。客户端无须知道自己调用的是远程 Web 服务，只需要简单地依赖基于 Web 服务约定（WSDL 和 XSD文件）生成的 Java 代码就可以使用这些远程服务器提供的功能了。 SOAP 的另一个重要的特性是其扩展性。一些高级特性，比如事务，身份认证，加密等已被集成到 SOAP 中。SOAP 的这些优点，使得 SOAP 在企业级应用中使用广泛，比如银行，保险之类的企业就大量使用了 SOAP。 然而，SOAP 对伸缩性却支持不够。我们知道，SOAP 请求是通过 XML 发送的，请求参数和方法名都在 XML 里面。URL 并不包含远程过程调用所需要的全部信息，响应也就没办法在 HTTP 层面基于 URL 进行缓存。也就是说，使用 SOAP 会使应用的伸缩性变差，应用没办法使用反向代理进行缓存。 以资源为中心的服务开发 Web 服务的一个替代方案是将服务的关注点从功能转移到资源上。REST是一个面向资源架构风格的方案，早在 2000 年初就已经提出来。由于其简单和轻量级，REST 已逐渐变成 Web 服务事实上的标准。 REST 是 Roy Thomas Fielding 在他的 2000 年博士论文中提出的，Fielding 将他对互联网软件的架构原则，定名为 REST，即 Representational State Transfer 的缩写。REST 中资源是指 Web 上一切可以识别的，可命名的，可以被找到并被处理的实体。REST 使用 URI（统一资源定位符）指向资源，使用 HTTP 请求方法操作资源。 REST 架构风格最重要的架构约束有如下5个。 客户端-服务器端，即 Client / Server 的架构形式 无状态，通信的会话状态由客户端负责维护，即请求中包含了全部必要的信息。如果服务器端保持会话，要么保证指定会话使用同一个服务器响应请求，要么创建一个保存会话数据的集中式存储系统 缓存，由于无状态，故可以使用缓存来提高响应效率 统一接口，每个 REST 应用都共享一种通用架构，接口的意义统一 分层系统，根据单一职责将系统分层，常见的分层为：– 应用层：负责返回 JSON 数据– 服务层：为应用层提供服务支持– 数据层：提供数据存取服务 REST 就是这一系列设计约束的集合，如果一个架构符合 REST 原则，就称它为 RESTful 架构。 REST 不是一种技术，也不是一个标准 / 协议，而是一种使用既有标准（HTTP + URI + JSON）来实现其要求的架构风格。与 REST 对应的不是 SOAP 协议，而是像 RPC 这样的架构风格，即上文我们说的以功能为中心的架构风格。 从 Web 服务发布者的角度看，REST 比 SOAP 更轻量，因为 REST 只需要创建一个在线 wiki 就可以了，在 wiki 里定义各种资源，每种资源的 HTTP 方法，以及请求响应的例子以展示数据格式。 REST 相比于 SOAP 的中另一个好处是无须再去管理 WSDL 及 XSD 这种十分复杂的 API 约定。从客户端角度看，REST 既有优点也有缺点。客户端无法自动生成客户端代码及没法发现 Web 服务，这是缺点。但同时，REST 的方式也降低了使用者的难度，因为不再需要集成自动生成的代码了。从安全的角度看， REST 服务没有 SOAP 服务控制的那么精细。上文提到 SOAP 其扩展性较好，已支持一大批附加的特性，例如事务，身份认证，加密等。从伸缩性角度看， REST 服务是无状态的，意味着可以通过反向代理来处理热门资源，减轻 Web 服务和数据存取的负载压力。 小结通过对两种 Web 服务类型的描述，我们看到 REST 并不一定就比 SOAP 好，REST 也不能取代 SOAP，REST 仅仅是在 SOAP 外提供的一个额外选择。从大企业角度看，REST 并不成熟，扩展特性并不丰富。从创业公司看，SOAP 又太过笨重，严格，难以驾驭。这看人需要看我们具体应用的细节和系统集成的需求。 参考资料 互联网创业核心技术——构建可伸缩的Web应用，Artur Ejsmont 著，李智慧 何坤译，电子工业出版社，2016年 Java RESTful Web Service 实战（第2版），韩陆 著，机械工业出版社，2016年 Python Web 开发实践，董伟明著，电子工业出版社，2016年 http://blog.csdn.net/zhuizhuziwo/article/details/8153327]]></content>
      <categories>
        <category>后台</category>
      </categories>
      <tags>
        <tag>RPC</tag>
        <tag>SOAP</tag>
        <tag>Web服务</tag>
        <tag>Webservice</tag>
        <tag>REST</tag>
        <tag>RESTful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在阿里云部署 Hexo 网站]]></title>
    <url>%2F%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91%E9%83%A8%E7%BD%B2-Hexo-%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[一开始自己的网站 leehao.me 托管在 GitHub 上面，考虑到 GitHub 的访问可能不稳定，另外，也是抱着学习的目的，就将网站迁移到阿里云上面来了。 网站的总体结构如下图所示： 域名 leehao.me 指向负载均衡 SLB（Server Load Balancer）的 IP，在 SLB 后面部署两台后端服务器 ECS（Elastic Compute Service），ECS 上部署 Hexo 的静态资源。 SLB 的监听配置如下： 由于网站需要支持 HTTPS ，故 SLB 前端需要监听 HTTPS 的 443 端口，此处，我们将前端 443 端口的请求转发到后端服务器的 80 端口处理，将前端的 80 端口的请求转发到后端服务器的 8080 端口处理。 接下来，我们看下后端服务器 ECS 的配置。 由于 Hexo 属于静态类的网站，我们直接使用 Nginx 来处理用户的访问请求。对于 Hexo 来说，每次使用hexo g生成好的静态站点内容，都生成在hexo/public目录下。为了部署网站，我们需要将hexo/public目录的内容拷贝到 Nginx 的html目录下。ECS 上 Nginx 的html目录是/usr/share/nginx/html。 接下来修改 Nginx 的配置：1234567891011121314151617181920212223242526log_format logstash &apos;$http_host $server_addr $remote_addr [$time_local] &quot;$request&quot; $request_body $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; $request_time $upstream_response_time&apos;;server &#123; listen 8080; return 301 https://leehao.me$request_uri;&#125;server &#123; listen 80; charset utf-8; client_max_body_size 2M; error_log /home/logs/nginx_hexo_error.log; access_log /home/logs/nginx_hexo_access.log logstash; location / &#123; root html/public; index index.html; &#125; error_page 403 404 = @not_exist; location @not_exist &#123; return 301 /; &#125;&#125; 我们将网站的根目录设置为对于的html目录。 为了处理用户使用 http://leehao.me 访问网站的情况，需要在 Nginx 配置正确的跳转。用户 http://leehao.me 的请求会先到达 SLB 的 80 端口处理，然后到达后端 ECS 的 8080 端口，在上面的 Nginx 配置中，我们已将 8080 端口配置为一个 301 跳转，即跳转为使用 https 访问网站。通过这个跳转，可以正常处理用户的 http 的访问请求。最后，SLB 与 ECS 的端口的对应关系如下图所示： 另外，在上面的配置中，我们还为 Nginx 配置用户404请求时返回301跳转，即默认跳转到网站首页。 参考 http://stackoverflow.com/questions/9147862/nginx-return-301-redirect-when-404-error http://www.jianshu.com/p/31eb5c754c01 https://www.aliyun.com/product/slb https://www.aliyun.com/product/ecs]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
        <tag>Hexo</tag>
        <tag>建站</tag>
        <tag>Nginx</tag>
        <tag>SLB</tag>
        <tag>ECS</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gSOAP 初体验]]></title>
    <url>%2FgSOAP-%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[由于工作调动关系，需要了解 gSOAP 的使用，写个文章记录一下学习的心得，免得以后忘记。 安装由于本人使用的是 Mac OS 系统，故以 Mac OS 为例说明如何安装 gSOAP。 1）下载 gSOAP可以在 https://sourceforge.net/projects/gsoap2 下载最新版本的 gSOAP。 2）安装 flex, bison, openssl可以使用brew install进行安装：1brew install flex bison openssl 3）编译安装 gSOAP解压上面下载的 gSOAP，然后执行下面的命令： 1234cd gsoap-2.8./configure --with-openssl=/usr/local/opt/opensslmakesudo make install 安装完成，会出现以下的提示： 安装中如果出现 fatal error: &#39;openssl/bio.h&#39; file not found 的错误，可以通过尝试重新安装 openssl 和使用最新的 gsoap-2.8 版本的方法来解决，具体解决办法也可以 google 一下。 其他平台的安装教程可以参考官方文档：https://www.genivia.com/downloads.html。 gSOAP 工具gSOAP 提供了两个工具来方便开发人员使用 C/C++ 语言快速开发Web 服务应用，通过 gSOAP 提供的这两个工具，开发人员可以快速生成服务端与客户端代码框架，接下来开发人员只需要实现具体的接口函数即可。 wsdl2hwsdl2h 工具根据 WSDL 文件生成 C/C++ .h 头文件。WSDL（Web Service Description Language）即 Web 服务描述语言，它使用 XML 来对 Web 服务进行描述。wsdl2h 的用法： 1wsdl2h -o 头文件名 WSDL文件名或URL 例如： 1wsdl2h -o calc.h http://www.genivia.com/calc.wsdl wsdl2h 根据 URL 指定的 WSDL 生成calc.h头文件。calc.h对 Web 服务接口进行定义。 wsdl2h 支持额外的参数： -s 生成的头文件不使用 STL -o 文件名，指定输出头文件的名称 -c 产生纯 C 代码，否则是 C++ 代码 -t 文件名，指定 type map 文件，默认是 typemap.dat soapcpp2soapcpp2 工具则从上面生成的头文件生成 SOAP 服务端和客户端框架代码。例如对于上面的cacl.h，使用 soapcpp2 命令： 1soapcpp2 -i -Iimport calc.h soapcpp2 也支持额外的参数： -i 生成 C++ 包装类，客户端为 xxxProxy.h(.cpp)，服务端为xxxService.h(.cpp) -I 指定 import 的路径，比如需要引入stlvector.h文件来支持 STL vector 的序列化 -C 仅生成客户端代码 -S 仅生成服务端代码 -c 产生纯 C 代码，否则是 C++ 代码 -x 不要产生 XML 示例文件 -L 不要产生soapClientLib.c和soapServerLib.c文件 例子gSOAP 中包含了大量的例子以便让开发人员快速了解如何使用 gSOAP 开发 Web 服务。我们以 gSOAP 的 samples 目录下的 calc++ 的代码为例，说明如何使用 gSOAP 来编写客户端和的服务端代码。 calc++ 目录已经包含了 calc.h 头文件，这个头文件跟上面我们使用 wsdl2h 生成的 calc.h 头文件并不完全相同，为了实验的方便，我们使用 calc++ 目录的calc.h 头文件进行实验。 calc.h头文件：1234567891011121314//gsoap ns service method: add Sums two valuesint ns__add(double a, double b, double *result);//gsoap ns service method: sub Subtracts two valuesint ns__sub(double a, double b, double *result);//gsoap ns service method: mul Multiplies two valuesint ns__mul(double a, double b, double *result);//gsoap ns service method: div Divides two valuesint ns__div(double a, double b, double *result);//gsoap ns service method: pow Raises a to bint ns__pow(double a, double b, double *result); 然后，我们使用 soapcpp2 工具来生成客户端和服务端的框架代码： 1soapcpp2 -i -Iimport calc.h 客户端代码calcclient.c++ 代码：1234567891011121314151617181920212223242526272829303132333435363738394041#include "soapcalcProxy.h"#include "calc.nsmap"const char server[] = "http://127.0.0.1:8080";int main(int argc, char **argv)&#123; if (argc &lt; 4) &#123; fprintf(stderr, "Usage: [add|sub|mul|div|pow] num num\n"); exit(0); &#125; double a, b, result; a = strtod(argv[2], NULL); b = strtod(argv[3], NULL); calcProxy calc; calc.soap_endpoint = server; switch (*argv[1]) &#123; case 'a': calc.add(a, b, &amp;result); break; case 's': calc.sub(a, b, &amp;result); break; case 'm': calc.mul(a, b, &amp;result); break; case 'd': calc.div(a, b, &amp;result); break; case 'p': calc.pow(a, b, &amp;result); break; default: fprintf(stderr, "Unknown command\n"); exit(0); &#125; if (calc.error) calc.soap_stream_fault(std::cerr); else printf("result = %g\n", result); return 0;&#125; 由于代码使用 STL，为了顺利编译通过，需要将 gSOAP 中的stdsoap2.cpp和stdsoap2.h文件拷贝到客户端和服务端代码所在的目录。改写好客户端代码后，使用 g++ 进行编译：1g++ -o calcclient calcclient.cpp soapC.cpp soapcalcProxy.cpp stdsoap2.cpp 编译顺利通过。 服务端代码calcserver.cpp代码如下，其中可以指定服务端的端口号: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include "soapcalcService.h"#include "calc.nsmap"int main(int argc, char **argv)&#123; calcService calc; if (argc &lt; 2) calc.serve(); /* serve as CGI application */ else &#123; int port = atoi(argv[1]); if (!port) &#123; fprintf(stderr, "Usage: calcserver++ &lt;port&gt;\n"); exit(0); &#125; /* run iterative server on port until fatal error */ if (calc.run(port)) &#123; calc.soap_stream_fault(std::cerr); exit(-1); &#125; &#125; return 0;&#125; int calcService::add(double a, double b, double *result)&#123; *result = a + b; return SOAP_OK;&#125; int calcService::sub(double a, double b, double *result)&#123; *result = a - b; return SOAP_OK;&#125; int calcService::mul(double a, double b, double *result)&#123; *result = a * b; return SOAP_OK;&#125; int calcService::div(double a, double b, double *result)&#123; if (b) *result = a / b; else &#123; char *s = (char*)soap_malloc(this, 1024); (SOAP_SNPRINTF(s, 1024, 100), "&lt;error xmlns=\"http://tempuri.org/\"&gt;Can't divide %f by %f&lt;/error&gt;", a, b); return soap_senderfault("Division by zero", s); &#125; return SOAP_OK;&#125; int calcService::pow(double a, double b, double *result)&#123; *result = ::pow(a, b); if (soap_errno == EDOM) /* soap_errno is like errno, but compatible with Win32 */ &#123; char *s = (char*)soap_malloc(this, 1024); (SOAP_SNPRINTF(s, 1024, 100), "&lt;error xmlns=\"http://tempuri.org/\"&gt;Can't take power of %f to %f&lt;/error&gt;", a, b); return soap_senderfault("Power function domain error", s); &#125; return SOAP_OK;&#125; 然后使用 g++ 来对服务端代码进行编译： 1g++ -o calcserver calcserver.cpp soapC.cpp soapcalcService.cpp stdsoap2.cpp 编译同样顺利通过。 测试运行上面编译好的calcserver和calcclient可执行文件来对 Web 服务进行测试，测试结果如下： 参考资料 https://www.cs.fsu.edu/~engelen/soapdoc2.html https://www.genivia.com/downloads.html https://www.genivia.com/dev.html http://blog.csdn.net/yangjun1115/article/details/29360389 https://www.cs.fsu.edu/~engelen/calc.html http://commandos.blog.51cto.com/154976/130652 http://www.cppblog.com/qiujian5628/archive/2008/10/11/54019.html http://blog.sina.com.cn/s/blog_5ee9235c0100de3g.html]]></content>
      <categories>
        <category>后台</category>
      </categories>
      <tags>
        <tag>gSOAP</tag>
        <tag>WSDL</tag>
        <tag>wsdl2h</tag>
        <tag>soapcpp2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Terminal折腾记]]></title>
    <url>%2FTerminal%E6%8A%98%E8%85%BE%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[闲逛网上文章时，发现可以对Mac的终端进行改造。之前自己一直使用默认的Mac终端 ，并没有过多的配置，使用起来并不是很便捷，这也更加坚定了自己对终端进行改造的决心。 开门见山，先直接上一幅改造后终端的最终效果图： 为了达到上图改造后的效果，需要安装以下几个软件： iTerm2 Z Shell(zsh) Solarized 配色方案 Oh My Zsh 我们逐个进行介绍。 iTerm2iterm2是一强大的终端工具，按照其官网的说法，它是Mac OS terminal的替代品，并带来了大量改进的功能。可以到iterm2官网http://www.iterm2.com下载iterm2的最新版本，并进行安装，过程比较简单，此处不再赘述。 Z ShellZ shell（zsh）由Bourne shell发展而来，功能更加强大。zsh最初由普林斯敦的一位叫Paul Falstad的学生实现（膜拜牛人），而zsh这个名字最初来源于普林斯敦一位老师登录zsh的ID：zsh。Paul Falstad觉得这个名字很好，就用来作为shell的名称了。Mac系统自带了4.0版本的zsh，可以使用brew isntall安装5.x版本的zsh。 brew install zsh 删除版本的zsh，并查询新版本的安装目录： sudo rm /bin/zshbrew –prefix zsh 得到输出： /usr/local/opt/zsh 即zsh的安装目录是/usr/local/opt/zsh，然后再执行下面的命令，使用新版本的zsh替换原有旧版本的zsh： sudo ln -s /usr/local/opt/zsh/bin/zsh /bin/zshchsh -s /bin/zsh # 切换系统当前用户的默认 shell 为 zsh Solarized 配色方案在http://ethanschoonover.com/solarized可以下载最新版本的solarized配色方案。下载后，进行iterm2-colors-solarized目录，点击Solarized Dark.itermcolors，便完成配色方案的安装。 打开iterm2的偏好设定，Profiles -&gt; Colors，选择Solarized Dark，完成iterm2配色方案的设置： Oh My Zshoh my zsh是一套强大的zsh配置方案，拥有众多强大的插件和酷炫的主题，绝对的装逼利器。 插一段oh my zsh的官方介绍，很有意思： Once installed, your terminal shell will become the talk of the town or your money back! With each keystroke in your command prompt, you’ll take advantage of the hundreds of powerful plugins and beautiful themes. Strangers will come up to you in cafés and ask you, “that is amazing! are you some sort of genius?” 可以使用curl安装oh my zsh： sh -c “$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)” 为避免乱码的问题，需要额外安装powerline字体： git clone https://github.com/powerline/fonts.gitcd fonts./install.shcd ..rm -rf fonts 安装好powerline字体后，按照下图设置iterm2的字体： 这步很重要，否则在iterm2上没法显示箭头等特殊字符，我也是在这个问题上折腾了一段时间:( 接着修改oh my zsh的配置。编辑~/.zshrc文件，设置oh my zsh的插件： plugins=(git brew node npm) 编辑~/.zshrc文件，设置oh my zsh的主题： ZSH_THEME=”agnoster” 编辑~/.zshrc文件，去除shell提示符前面多余的用户名： export DEFAULT_USER=”$(whoami)” 在完成上述的软件安装和配置后，终端便是折腾完成了。俗话说磨刀不误砍柴工，有时折腾一下还是很有必要的。 参考资料 http://www.jianshu.com/p/bb1c97269b11 http://xingrz.me/2013/2013-06-19/terminal-zhuangbility.html https://en.wikipedia.org/wiki/Z_shell http://ohmyz.sh http://tieba.baidu.com/p/4083811481 http://stackoverflow.com/questions/31848957/zsh-hide-computer-name-in-the-terminal]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>terminal</tag>
        <tag>终端</tag>
        <tag>zsh</tag>
        <tag>iterm2</tag>
        <tag>oh my zsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmux：终端复用利器]]></title>
    <url>%2Ftmux%EF%BC%9A%E7%BB%88%E7%AB%AF%E5%A4%8D%E7%94%A8%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[什么是终端复用（terminal multiplexer）？终端复用可以让你在同一个终端同时打开不同的程序并观察输出，同时允许你方便地退出和恢复这些程序的使用。使用tmux可以达到终端复用的目的，下图为tmux在同一个终端窗口中同时打开不同程序的一个示例： 安装对于 Mac OS，使用简单的 brew命令即可完成tmux安装： brew install tmux Mac下如何安装的详细教程可以参考这个链接：Mac安装tmux教程。 基本概念在tmux中，session包括一个或者多个window，window包括一个或者多个pane，这三者的关系如下图所示： Session：tmux使用session来区分不同的工作环境，例如对于一个程序员来说，可以区分使用work session和play session，work session上班工作时使用，play session在家使用。 Window：这个可以以Mac系统的虚拟桌面的来类比，一个window就相当一个虚拟桌面，记住这点就行了。 Pane：一个pane对应一个视图，在不同的pane中执行不同的命令并进行显示。可以说pane就是实现终端复用最直接的载体。 举个例子来说明这三者的关系：假设我们有一个sysadmin的session，该session下面有一个log的window，为了查看不同的日志，我们创建了三个不同的pane：access log pane，error log pane以及syslog log pane。 这个就是tmux需要掌握的三个概念了，上面的文字描述也许还有些抽象，不过不用担心，tmux其实使用起来很简单的，使用几次后，上面的概念就都理解了。 基本使用使用简单的tmux命令，可以创建一个默认的session： tmux 然后进入下面的界面： 底部的状态栏是tmux的一个重要组成部分。状态栏左边是tmux的session，window以及pane信息，状态栏右边则是用户名称和日期等信息。tmux状态栏也是可以定制的，如何定制不在本文的描述范围，具体可以参考网上资料。 Pane命令创建pane上面我们使用tmux命令创建了一个session，也默认创建一个window以及window下的一个pane。只有一个pane还没法达到终端复用的目的，为此我们还需要创建额外的pane。 在tmux中，所有的命令都需要一个前缀。默认地，tmux使用control + b（下面以C - b表示）来作用命令的前缀。为创建一个pane，可以使用命令C - b %。这个命令的意思是，先按下control键和b键，松开后，再按下%键。 C - b %创建一个水平分割的pane： 如果需要创建一个垂直分割的pane，可以使用命令C - b &quot;。 切换pane创建不同的pane后，如果需要激活不同的pane，可以使用命令C - b &lt;上下左右箭头&gt;，即先按下control键和b键，松开后，再按下箭头按键，选择不同的pane。 如果需要将当前pane全屏，可以使用命令C - b z，恢复则再操作一次C - b z，这个命令很实用。 关闭pane如果需要关闭一个pane，可以在该pane输入exit命令。 pane的使用占据了tmux使用的大部分，不夸张地说，掌握了pane命令的使用就基本掌握tmux的使用，从而可以应对日常工作需要了。 Window命令使用C - b c命令（按下control键和b键，松开后，再按下c键）创建一个window： 状态栏左侧出现了两个提示符0:和1:，表示两个windows。如果需要在不同的window进行切换，可以使用命令C - b &lt;数字&gt;来选择不同的window。 Session命令使用C - b d命令可以退出一个session。虽然退出了某个session，但tmux仍然保持该session在后台运行，我们随时可以重新恢复该session的使用。为了查看当前有哪些sessions，可以使用以下命令： tmux ls 输出： 0: 1 windows (created Mon Apr 3 10:26:09 2017) [88x21] 为了重新进入原来的session，可以使用命令： tmux attach -t 0 -t 0的含义是让tmux进入session 0。session 0是tmux默认创建的第一个session。 如果需要对session进行重命名，可以使用命令： tmux rename-session -t 0 code 这时，session的名称已发生变化： tmux lscode: 1 windows (created Mon Apr 3 10:26:09 2017) [88x21] 当然，也可以在创建时指定session的名称： tmux new -s db 这样就创建一个名为db的session。 如果需要进入db的session，可以使用命令： tmux attach -t db 小结tmux可谓终端复用的利器。以前在使用terminal.app时，假设需要实现同时连接服务器和查看本机的信息时，就不得不打开两个终端窗口（使用iterm2会方便一点）。之从使用上了tmux，再也不用打开多余的窗口了，桌面变成更整洁舒畅，查看各个命令的输出也更加方便直接。 参考资料 https://tmux.github.io http://harttle.com/2015/11/06/tmux-startup.html https://robots.thoughtbot.com/a-tmux-crash-course https://gist.github.com/simme/1297707 https://danielmiessler.com/study/tmux/ http://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tmux</tag>
        <tag>终端复用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客使用上了网易云跟贴]]></title>
    <url>%2F%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E4%B8%8A%E4%BA%86%E7%BD%91%E6%98%93%E4%BA%91%E8%B7%9F%E8%B4%B4%2F</url>
    <content type="text"><![CDATA[本来对于多说不支持 HTTPS 就不太满意，早就想换一个评论系统了。昨天上去多说的官网一看，发现多说竟然要在今年6月1号关闭了。 可能是多说没有找到赚钱的路子吧，这样的话自己只能默默祝福一下了。 多说的关闭也坚定了自己将博客的评论跟换成网易跟贴的决心。 说干就干。原来以为 next 的官网仍然没有网易跟贴的支持的文档，今天再次上去一看，发现文档已更新，其中多说的支持也已经去掉了，next更新果然及时，也发现多说也是人走茶凉啊。 next 集成网易跟贴很简单，就是只有这步，只需要将next目录下的_config.yml中的gentie_productKey填入即可，所以关键是申请一个gentie_productKey。 进入网易云跟贴的首页，填写必要的信息后，然后获取APP KEY,将这个APP KEY填入上面的_config.yml中的gentie_productKey即可。 贴一下自己使用网易云跟贴的效果： 网易云跟贴的UI和用户体验还是不错的，值得推荐~]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>网易云跟贴</tag>
        <tag>评论</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[make命令与makefile文件]]></title>
    <url>%2Fmake%E5%91%BD%E4%BB%A4%E4%B8%8Emakefile%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[一、多个源文件带来的问题在编写c/c++测试程序时，我们习惯每次修改一处代码，然后就马上编译运行来查看运行的结果。这种编译方式对于小程序来说是没有多大问题的，可对于大型程序来说，由于包含了大量的源文件，如果每次改动一个地方都需要编译所有的源文件，这个简单的直接编译所有源文件方式对程序员来说简直是噩耗。我们看一个例子： 12345678910// main.c#include "a.h"// 2.c#include "a.h"#include "b.h"// 3.c#include "b.h"#include "c.h" 如果程序员只修改了头文件c.h，则源文件main.c和2.c都无需编译，因为它们不依赖这个头文件。而对3.c来说，由于它包含了c.h,所以在头文件c.h改动后，就必须得新编译。而如果改动了b.h可是忘记编译了2.c，那么最终的程序就可能无法正常工作。make 工具就是为了解决上述问题而出现的，它会在必要时重新编译所有受改动影响的源文件。 二、make 命令make命令本身支持许多选项，最常用的是-f选项。如果我们直接运行 make 那么make命令会首先在当前目录查找名为makefile的文件，如果找不到，就会查找名为Makefile的文件。为了指示make命令将哪个文件作为makefile文件，可以使用 -f 选项： make -f Makefile1 三、makefile 文件上面提到makefile文件，那么什么是makefile文件呢？make命令功能虽然十分强大，但是光凭其自身无法了解如何构建应用程序的。这时，makefile就出来了，它告诉make应用程序如何构建的。make命令和makefile文件的结合提供了一个在管理项目的十分强大的工具，它们不仅用于控制源文件的编译，而且还提供了将应用程序安装到目标目录等其他功能。 3.1 依赖关系依赖关系定义了应用程序里面每个文件与其他源文件之间的关系。例如在上面的例子中，我们可以定义最终应用程序依赖于目标文件main.o，2.o和3.o。同样，main.o依赖于main.c和a.h，2.o依赖于2.c，a.h和b.h，3.o依赖于3.c，b.h和c.h。在makefile文件中，依赖关系的写法是：先写目标的名称，然后紧跟一个冒号，接着是空格或者制表符tab，最后是用空格或者制表符tab隔开的文件列表。上面的例子的依赖关系如下： 1234myapp: main.o 2.o 3.omain.o: main.c a.h2.o: 2.c a.h b.h3.o: 3.c b.h c.h 这组依赖关系形成一个层次结构，展示了源文件之间的关系。例如，如果源文件b.h发生改变，就需要重新编译2.o和3.o，接下来还需要重新编译myapp。 3.2 规则makefiel文件中的规则定义了目标的创建方式。在上面的例子中，我们使用gcc -c 2.c创建2.o。这个gcc命令即是目标2.o的创建方式，也即是规则。在makefile文件中，规则都必须以tab开头。在源文件所在的目录下创建Makefile1文件，其内容如下。 12345678myapp: main.o 2.o 3.o gcc -o myapp main.o 2.o 3.omain.o: main.c a.h gcc -c main.c2.o: 2.c a.h b.h gcc -c 2.c3.o: 3.c b.h c.h gcc -c 3.c 三个头文件a.h，b.h，c.h内容都为空，源文件的内容如下： 12345678910111213/* main.c */#include &lt;stdlib.h&gt;#include "a.h"extern void function_two();extern void function_three();int main()&#123; function_two(); function_three(); exit(EXIT_SUCCESS);&#125; 12345678/* 2.c */#include &lt;stdio.h&gt;#include "a.h"#include "b.h"void function_two() &#123; printf("function two\n");&#125; 12345678/* 3.c */#include &lt;stdio.h&gt;#include "b.h"#include "c.h"void function_three() &#123; printf("function three\n");&#125; 执行make命令，： $ make -f Makefile1gcc -c main.cgcc -c 2.cgcc -c 3.cgcc -o myapp main.o 2.o 3.o 运行应用程序： $ ./myappfunction twofunction three 从输出可以说明应用程序已被正确构建。 如果改变b.h头文件，makefile能够正确处理这一变化，只有2.c和3.c发生重新编译： $ touch b.h $ make -f Makefile1gcc -c 2.cgcc -c 3.cgcc -o myapp main.o 2.o 3.o 3.3 注释makefile文件使用#来表示注释，一直延续到这一行的结束。 3.4 宏不同的平台下可能使用不同的编译器，不同的环境（例如开发与线上环境）也可能使用不同的编译器选项，为了便于修改makefile这些可变的参数，我们可以使用宏来实现makefile。makefile引用宏定义的方法为$(MACRONAME)。我们来看如何使用宏来改写上面的makefile文件。 123456789101112131415161718192021all: myapp# 编译器CC = gcc# include的搜索路径INCLUDE = .# 编译器参数CFLAGS = -g -Wall -ansimyapp: main.o 2.o 3.o $(CC) -o myapp main.o 2.o 3.omain.o: main.c a.h $(CC) -I$(INCLUDE) $(CFLAGS) -c main.c2.o: 2.c a.h b.h $(CC) -I$(INCLUDE) $(CFLAGS) -c 2.c3.o: 3.c b.h c.h $(CC) -I$(INCLUDE) $(CFLAGS) -c 3.c 我们习惯在makefile文件中将第一个目标定义为all，然后再列出其他从属的目标，上面的makefile也遵循这个约定。 运行make命令： $ make -f Makefile2gcc -I. -g -Wall -ansi -c main.cgcc -I. -g -Wall -ansi -c 2.cgcc -I. -g -Wall -ansi -c 3.cgcc -o myapp main.o 2.o 3.o 同样也正确构建了应用程序myapp。 3.5 多个目标makefile文件除了定义编译的目标外，还可以定义其他的目标。例如，增加一个clean选项来删除不需要的目标文件，增加一个install选项来将编译成功的应用程序安装到另一个目录下，等等。 1234567891011121314151617181920212223242526272829303132all: myappCC = gccINSTDIR = /usr/local/binINCLUDE = .CFLAGS = -g -Wall -ansimyapp: main.o 2.o 3.o $(CC) -o myapp main.o 2.o 3.omain.o: main.c a.h $(CC) -I$(INCLUDE) $(CFLAGS) -c main.c2.o: 2.c a.h b.h $(CC) -I$(INCLUDE) $(CFLAGS) -c 2.c3.o: 3.c b.h c.h $(CC) -I$(INCLUDE) $(CFLAGS) -c 3.cclean: -rm main.o 2.o 3.oinstall: myapp @if [ -d $(INSTDIR) ]; \ then \ cp myapp $(INSTDIR);\ chmod a+x $(INSTDIR)/myapp;\ chmod og-w $(INSTDIR)/myapp;\ echo "Install in $(INSTDIR)";\ else \ echo "sorry, $(INSTDIR) does not exist";\ fi 上面的makefile文件有几点需要注意的。（1）特殊目标all只指定了myapp这个目标，因此，在执行make命令时未指定目标，它的默认行为就是创建目标myapp。（2）目标clean用来测试编译过程中产生的中间文件。（3）目标install用于将应用程序安装到指定目录，它依赖于myapp，即执行install前须先创建myapp。install目标由shell脚本组成，由于make命令在执行规则时会调用一个shell，并且会针对每个规则使用一个新的shell，所以必须在上面每行代码的结尾加上一个\，让所有的shell脚本都处于同一行。脚本以@开头，说明make在执行这些规则之前不会在标准输出显示命令本身。 创建myapp： $ make -f Makefile3gcc -I. -g -Wall -ansi -c main.cgcc -I. -g -Wall -ansi -c 2.cgcc -I. -g -Wall -ansi -c 3.cgcc -o myapp main.o 2.o 3.o 将myapp安装到指到目录： $ make -f Makefile3 installInstall in /usr/local/bin 然后可以直接执行myapp: $ myappfunction twofunction three 删除中间文件： $ make -f Makefile3 cleanrm main.o 2.o 3.o 四、参考资料 Linux程序设计（第4版），Neil Matthew等著，人民邮电出版社，2010年 http://mrbook.org/blog/tutorials/make/ http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>make</tag>
        <tag>makefile</tag>
        <tag>依赖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll 水平触发与边缘触发]]></title>
    <url>%2Fepoll-%E6%B0%B4%E5%B9%B3%E8%A7%A6%E5%8F%91%E4%B8%8E%E8%BE%B9%E7%BC%98%E8%A7%A6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[epoll也是实现I/O多路复用的一种方法，为了深入了解epoll的原理，我们先来看下epoll水平触发（level trigger，LT，LT为epoll的默认工作模式）与边缘触发（edge trigger，ET）两种工作模式。 使用脉冲信号来解释LT和ET可能更加贴切。Level是指信号只需要处于水平，就一直会触发；而edge则是指信号为上升沿或者下降沿时触发。说得还有点玄乎，我们以生活中的一个例子来类比LT和ET是如何确定读操作是否就绪的。 水平触发儿子：妈妈，我收到了500元的压岁钱。妈妈：嗯，省着点花。儿子：妈妈，我今天花了200元买了个变形金刚。妈妈：以后不要乱花钱。儿子：妈妈，我今天买了好多好吃的，还剩下100元。妈妈：用完了这些钱，我可不会再给你钱了。儿子：妈妈，那100元我没花，我攒起来了妈妈：这才是明智的做法！儿子：妈妈，那100元我还没花，我还有钱的。妈妈：嗯，继续保持。儿子：妈妈，我还有100元钱。妈妈：… 接下来的情形就是没完没了了：只要儿子一直有钱，他就一直会向他的妈妈汇报。LT模式下，只要内核缓冲区中还有未读数据，就会一直返回描述符的就绪状态，即不断地唤醒应用进程。在上面的例子中，儿子是缓冲区，钱是数据，妈妈则是应用进程了解儿子的压岁钱状况（读操作）。 边缘触发儿子：妈妈，我收到了500元的压岁钱。妈妈：嗯，省着点花。（儿子使用压岁钱购买了变形金刚和零食。）儿子：妈妈：儿子你倒是说话啊？压岁钱呢？ 这个就是ET模式，儿子只在第一次收到压岁钱时通知妈妈，接下来儿子怎么把压岁钱花掉并没有通知妈妈。即儿子从没钱变成有钱，需要通知妈妈，接下来钱变少了，则不会再通知妈妈了。在ET模式下， 缓冲区从不可读变成可读，会唤醒应用进程，缓冲区数据变少的情况，则不会再唤醒应用进程。 我们再详细说明LT和ET两种模式下对读写操作是否就绪的判断。 水平触发1. 对于读操作只要缓冲内容不为空，LT模式返回读就绪。 2. 对于写操作只要缓冲区还不满，LT模式会返回写就绪。 边缘触发1. 对于读操作（1）当缓冲区由不可读变为可读的时候，即缓冲区由空变为不空的时候。 （2）当有新数据到达时，即缓冲区中的待读数据变多的时候。 （3）当缓冲区有数据可读，且应用进程对相应的描述符进行EPOLL_CTL_MOD 修改EPOLLIN事件时。 2. 对于写操作（1）当缓冲区由不可写变为可写时。 （2）当有旧数据被发送走，即缓冲区中的内容变少的时候。 （3）当缓冲区有空间可写，且应用进程对相应的描述符进行EPOLL_CTL_MOD 修改EPOLLOUT事件时。 实验实验1实验1对标准输入文件描述符使用ET模式进行监听。当我们输入一组字符并接下回车时，屏幕中会输出”hello world”。 12345678910111213141516171819202122#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/epoll.h&gt;int main()&#123; int epfd, nfds; struct epoll_event event, events[5]; epfd = epoll_create(1); event.data.fd = STDIN_FILENO; event.events = EPOLLIN | EPOLLET; epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;event); while (1) &#123; nfds = epoll_wait(epfd, events, 5, -1); int i; for (i = 0; i &lt; nfds; ++i) &#123; if (events[i].data.fd == STDIN_FILENO) &#123; printf("hello world\n"); &#125; &#125; &#125;&#125; 输出： $ ./epoll1ahello worldabchello worldhellohello worldttthello world 当用户输入一组字符，这组字符被送入缓冲区，因为缓冲区由空变成不空，所以ET返回读就绪，输出”hello world”。之后再次执行epoll_wait，但ET模式下只会通知应用进程一次，故导致epoll_wait阻塞。如果用户再次输入一组字符，导致缓冲区内容增多，ET会再返回就绪，应用进程再次输出”hello world”。如果将上面的代码中的event.events = EPOLLIN | EPOLLET;改成event.events = EPOLLIN;，即使用LT模式，则运行程序后，会一直输出hello world。 实验2实验2对标准输入文件描述符使用LT模式进行监听。当我们输入一组字符并接下回车时，屏幕中会输出”hello world”。 123456789101112131415161718192021222324#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/epoll.h&gt;int main()&#123; int epfd, nfds; char buf[256]; struct epoll_event event, events[5]; epfd = epoll_create(1); event.data.fd = STDIN_FILENO; event.events = EPOLLIN; // LT是默认模式 epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;event); while (1) &#123; nfds = epoll_wait(epfd, events, 5, -1); int i; for (i = 0; i &lt; nfds; ++i) &#123; if (events[i].data.fd == STDIN_FILENO) &#123; read(STDIN_FILENO, buf, sizeof(buf)); printf("hello world\n"); &#125; &#125; &#125;&#125; 输出： $ ./epoll2abchello worldeeeeehello worldlihaohello world 实验2中使用的是LT模式，则每次epoll_wait返回时我们都将缓冲区的数据读完，下次再调用epoll_wait时就会阻塞，直到下次再输入字符。如果将上面的程序改为每次只读一个字符，那么每次输入多少个字符，则会在屏幕中输出多少个“hello world”。有意思吧。 实验3实验3对标准输入文件描述符使用ET模式进行监听。当我们输入任何输入并接下回车时，屏幕中会死循环输出”hello world”。 12345678910111213141516171819202122232425#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/epoll.h&gt;int main()&#123; int epfd, nfds; struct epoll_event event, events[5]; epfd = epoll_create(1); event.data.fd = STDIN_FILENO; event.events = EPOLLIN | EPOLLET; epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;event); while (1) &#123; nfds = epoll_wait(epfd, events, 5, -1); int i; for (i = 0; i &lt; nfds; ++i) &#123; if (events[i].data.fd == STDIN_FILENO) &#123; printf("hello world\n"); event.data.fd = STDIN_FILENO; event.events = EPOLLIN | EPOLLET; epoll_ctl(epfd, EPOLL_CTL_MOD, STDIN_FILENO, &amp;event); &#125; &#125; &#125;&#125; 实验3使用ET模式，但是每次读就绪后都主动对描述符进行EPOLL_CTL_MOD 修改EPOLLIN事件，由上面的描述我们可以知道，会再次触发读就绪，这样就导致程序出现死循环，不断地在屏幕中输出”hello world”。但是，如果我们将EPOLL_CTL_MOD 改为EPOLL_CTL_ADD，则程序的运行将不会出现死循环的情况。 参考资料 http://blog.lucode.net/linux/epoll-tutorial.html http://blog.chinaunix.net/uid-28541347-id-4285054.html http://blog.chinaunix.net/uid-28541347-id-4288802.html https://banu.com/blog/2/how-to-use-epoll-a-complete-example-in-c/]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>epoll</tag>
        <tag>ET</tag>
        <tag>LT</tag>
        <tag>水平触发</tag>
        <tag>边缘触发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[I/O多路复用之POLL系统调用]]></title>
    <url>%2FI-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E4%B9%8BPOLL%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[poll函数类似于select函数，也可以实现I/O多路复用。poll函数的声明如下： 12#include &lt;poll.h&gt;int poll(struct pollfd *fdarray, unsigned long nfds, int timeout); 第一个参数是指向一个结构数组第一个元素的指针。每个数组元素都是一个pollfd结构，用于指定测试某个给定描述符fd的条件。 12345struct pollfd &#123; int fd; // 需要测试的描述符 short event; // 对fd感兴趣的事件 short revents; // 发生在fd的事件：期待的事件或者异常情况发生&#125;; 要测试的条件由events成员指定，poll函数在相应的revents成员中返回该描述符的状态。events和revents都由某个特定条件的一位或多位构成。下面表格列出了用于指定events标志以及测试revents标志的一些常值。 常值 是否作为events的输入 是否作为revents的结果 说明 POLLIN 是 是 普通或优先级带数据可读 POLLRDNORM 是 是 普通数据可读 POLLRDBAND 是 是 优先级带数据可读 POLLPRI 是 是 高优先级数据可读 POLLOUT 是 是 普通数据可写 POLLWRNORM 是 是 普通数据可写 POLLWRBAND 是 是 优先级带数据可写 POLLERR 否 是 发生错误 POLLHUP 否 是 发生挂起 POLLNVAL 否 是 描述符不是一个打开的文件 上表可以分为三个部分，第一部分是处理输入的4个常值，第二部分是处理输出的3个常值，第三部分是处理错误的3个常值。其中第三部分的3个常值不能在events中设置，只能在revents中返回。poll识别三类数据：普通（normal），优先级带（priority band），高优先级（high priority）。 nfds参数指定被监听集合fdarray的大小。 timeout参数指定poll函数返回前等待多长的时间，其可能的取值如下表所示： timeout值 说明 INFTIM 永远等待 0 立即返回，不阻塞进程 大于0 等待指定数目的毫秒数 当发生错误时，poll函数的返回值为-1，若经历了timeout时间后仍没有任何描述符就绪，则返回0，否则返回就绪描述符的个数，即revents成员值非0的描述符个数。 如果我们不再关心某个特定描述符，那么可以把与它对应的pollfd结构的fd成员设置成一个负值，poll函数将忽略这样的pollfd结构。 我们使用poll函数来实现一个echo服务器，其代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;poll.h&gt;int main() &#123; int i, maxi, listenfd, connfd, sockfd; int nready; ssize_t n; const int MAXLINE = 1024; char buf[MAXLINE]; int server_len, client_len; struct sockaddr_in server_address; struct sockaddr_in client_address; const int OPEN_MAX = 256; struct pollfd client[OPEN_MAX]; // 创建套接字 listenfd = socket(AF_INET, SOCK_STREAM, 0); // 命名套接字 server_address.sin_family = AF_INET; server_address.sin_addr.s_addr = htonl(INADDR_ANY); server_address.sin_port = htons(6240); server_len = sizeof(server_address); bind(listenfd, (struct sockaddr*)&amp;server_address, server_len); // 创建套接字队列 listen(listenfd, 5); client[0].fd = listenfd; client[0].events = POLLRDNORM; for (i = 1; i &lt; OPEN_MAX; ++i) &#123; client[i].fd = -1; &#125; maxi = 0; // 当前client数组正在使用的最大下标值 for ( ; ; ) &#123; nready = poll(client, maxi + 1, -1); printf("poll ready, num: %d\n", nready); // 处理新的客户连接的情况 if (client[0].revents &amp; POLLRDNORM) &#123; client_len = sizeof(client_address); connfd = accept(listenfd, (struct sockaddr*)&amp;client_address, &amp;client_len); printf("new client, fd: %d\n", connfd); for (i = 1; i &lt; OPEN_MAX; ++i) &#123; if (client[i].fd &lt; 0) &#123; client[i].fd = connfd; // 保存新的客户的已连接套接字 break; &#125; &#125; if (i == OPEN_MAX) &#123; perror("too many clients"); &#125; client[i].events = POLLRDNORM; if (i &gt; maxi) &#123; maxi = i; &#125; if (--nready &lt;= 0) &#123; continue; &#125; &#125; // 检查所有已连接套接字的可读情况 for (i = 1; i &lt;= maxi; ++i) &#123; if ((sockfd = client[i].fd) &lt; 0) &#123; continue; &#125; if (client[i].revents &amp; (POLLRDNORM | POLLERR)) &#123; if ((n = read(sockfd, buf, MAXLINE)) &lt; 0) &#123; // 读出现异常 if (errno == ECONNRESET) &#123; close(sockfd); client[i].fd = -1; &#125; else &#123; perror("read error"); &#125; &#125; else if (n == 0) &#123; // 客户关闭连接 printf("client close, fd: %d\n", sockfd); close(sockfd); client[i].fd = -1; &#125; else &#123; // 正常读取客户数据 printf("receive client data, fd: %d, data len: %d\n", sockfd, n); write(sockfd, buf, n); &#125; if (--nready &lt;= 0) &#123; // 已处理完成所有的就绪事件 break; &#125; &#125; &#125; &#125;&#125; 程序解释如下：（1）我们声明在pollfd结构数组中存储OPEN_MAX个元素，在上面的例子中，我们声明为256个，即进程能够打开的最大描述符数目为256个。（2）我们把client数组的第一项用于监听套接字，其余各项用于已连接套接字（当有新的客户连接时）。maxi用于标识client数组当前正在使用的最大下标值。（3）我们调用poll以等待新的连接或者现有连接上有数据可读。当一个新的连接被接受后，我们在client数组中查找第一个描述符成员为负的可用项。找到一个可用项后，我们把新连接的描述符保存到其中，并设置POLLRDNORM事件。（4）在检查某个现有连接上的数据可读时，我们调用read，并根据read的返回值来做不同的处理。如果是出错或者客户断开连接，那么我们就把客户相应的fd成员设置为-1；如果是客户数据可读，那么我们返回相同的数据给客户端。 运行上面的服务器代码，然后执行客户测试代码： $ ./client &amp; ./client &amp; ./client 服务器代码输出： poll ready, num: 1new client, fd: 4poll ready, num: 2new client, fd: 5receive client data, fd: 4, data len: 1poll ready, num: 2client close, fd: 4receive client data, fd: 5, data len: 1poll ready, num: 1new client, fd: 4poll ready, num: 2receive client data, fd: 4, data len: 1client close, fd: 5poll ready, num: 1client close, fd: 4 参考资料UNIX 网络编程卷1：套接字联网API（第三版）, W.Richard Stevens 等著 附：客户测试程序 1234567891011121314151617181920212223242526272829303132333435/* Make the necessary includes and set up the variables. */#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main()&#123; int sockfd; int len; struct sockaddr_in address; int result; char ch = 'A';/* Create a socket for the client. */ sockfd = socket(AF_INET, SOCK_STREAM, 0);/* Name the socket, as agreed with the server. */ address.sin_family = AF_INET; address.sin_addr.s_addr = inet_addr("127.0.0.1"); address.sin_port = htons(6240); len = sizeof(address);/* Now connect our socket to the server's socket. */ result = connect(sockfd, (struct sockaddr *)&amp;address, len); if(result == -1) &#123; perror("oops: client3"); exit(1); &#125;/* We can now read/write via sockfd. */ write(sockfd, &amp;ch, 1); read(sockfd, &amp;ch, 1); printf("char from server = %c\n", ch); close(sockfd); exit(0);&#125;]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>poll</tag>
        <tag>select</tag>
        <tag>IO多路复用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[I/O多路复用之select系统调用]]></title>
    <url>%2FI-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E4%B9%8Bselect%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[I/O多路复用模型允许我们同时等待多个套接字描述符是否就绪。Linux系统为实现I/O多路复用提供的最常见的一个函数是select函数，该函数允许进程指示内核等待多个事件中的任何一个发生，并只有在一个或多个事件发生或经历一段指定的时间后才唤醒它。作为一个例子，我们可以调用select，告知内核仅在下列情况发生时才返回： 当集合{0, 4}中任意描述符准备好读时返回 当集合{1, 2, 7}中任意描述符准备好写时返回 已经历了10.2秒 也就是说，我们调用select可以告知内核我们对哪些描述符感兴趣以及等待多久时间。select是一个复杂的函数，有许多不同的应用场景，我们将只讨论第一种场景：等待一组描述符准备好读。 123456789#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int select(int n, fd_set *fdset, NULL, NULL, struct timeval *timeout);FD_ZERO(fd_set *fdset); // 将fdset初始为为空集合FD_CLR(int fd, fd_set *fdset); // 从fdset清除fdFD_SET(int fd, fd_set *fdset); // 将fd添加到fdsetFD_ISSET(int fd, fd_set *fdset); // fd是否存在于fdset 我们来看下select函数的参数。参数n指定需要测试的描述符的数目，测试的描述符范围从0到n-1。第二个参数fdset指定需要测试的可读描述符集合。当fdset集合中有描述符可读，或者经历了timeout时间时，select将返回。当select返回时，作为一个副作用，select修改了参数fdset指向的描述符集合，这时fdset变成由读集合中准备好可以读了的描述符组成。select函数的返回值则指明了就绪集合的基数。值得注意的是，由于这个副作用，我们必须每次在调用select时都更新读集合。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/time.h&gt;#include &lt;sys/ioctl.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main() &#123; int listenfd, connfd; int server_len, client_len; struct sockaddr_in server_address; struct sockaddr_in client_address; fd_set readfds, testfds; // 创建套接字 listenfd = socket(AF_INET, SOCK_STREAM, 0); // 命名套接字 server_address.sin_family = AF_INET; server_address.sin_addr.s_addr = htonl(INADDR_ANY); server_address.sin_port = htons(6240); server_len = sizeof(server_address); bind(listenfd, (struct sockaddr*)&amp;server_address, server_len); // 创建套接字队列 listen(listenfd, 5); FD_ZERO(&amp;readfds); FD_SET(listenfd, &amp;readfds); // 等待客户请求 while (1) &#123; char ch; int fd; int nread; // 同时检查监听套接字和已连接套接字 testfds = readfds; printf("server waiting\n"); int result = select(FD_SETSIZE, &amp;testfds, (fd_set*)0, (fd_set*)0, (struct timeval*)0); if (result &lt; 1) &#123; perror("select error"); exit(1); &#125; for (fd = 0; fd &lt; FD_SETSIZE; fd++) &#123; // 检查哪个描述符可读 if (FD_ISSET(fd, &amp;testfds)) &#123; // 如果是一个新的客户连接请求 if (fd == listenfd) &#123; client_len = sizeof(client_address); connfd = accept(listenfd, (struct sockaddr*)&amp;client_address, &amp;client_len); FD_SET(connfd, &amp;readfds); printf("adding client on fd %d\n", connfd); &#125; // 如果是旧的客户活动 else &#123; ioctl(fd, FIONREAD, &amp;nread); // 如果客户断开连接 if (nread == 0) &#123; close(fd); FD_CLR(fd, &amp;readfds); printf("removing client on fd %d\n", fd); &#125; // 客户请求数据到达 else &#123; read(fd, &amp;ch, 1); sleep(5); printf("serving client on fd %d\n", fd); ch++; write(fd, &amp;ch, 1); &#125; &#125; &#125; &#125; &#125;&#125; 上面的代码展示了如何使用select来编写多并发服务器的过程。服务器可以让select调用同时检查监听套接字和已连接套接字。一旦select指示有活动发生，就可以用FD_ISSET来遍历所有可能的文件描述符，以检查是哪个描述符上面有活动发生。如果是监听套接字可读，这说明正有一个客户试图建立连接，此时就可以调用accept创建一个客户的已连接套接字而不用担心阻塞。如果是某个客户描述符准备好，这说明该描述符上有一个客户请求需要我们读取处理。如果读操作返回零字节，这表示有一个客户进程已结束，这时我们可以关闭该套接字并把它从描述符集合中删除。 参考资料 深入理解计算机系统，第2版，机械工业出版社 Linux程序设计（第4版），Neil Matthew等著，人民邮电出版社，2010年 UNIX 网络编程卷1：套接字联网API（第三版）, W.Richard Stevens 等著]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>select</tag>
        <tag>IO多路复用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程并发服务器]]></title>
    <url>%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B9%B6%E5%8F%91%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[在多进程并发服务器的应用程序中，父进程accept一个连接，fork一个子进程，该子进程负责处理与该连接对端的客户之间的通信。尽管多进程的编程模型中，各进程拥有独立的地址空间，减少了出错的概率，然而，fork调用却存在一些问题： fork是昂贵的，fork要把父进程的内存映像复制到子进程，并在子进程中复制所有描述符，这个操作是较重量级的。 fork返回之后父子进程之间信息的传递需要进程间通信（IPC）机制。 线程则可以解决上述两个问题。线程有时也称为轻量级的进程，线程的创建可能比进程的创建快10-100倍。同一个进程内所有线程共享相同的全局内存，这使得线程之间易于共享信息，但伴随这种简易性而来的是线程安全问题。 线程函数1. pthread_create 函数我们介绍的第一个线程的函数是pthread_create，它的作用是创建一个新线程。它的定义如下： 12#include &lt;pthread.h&gt;int pthread_create(pthread_t *thread, pthread_attr_t *attr, void *(*start_routine)(void *), void *arg); 这个函数的定义看起来很复杂，其实用起来很简单。第一个参数是指向pthread_t类型的指针。线程被创建时，这个指针指向的变量将被写入一个标识符（线程ID）我们用该标识符来引用新线程。第二个参数用于设置线程的属性，一般不需要特殊的属性，所以只需要设置该参数为NULL。最后两个参数，分别告诉新线程将要启动执行的函数和传递给该函数的参数。pthread_create函数在成功调用时返回0，如果失败则返回失败码。 2. pthread_exit 函数线程通过调用pthread_exit函数终止执行。这个函数的作用是，终止调用它的线程并返回一个指向某个对象的指针。 12#include &lt;pthread.h&gt;void pthread_exit(void *retval); 3. pthread_join 函数pthread_join函数的作用是等待某个线程的结束。其第一个参数指定了需要等待的线程ID，第二个参数是一个二级指针，它指向另一个指针，而后者指向线程的返回值。12#include &lt;pthread.h&gt;int pthread_join(pthread_t th, void **thread_return); 4. pthread_self 函数每个线程都有一个在所属进程内标识自己的ID。线程ID由phtread_create返回，而且我们已经看到pthread_join也使用了线程ID来指定等待哪个线程。pthread_self的作用是返回自身的线程ID。 12#include &lt;pthread.h&gt;pthread_t pthread_self(void); 5. pthread_detach 函数一个线程或者是可汇合（joinable），或者是脱离的（detached）。当一个可汇合的线程终止时，它的线程ID和退出状态将留存到另一个线程对它调用pthread_join的返回值中。脱离的线程终止时，所有相关资源都被释放，我们不能等待它们终止。如果一个线程需要知道另一个线程什么时候终止，那就最好保持第二个线程的可汇合状态。pthread_detach函数把指定的线程转变为脱离的状态。12#include &lt;pthread.h&gt;int pthread_detach(pthread_t th); 第一个线程例子我们的第一个线程的例子如下：12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;pthread.h&gt;void *thread_function(void *arg);char message[] = "Hello pthread!";int main() &#123; int res; pthread_t a_thread; void *thread_result; res = pthread_create(&amp;a_thread, NULL, thread_function, (void*)message); if (res != 0) &#123; perror("Thread creation failed."); exit(EXIT_FAILURE); &#125; printf("Waiting for thread to finish..\n"); res = pthread_join(a_thread, &amp;thread_result); if (res != 0) &#123; perror("Thread join failed."); exit(EXIT_FAILURE); &#125; printf("Thread joined, it returned %s\n", (char*)thread_result); printf("Message is now %s\n", message); exit(EXIT_SUCCESS);&#125;void *thread_function(void *arg) &#123; printf("Thread_function is running. Argument was %s\n", (char*)arg); sleep(3); strcpy(message, "Bye!"); pthread_exit("Thank you for the CPU time.");&#125; 首先，我们调用pthread_create创建了一个新线程，在调用pthread_create函数时，我们向其传递了一个函数指针thread_function，即新线程的执行函数，以及传递给该执行函数的参数message。创建新线程后，主线程通过pthread_join等待新线程的执行完毕。而新线程执行thread_function函数，修改全局数据message，然后退出线程并向主线程返回一个字符串。主线程等待新线程执行完毕后，获得新线程的返回值和修改后的全局数组。 编译运行上面的程序，得到以下输出： Waiting for thread to finish..thread_function is running. Argument was Hello pthread!Thread joined, it returned Thank you for the CPU time.Message is now Bye! 基于线程的并发服务器下面展示了基于线程的并发服务器的代码。整体结构类似于基于进程的设计。主线程不断地等待连接请求，然后创建一个新线程处理该请求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;void *thread_function(void *arg);int main() &#123; int listenfd, *connfdp; socklen_t server_len, client_len; struct sockaddr_in server_address; struct sockaddr_in client_address; pthread_t th; // 创建套接字 listenfd = socket(AF_INET, SOCK_STREAM, 0); // 命名套接字 server_address.sin_family = AF_INET; server_address.sin_addr.s_addr = htonl(INADDR_ANY); server_address.sin_port = htons(6240); server_len = sizeof(server_address); bind(listenfd, (struct sockaddr*)&amp;server_address, server_len); // 创建套接字队列 listen(listenfd, 5); // 接受客户的连接 while (1) &#123; printf("server waiting\n"); connfdp = malloc(sizeof(int)); client_len = sizeof(client_address); *connfdp = accept(listenfd, (struct sockaddr*)&amp;client_address, &amp;client_len); // 创建新线程 pthread_create(&amp;th, NULL, thread_function, connfdp); &#125;&#125;void *thread_function(void *arg) &#123; int connfd = *((int*)arg); printf("Thread_function is running. Argument was %d\n", connfd); pthread_detach(pthread_self()); free(arg); // 处理客户的请求 char ch; read(connfd, &amp;ch, 1); ch++; write(connfd, &amp;ch, 1); close(connfd); return NULL;&#125; 代码虽然较简单，但有几个地方值得我们重点关注一下。第一个问题是我们在调用pthread_create时，如何将已连接套接字描述符传递给新线程。最容易想到的方法如下： 12connfd = accept(listenfd, (struct sockaddr*)&amp;client_address, &amp;client_len);pthread_create(&amp;th, NULL, thread_function, &amp;connfd); 然后，在线程函数中引用这个指针变量，并将其赋值给一个局部变量。 1234void *thread_function(void *arg) &#123; int connfd = *((int*)arg); ...&#125; 然而，这个做法可能会带来线程安全的问题。如果赋值语句在下一个accept之前完成，那么线程函数中的局部变量connfd将得到正确值。如果赋值语句在下一个accept之后才完成，那么线程函数中的局部变量connfd就会得到下一次连接的描述符的值。这显然不是我们想要的结果。为了避免这种情况的出现，每次调用accept返回时，将返回的已连接套接字描述符存储在动态分配的内存中，这样无论线程函数中的赋值先于还是后于下一个accept完成，都不会出现线程安全的问题。 另一个问题是在线程函数中避免存储器资源泄漏。既然我们不显示式回收线程，我们就必须分离每个线程，使得它们在终止时存储器资源能够被回收。另外，还有一点需要提醒的，在线程函数中必须将主线程分配的动态内存释放了。 最后一个问题是如何关闭套接字描述符的问题。在基于进程的服务器中，我们在父进程和子进程两个位置都关闭了已连接套接字描述符。但在基于线程的服务器中，我们只需要在线程函数中关闭已连接套接字描述符，而不需要在主线程中关闭。在Linux系统中，每个文件或者套接字都有一个引用计数，引用计数在文件表项中维护，它是当前打开着的引用该文件或者套接字的描述符的个数。对于多进程服务器的情形，已连接套接字描述符在父进程和子进程间共享（也就是被复制），因此已连接套接字相关联的文件表项的访问计数值为2，故在父进程和子进程都需要执行close操作。而对于多线程服务器的情形，由于线程间具有相同的地址空间，套接字描述符并不进程复制操作，即已连接套接字描述符的计数值为1，故只需要在创建的新线程中执行一次close操作即可。 参考资料 深入理解计算机系统，第2版，机械工业出版社 Linux程序设计（第4版），Neil Matthew等著，人民邮电出版社，2010年 UNIX 网络编程卷1：套接字联网API（第三版）, W.Richard Stevens 等著 http://www.tuicool.com/articles/fiEfaa]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>多进程</tag>
        <tag>并发服务器</tag>
        <tag>并发</tag>
        <tag>多线程</tag>
        <tag>pthread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多进程并发服务器]]></title>
    <url>%2F%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%B9%B6%E5%8F%91%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[我们来考虑有多个客户同时连接一个服务器的情况。在前面的TCP套接字编程的例子中，我们已经看到，服务器程序在接受来自客户端的一个新连接时，会创建出一个新的套接字（已连接套接字），而原先的监听套接字则继续监听后面的连接请求。如果服务器不能立刻接受后来的连接，它们将被放到队列中以等待处理。原先的套接字仍然可用并且套接字的行为就像文件描述符，这一事实给我们提供了一种同时服务多个客户的方法。如果服务器调用fork为自己创建第二份副本，打开的套接字就将被新的子进程所继承。新的子进程可以和连接的客户进行通信，而主服务器进程可以继续接受以后的客户连接。 为了了解这是如何工作的，假设我们有两个客户端和一个服务器，服务器正在监听一个监听套接字（比如描述符3）上的连接请求。现在假设服务器接受了客户端1的连接请求，并返回一个已连接套接字（比如描述符4），如图1所示。图1：第一步：服务器接受客户端的连接请求 在接受连接请求后，服务器派生一个子进程，这个子进程获得服务器描述符表的完整拷贝。子进程关闭它的拷贝中的监听套接字（描述符为3），而父进程关闭它的已连接套接字（描述符为4）的拷贝，因为不需要这些描述符了。这就得到图2中的状态，其中的子进程正忙于为客户端提供服务。因为父、子进程中的已连接套接字描述符都指向同一个文件表表项，所以父进程关闭它的已连接套接字描述符的拷贝是至关重要的。否则，将永远不会释放已连接套接字描述符4的文件表表项，这会导致存储器资源泄漏并将最终消耗尽可用的存储器，使系统崩溃。 图2：第二步：服务器派生一个子进程为这个客户端服务 现在，假设在父进程为客户端1创建了子进程后，它接受一个新的客户端2的连接请求，并返回一个新的已连接套接字（比如描述符5），如图3所示。 图3：第三步：服务器接受另一个连接请求 然后父进程又派生另一个子进程，这个子进程利用已连接套接字（描述符为5）为它的客户端提供服务，如图4所示。 图4：服务器派生另一个子进程为新的客户端服务 此时，父进程继续等待下一个连接请求，而两个子进程正在并发地为它们各自的客户端提供服务。 例子我们现在来看下如何使用代码来实现多进程并发服务器。在编写代码时，有几点需要着重强调的： 因为我们创建子进程，但并不等待子进程的完成，所以安排服务器忽略SIGCHLD信号以避免出现僵尸进程。 父子进程必须关闭它们各自的已连接套接字拷贝，如上面所述，这样才能避免存储器资源泄漏。 因为套接字的文件表项中的引用计数，直到父子进程的已连接套接字描述符都关闭了，到客户端的连接才会终止。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main() &#123; int listenfd, connfd; int server_len, client_len; struct sockaddr_in server_address; struct sockaddr_in client_address; // 创建套接字 listenfd = socket(AF_INET, SOCK_STREAM, 0); // 命名套接字 server_address.sin_family = AF_INET; server_address.sin_addr.s_addr = htonl(INADDR_ANY); server_address.sin_port = htons(6240); server_len = sizeof(server_address); bind(listenfd, (struct sockaddr*)&amp;server_address, server_len); // 创建套接字队列 listen(listenfd, 5); // 避免出现僵尸进程 signal(SIGCHLD, SIG_IGN); // 接受客户连接 while (1) &#123; char ch; printf("server waiting\n"); client_len = sizeof(client_address); connfd = accept(listenfd, (struct sockaddr*)&amp;client_address, &amp;client_len); // 创建子进程，为这个客户创建一个子进程，并判断当前是运行是在父进程还是在子进程中 if (fork() == 0) &#123; // 在子进程中 close(listenfd); read(connfd, &amp;ch, 1); ch++; write(connfd, &amp;ch, 1); close(connfd); sleep(5); printf("subprocess, ch: %d, exit\n", ch); exit(0); &#125; else &#123; // 在父进程中 close(connfd); &#125; &#125;&#125; 运行上面的代码，然后使用客户端测试程序（见本文附录）来测试多进程并发服务器的实现。运行客户端程序： $ ./client3 &amp; ./client3 &amp; ./client3 客户端终端输出： char from server = Bchar from server = Bchar from server = B 同时，可以看到服务器程序输出： server waitingserver waitingserver waitingserver waitingsubprocess, ch: 66, exitsubprocess, ch: 66, exitsubprocess, ch: 66, exit 多进程优劣在父、子进程间共享状态信息，进程有一个非常清晰的模型：共享文件表，但是不共享用户地址空间。这样一来，一个进程不可能覆盖另一个进程的用户地址空间。这就消除了许多令人迷惑的错误。这是多进程实现并发服务器的优点。另一方面，独立的地址空间使用进程共享状态信息变得困难，为了共享信息，必须使用IPC（进程间通信机制）。多进程的另一个缺点是，它们往往比较慢，因为进程控制和IPC的开销都较高。 参考资料 深入理解计算机系统，第2版，机械工业出版社 Linux程序设计（第4版），Neil Matthew等著，人民邮电出版社，2010年 UNIX 网络编程卷1：套接字联网API（第三版）, W.Richard Stevens 等著 附：客户端测试代码 1234567891011121314151617181920212223242526272829303132333435363738394041/* Make the necessary includes and set up the variables. */#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;stdio.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main()&#123; int sockfd; int len; struct sockaddr_in address; int result; char ch = 'A';/* Create a socket for the client. */ sockfd = socket(AF_INET, SOCK_STREAM, 0);/* Name the socket, as agreed with the server. */ address.sin_family = AF_INET; address.sin_addr.s_addr = inet_addr("127.0.0.1"); address.sin_port = htons(6240); len = sizeof(address);/* Now connect our socket to the server's socket. */ result = connect(sockfd, (struct sockaddr *)&amp;address, len); if(result == -1) &#123; perror("oops: client3"); exit(1); &#125;/* We can now read/write via sockfd. */ write(sockfd, &amp;ch, 1); read(sockfd, &amp;ch, 1); printf("char from server = %c\n", ch); close(sockfd); exit(0);&#125;]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>多进程</tag>
        <tag>并发服务器</tag>
        <tag>多客户</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[监听套接字与已连接套接字]]></title>
    <url>%2F%E7%9B%91%E5%90%AC%E5%A5%97%E6%8E%A5%E5%AD%97%E4%B8%8E%E5%B7%B2%E8%BF%9E%E6%8E%A5%E5%A5%97%E6%8E%A5%E5%AD%97%2F</url>
    <content type="text"><![CDATA[监听套接字（listening socket）和已连接套接字（connected socket）之间的区别常会使很多人感到迷惑。本文简要描述一下这两者的区别。 为了说明监听套接字与已连接套接字的区别，我们先来看一下套接字在连接中的含义。从内核的角度来看，一个套接字就是通信的一个端点。一个连接由它两端的套接了地址唯一确定，这对套接字地址叫做套接字对（socket pair），由下列4元组来表示： （clientip:clientport， serverip:serverport） 其中，clientip 是客户端的IP地址，clientport 是客户端的端口，serverip 是服务器的IP地址，而 serverport 是服务器的端口。 图1：套接字对4元组示意图 上图1展示了一个套接字对4元组，即一个客户端与一个服务器之间的连接。在这个示例中，客户端套接字为 128.2.194.242:51234 服务器套接字地址为 114.113.200.133:80 给定客户端和服务器地址，客户和服务器之间的连接就由下列套接字对唯一确定了： (128.2.194.242:51234, 114.113.200.133:80) 在上面的例子中，客户端是发起连接请求的主动实体，服务器是等待来自客户端连接请求的被动实体。我们知道，socket函数可以创建一个套接字。默认情况，内核会认为socket函数创建的套接字是主动套接字（active socket），它存在于一个连接的客户端。而服务器调用listen函数告诉内核，该套接字是被服务器而不是客户端使用的，即listen函数将一个主动套接字转化为监听套接字（下文以 listenfd 表示）。监听套接字可以接受来自客户端的连接请求。服务器通过accept函数等待来自客户端的连接请求到达监听套接字 listenfd，并返回一个已连接套接字（下文以 connfd 表示）。利用 I/O 函数，这个 connfd 可以被用来与客户端进行通信。 上面就是监听套接字与已连接套接字的基本区别了。具体来说，监听套接字，是服务器作为客户端连接请求的一个端点，它被创建一次，并存在于服务器的整个生命周期。已连接套接字是客户端与服务器之间已经建立起来了的连接的一个端点，服务器每次接受连接请求时都会创建一次已连接套接字，它只存在于服务器为一个客户端服务的过程中。值得指出的是，无论是监听套接字，还是已连接套接字，都是只存在于服务器端。 图2：监听套接字与已连接套接字的角色 图2描绘了监听套接字和已连接套接字的角色。在第一步中，服务器调用accept，等待连接请求到达监听套接字 listenfd，假设该监听套接字的文字描述符为3（0，1，2已预留给标准文件使用）。在第二步中，客户端调用connect函数，发送一个连接请求到 listenfd。第三步，accept函数打开一个新的已连接套接字 connfd （假设套接字的文件描述符为4）,在 clientfd 和 connfd 之间建立连接，并且随后返回给服务器应用程序。客户端也从connect函数返回。此时，客户端和服务器就可以分别通过读写 clientfd 和 connfd 来回传送数据了。 参考资料《深入理解计算机系统》，第2版，机械工业出版社]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>socket</tag>
        <tag>监听套接字</tag>
        <tag>已连接套接字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP套接字编程入门]]></title>
    <url>%2FTCP%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[概述套接字（socket）是一种通信机制，凭借这种机制，客户与服务器的通信既可以在本地单机上进行，也可以跨网络进行。 图：基本的TCP客户/服务器应用程序的套接字函数 图中展示了一对TCP客户与服务器进程之间进行通信时调用套接字函数的交互情况。服务器首先启动，然后监听客户的连接。稍后客户试图连接服务器，客户连接成功后，客户给服务器发送请求，服务器处理请求，并且返回给客户一个响应。这个过程一直持续下去，直到客户关闭客户端的连接，接着服务器也关闭相应的服务器端的连接，接着服务器继续等待新的客户连接。 如何编写TCP套接字程序编写 TCP套接字程序，涉及具体的步骤： 创建套接字 命名套接字 创建套接字队列 服务器接受客户连接 客户请求连接服务器 发送和接收消息 关闭套接字 创建套接字可以使用系统调用socket来创建一个套接字并返回该套接字的文件描述符。 12#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol); 创建的套接字是一条通信线路的一个端点。 domaindomain参数指定哪种协议族，常见的协议族包括 AF_UNIX 和 AF_INET。AF_UNIX 用于通过文件系统实现的本地套接字（类似于pipe管道），AF_INET 用于网络套接字。 typetype参数指定这个套接字的通信类型，取值包括 SOCK_STREAM 和 SOCK_DGRAM。SOCK_STREAM 即流套接字，基于 TCP，提供可靠，有序的服务。SOCK_DGRAM 即数据报套接字，基于 UDP，提供不可靠，无序的服务。SOCK_STREAM 类型的套接字为本文讲述的重点。 protocolprotocol允许为套接字指定一种协议。对于 AF_UNIX 和 AF_INET，我们使用默认值即可。 以下代码创建一个 TCP套接字，domain使用 AF_INET，type 使用 SOCK_STREAM，protocol 协议使用默认的 0 值。 123456789101112131415#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main(int argc, char **argv)&#123; // 创建TCP套接字 int fd = socket(AF_INET, SOCK_STREAM, 0); if (fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, fd: %d\n", fd); exit(0);&#125; 命名套接字要想让创建的套接字可以被其他进程使用，那必须给该套接字命名。对套接字命名的意思是指将该套接字关联一个IP地址和端口号，可以使用系统调用bind来实现命名套接字。 12#include &lt;sys/socket.h&gt;int bind(int socket, const struct sockaddr *address, size_t address_len); bind系统调用把参数address中的地址分配给与文件描述符socket关联的套接字，地址结构的长度由参数address_len传递。 每种套接字域都有其自己的格式，对于 AF_INET 域来说，套接字地址由结构 socket_in来指定，它至少包含以下几个成员： 12345struct sockaddr_in &#123; short int sin_family; // AF_INET unsigned short int sin_port; // 端口号 struct in_addr sin_addr; // IP地址&#125;; 成员sin_port表示套接字的端口号。对于客户套接字，我们一般不需要指定套接字的端口号，而对于服务器套接字，我们需要指定套接字的端口号以便让客户正确向服务器发送数据。如果不需要指定端口号，可以将sin_port的值赋为0。 成员sin_addr表示套接字的地址，即机器的IP地址。如果我们没特别为套接字绑定IP地址，操作系统会为我们选择一个机器IP地址，这时sin_addr使用地址0.0.0.0，使用INADDR_ANY来表示这个地址常量。 对一个套接字进行命名的示例代码如下，该代码创建一个TCP套接字，并监听机器的6240端口。 123456789101112131415161718192021222324252627282930#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;int main(int argc, char **argv)&#123; // 创建套接字 int fd = socket(AF_INET, SOCK_STREAM, 0); if (fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, fd: %d\n", fd); // 命名套接字 struct sockaddr_in myaddr; memset((void *)&amp;myaddr, 0, sizeof(myaddr)); myaddr.sin_family = AF_INET; myaddr.sin_addr.s_addr = htonl(INADDR_ANY); myaddr.sin_port = htons(6240); if (bind(fd, (struct sockaddr *)&amp;myaddr, sizeof(myaddr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; printf("bind complete, port number: %d\n", ntohs(myaddr.sin_port)); exit(0);&#125; 输出： created socket, fd: 3bind complete, port number: 6240 htonl（host to network, long，长整数从主机字节序到网络字节序的转换）htons（host to network, short，短整数从主机字节序到网络字节序的转换）这两个函数用于字机字节序和网络字节序的转换。 创建套接字队列为了能够在套接字上接受进入的连接，服务器程序必须创建一个队列来保存未处理的请求。使用listen系统调用来完成这一工作。12#include &lt;sys/socket.h&gt;int listen(int socket, int backlog); Linux系统可能会对队列中可以容纳的未处理连接的最大数量做限制。为了遵守这个最大值限制，listen函数将队列长度设置为backlog参数的值。在套接字队列中，等待处理的进入连接的个数最多不能超过这个数字，再往后的连接将被拒绝，导致客户的连接请求失败。listen函数提供的这种机制允许当服务器繁忙时将后续的客户连接放入队列等待处理。backlog参数常用的值是5。 创建套接字队列的代码如下所示：123456789101112131415161718192021222324252627282930313233343536#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;int main(int argc, char **argv)&#123; // 创建套接字 int fd = socket(AF_INET, SOCK_STREAM, 0); if (fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, fd: %d\n", fd); // 命名套接字 struct sockaddr_in myaddr; memset((void *)&amp;myaddr, 0, sizeof(myaddr)); myaddr.sin_family = AF_INET; myaddr.sin_addr.s_addr = htonl(INADDR_ANY); myaddr.sin_port = htons(6240); if (bind(fd, (struct sockaddr *)&amp;myaddr, sizeof(myaddr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; printf("bind complete, port number: %d\n", ntohs(myaddr.sin_port)); // 创建套接字队列 if (listen(fd, 5) &lt; 0) &#123; perror("listen failed"); exit(1); &#125; exit(0);&#125; 服务器接受客户连接服务器创建并命名了套接字之后，就可以通过accept系统调用来等待客户建立对该套接字的连接。 12#include &lt;sys/socket.h&gt;int accept(int socket, struct sockaddr *address, socklen_t *address_len); accept只有在有客户尝试连接到由socket参数指定的套接字时才返回。即如果套接字队列中没有未处理的连接，accept将阻塞直到有客户建立连接为止。accept函数将创建一个新套接字来与该客户进行通信，并且返回新套接字的描述符。值得指出的是，新套接字的类型和服务器监听套接字类型是一样的，都是SOCK_STREAM套接字。这个新套接字也称之为已连接套接字（connected socket），原来的用作监听客户连接请求的套接字称之为监听套接字（listen socket）。关于监听套接字和已连接套接字的介绍可以参考《UNIX网络编程，卷1：套接字和联网API（第三版）》第二章对于TCP端口的介绍。 理解的关键是将TCP连接看作是一个4元组：{（IP1: port1）,（IP2: port2）}。 accept接受客户连接的示意代码如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;int main(int argc, char **argv)&#123; // 创建套接字 int server_fd = socket(AF_INET, SOCK_STREAM, 0); if (server_fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, server_fd: %d\n", server_fd); // 命名套接字 struct sockaddr_in server_addr; memset((void *)&amp;server_addr, 0, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = htonl(INADDR_ANY); server_addr.sin_port = htons(6240); if (bind(server_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; printf("bind complete, port number: %d\n", ntohs(server_addr.sin_port)); // 创建套接字队列 if (listen(server_fd, 5) &lt; 0) &#123; perror("listen failed"); exit(1); &#125; printf("socket listen, server_fd: %d\n", server_fd); // 服务器等待客户连接 const int MAXBUF = 256; char buffer[MAXBUF]; struct sockaddr_in client_addr; int client_addr_len = sizeof(client_addr); int client_fd; while (1) &#123; client_fd = accept(server_fd, (struct sockaddr *)&amp;client_addr, &amp;client_addr_len); printf("accept client, client fd: %d, ip: %s, port: %d\n", client_fd, inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port)); // 接收数据 int nbytes = read(client_fd, buffer, MAXBUF); printf("read from client, bytes: %d, data: %s\n", nbytes, buffer); // 关闭套接字 close(client_fd); &#125;&#125; 为了节省篇幅，在上面代码中，我们添加了服务器接收客户数据以及关闭套接字的代码，关于这两部分处理的解释会在下文介绍。 客户请求连接服务器客户套接字与服务器套接字之间建立连接，这一过程是调用connect系统调用来完成。 12#include &lt;sys/socket.h&gt;int connect(int socket, const struct sockaddr *address, site_t address len); 参数socket指定的客户套接字将连接到参数address指定的服务器套接字，address指向的结构的长度由参数address_len指定。调用connect后，会触发TCP三次握手过程，接着会建立起客户与服务器之间的连接。如果连接不能立刻建立，connect调用将阻塞一段不确定的时间。一旦超过这个时间，连接将被放弃，connect调用失败。 客户使用connect函数来连接服务器的代码示例如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;int main(int argc, char **argv) &#123; // 创建套接字 int client_fd = socket(AF_INET, SOCK_STREAM, 0); if (client_fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, client_fd: %d\n", client_fd); // 命名套接字，客户端由内核选择IP地址和端口 struct sockaddr_in client_addr; memset((char *)&amp;client_addr, 0, sizeof(client_addr)); client_addr.sin_family = AF_INET; client_addr.sin_addr.s_addr = htonl(INADDR_ANY); client_addr.sin_port = htons(0); if (bind(client_fd, (struct sockaddr *)&amp;client_addr, sizeof(client_addr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; // 构造服务器的地址 const char *server = "127.0.0.1"; // 服务器IP struct sockaddr_in server_addr; memset((char*)&amp;server_addr, 0, sizeof(server_addr)); server_addr.sin_family = AF_INET; inet_aton(server, &amp;server_addr.sin_addr); server_addr.sin_port = htons(6240); // 请求连接服务器 int result = connect(client_fd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)); if (result &lt; 0) &#123; perror("connect failed"); &#125; // 发送数据 const int MAXBUF = 256; char buffer[MAXBUF] = "hello tcp"; int nbytes = write(client_fd, buffer, 10); printf("write to server, bytes: %d, data: %s\n", nbytes, buffer); // 关闭套接字 close(client_fd);&#125; 在上面代码中，为方便起见，我们使用了inet_aton来构造服务器的套接字地址，inet_aton的作用是将一个IP地址字符串转换为一个32位的网络字节序的IP地址。 利用套接字收发数据客户与服务器建立连接后，就可以利用read和write系统调用来传输数据了。 例如，在上面的代码中，包含了客户向服务器发送消息的方法：123const int MAXBUF = 256;char buffer[MAXBUF] = "hello tcp";int nbytes = write(client_fd, buffer, 10); 如果服务器要读取客户发送的数据，则可以使用类似下面的代码： 123const int MAXBUF = 256;char buffer[MAXBUF];int nbytes = read(client_fd, buffer, MAXBUF); 关闭连接我们可以通过调用close函数来终止服务器与客户的套接字连接，就如同对底层文件描述符进行关闭一样。对套接字的使用结束后，我们应该正确关闭套接字以避免套接字资源的泄漏。 12#include &lt;unistd.h&gt;int close(int socket); 上面的程序中，我们都看到了close函数的调用。 对于多进程并发服务器的情形，使用 close只是导致套接字描述符的引用计数值减1。如果引用计数值仍大于0，这个close调用并不引发TCP终止连接的四次挥手过程。对于父进程与子进程共享已连接套接字（connected socket）的并发服务器来说，这正是所期望的。如果我们确实想在某个TCP连接上发送一个FIN，那么可以改用shutdown函数以代替close。关于shutdown与close的区别，我们再会在另一篇文章描述。 参考资料 Linux程序设计（第4版），Neil Matthew等著，人民邮电出版社，2010年 UNIX 网络编程卷1：套接字联网API（第三版）, W.Richard Stevens 等著 https://www.cs.rutgers.edu/~pxk/417/notes/sockets/index.html]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>socket</tag>
        <tag>套接字</tag>
        <tag>bind</tag>
        <tag>connect</tag>
        <tag>close</tag>
        <tag>accept</tag>
        <tag>listen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UDP 套接字编程入门]]></title>
    <url>%2FUDP-%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[概述在使用TCP编写的应用程序和使用UDP编写的应用程序之间存在一些本质差异，其原因在于这两个传输层之间的差别：UDP是无连接不可靠的数据报协议，不同于TCP提供的面向连接的可靠字节流。从资源的角度来看，相对来说UDP套接字开销较小，因为UDP套接字不需要维持网络连接，而且因为无需花费时间来连接连接，所以UDP套接字的速度也较快。因为UDP提供的是不可靠服务，所以数据可能会丢失。如果数据对于我们来说非常重要，就需要小心编写UDP客户程序，以检查错误并在必要时重传。实际上，UDP套接字在局域网中是非常可靠的。 图：UDP 客户 / 服务器程序使用的套接字函数 上图展示了客户与服务器使用UDP套接字进行通信的过程。在UDP套接字程序中，客户不需要与服务器建立连接，而只管直接使用sendto函数给服务器发送数据报。同样的，服务器不需要接受来自客户的连接，而只管调用recvfrom函数，等待来自某个客户的数据到达。 如何编写UDP套接字程序编写UDP套接字应用程序，涉及一定的步骤： 创建套接字 命名套接字 在服务器端，等待客户的消息 在客户端，发送客户消息 关闭套接字 步骤1：创建套接字可以使用系统调用socket来创建一个套接字并返回该套接字的文件描述符。 12#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol); 创建的套接字是一条通信线路的一个端点。 domaindomain参数指定哪种协议族，常见的协议族包括 AF_UNIX 和 AF_INET。AF_UNIX 用于通过文件系统实现的本地套接字，AF_INET 用于网络套接字。 typetype参数指定这个套接字的通信类型，取值包括 SOCK_STREAM 和 SOCK_DGRAM。SOCK_STREAM 即流套接字，基于 TCP，提供可靠，有序的服务。SOCK_DGRAM 即数据报套接字，基于 UDP，提供不可靠，无序的服务。SOCK_DGRAM 类型的套接字为本文讲述的重点。 protocolprotocol允许为套接字指定一种协议。对于 AF_UNIX 和 AF_INET，我们使用默认值即可。 以下代码创建一个 UDP 套接字，domain使用 AF_INET，type 使用 SOCK_DGRAM，protocol 协议使用默认的 0 值。 123456789101112131415#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main(int argc, char **argv)&#123; int fd = socket(AF_INET, SOCK_DGRAM, 0); if (fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, fd: %d\n", fd); exit(0);&#125; 步骤2：命名套接字要想让创建的套接字可以被其他进程使用，那必须给该套接字命名。对套接字命名的意思是指将该套接字关联一个IP地址和端口号，可以使用系统调用bind来实现命名套接字。 12#include &lt;sys/socket.h&gt;int bind(int socket, const struct sockaddr *address, size_t address_len); bind系统调用把参数address中的地址分配给与文件描述符socket关联的套接字，地址结构的长度由参数address_len传递。 每种套接字域都有其自己的格式，对于 AF_INET 域来说，套接字地址由结构 socket_in来指定，它至少包含以下几个成员： 12345struct sockaddr_in &#123; short int sin_family; // AF_INET unsigned short int sin_port; // 端口号 struct in_addr sin_addr; // IP地址&#125;; 成员sin_port表示套接字的端口号。对于客户套接字，我们一般不需要指定套接字的端口号，而对于服务器套接字，我们需要指定套接字的端口号以便让客户正确向服务器发送数据。如果不需要指定端口号，可以将sin_port的值赋为0。 成员sin_addr表示套接字的地址，即机器的IP地址。如果我们没特别为套接字绑定IP地址，可以让操作系统选择一个，即sin_addr使用地址0.0.0.0，使用INADDR_ANY来表示这个地址常量。 对一个套接字进行命名的示例代码如下： 123456789101112131415161718192021222324252627282930#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;int main(int argc, char **argv)&#123; // 创建套接字 int fd = socket(AF_INET, SOCK_DGRAM, 0); if (fd &lt; 0) &#123; perror("cannot create socket"); return 0; &#125; printf("created socket, fd: %d\n", fd); // 命名套接字 struct sockaddr_in myaddr; memset((void *)&amp;myaddr, 0, sizeof(myaddr)); myaddr.sin_family = AF_INET; myaddr.sin_addr.s_addr = htonl(INADDR_ANY); myaddr.sin_port = htons(6240); if (bind(fd, (struct sockaddr *)&amp;myaddr, sizeof(myaddr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; printf("bind complete, port number: %d\n", ntohs(myaddr.sin_port)); exit(0);&#125; htonl（host to network, long，长整数从主机字节序到网络字节序的转换）htons（host to network, short，短整数从主机字节序到网络字节序的转换）这两个函数用于字机字节序和网络字节序的转换。 步骤三a：服务器接收客户消息不同于TCP提供的面向连接的可靠字节流协议，UDP 是无连接不可靠的数据报协议。服务器不接受来自客户的连接，而只管调用recvfrom系统调用等待客户的数据到达。recvfrom的声明如下： 12#include &lt;sys/socket.h&gt;int recvfrom(int socket, void *buffer, size_t length, int flags, struct sockaddr *src_addr, socklen_t *src_len); recvfrom的参数说明如下。 socket：创建的套接字描述符 buffer：指向输入缓冲区的指针 length：缓冲区大小 flags：在本文中，可以将 flags 置为0即可 src_addr：指向客户套接字地址的指针 src_len：地址长度 recvfrom的返回值为读入数据的长度。 我们来看下服务器是如何使用recvfrom来接收客户的数据。123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netdb.h&gt;#include &lt;arpa/inet.h&gt;#define BUFSIZE 2048#define SERVICE_PORT 6240int main(int argc, char **argv)&#123; struct sockaddr_in myaddr; // 服务器地址 struct sockaddr_in remaddr; // 客户地址 socklen_t addrlen = sizeof(remaddr); int recvlen; int fd; unsigned char buf[BUFSIZE]; // 创建套接字 if ((fd = socket(AF_INET, SOCK_DGRAM, 0)) &lt; 0) &#123; perror("cannot create socket\n"); return 0; &#125; // 命名套接字 memset((char *)&amp;myaddr, 0, sizeof(myaddr)); myaddr.sin_family = AF_INET; myaddr.sin_addr.s_addr = htonl(INADDR_ANY); myaddr.sin_port = htons(SERVICE_PORT); if (bind(fd, (struct sockaddr *)&amp;myaddr, sizeof(myaddr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; // 服务器接收客户消息 while (1) &#123; printf("waiting on port %d\n", SERVICE_PORT); recvlen = recvfrom(fd, buf, BUFSIZE, 0, (struct sockaddr *)&amp;remaddr, &amp;addrlen); printf("received %d bytes\n", recvlen); if (recvlen &gt; 0) &#123; buf[recvlen] = 0; printf("received message: \"%s\"\n", buf); &#125; &#125;&#125; 服务器在循环里面不断调用recvfrom函数，接收客户的数据，并输出接收到的客户数据的长度和具体内容。 步骤三b：客户向服务器发达消息UDP是无连接的，故客户可以直接向服务器发送消息而不需要建立连接。客户使用sendto系统调用向服务器发送消息： 12#include &lt;sys/socket.h&gt;int sendto(int socket, const void *buffer, size_t length, int flags, const struct sockaddr *dest_addr, socklen_t dest_len); sendto函数的参数说明如下： socket：创建的套接字描述符 buffer：输出缓冲区的指针 length：缓冲区大小 flags：正常应用中，flags一般设置为0 dest_addr：指向服务器套接字地址的指针 dest_len：地址长度 以下代码演示客户如何使用UDP套接字向服务器发送消息：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netdb.h&gt;#include &lt;sys/socket.h&gt;#define BUFLEN 2048#define MSGS 5#define SERVICE_PORT 6240int main(void)&#123; struct sockaddr_in myaddr, remaddr; int fd, i, slen = sizeof(remaddr); const char *server = "127.0.0.1"; // 服务器IP char buf[BUFLEN]; // 创建套接字 if ((fd=socket(AF_INET, SOCK_DGRAM, 0))==-1) printf("socket created\n"); // 命名套接字 memset((char *)&amp;myaddr, 0, sizeof(myaddr)); myaddr.sin_family = AF_INET; myaddr.sin_addr.s_addr = htonl(INADDR_ANY); myaddr.sin_port = htons(0); // 无须指定特定的端口 if (bind(fd, (struct sockaddr *)&amp;myaddr, sizeof(myaddr)) &lt; 0) &#123; perror("bind failed"); return 0; &#125; // 构造服务器的地址 memset((char *) &amp;remaddr, 0, sizeof(remaddr)); remaddr.sin_family = AF_INET; remaddr.sin_port = htons(SERVICE_PORT); if (inet_aton(server, &amp;remaddr.sin_addr)==0) &#123; fprintf(stderr, "inet_aton() failed\n"); exit(1); &#125; // 客户向服务器发送消息 for (i=0; i &lt; MSGS; i++) &#123; printf("Sending packet %d to %s port %d\n", i, server, SERVICE_PORT); sprintf(buf, "This is packet %d", i); if (sendto(fd, buf, strlen(buf), 0, (struct sockaddr *)&amp;remaddr, slen)==-1) perror("sendto"); &#125; // 关闭套接字 close(fd); return 0;&#125; 上面代码中，为方便起见，我们使用了inet_aton来构造服务器的套接字地址，inet_aton的作用是将一个字符串IP地址转换为一个32位的网络字节序的IP地址。 执行客户程序，可以看到以下输出： $ ./sendSending packet 0 to 127.0.0.1 port 6240Sending packet 1 to 127.0.0.1 port 6240Sending packet 2 to 127.0.0.1 port 6240Sending packet 3 to 127.0.0.1 port 6240Sending packet 4 to 127.0.0.1 port 6240 服务器程序则看到以下输出： $ ./recvwaiting on port 6240received 16 bytesreceived message: “This is packet 0”waiting on port 6240received 16 bytesreceived message: “This is packet 1”waiting on port 6240received 16 bytesreceived message: “This is packet 2”waiting on port 6240received 16 bytesreceived message: “This is packet 3”waiting on port 6240received 16 bytesreceived message: “This is packet 4”waiting on port 6240 步骤四：关闭套接字操作系统为每个套接字分配了一个文件描述符，为了让操作系统回收该文件描述符，可以使用 close 系统调用： 1close(fd); 参考资料 Linux程序设计（第4版），Neil Matthew等著，人民邮电出版社，2010年 UNIX 网络编程卷1：套接字联网API（第三版）, W.Richard Stevens 等著 https://www.cs.rutgers.edu/~pxk/417/notes/sockets/udp.html]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>套接字</tag>
        <tag>Socket</tag>
        <tag>UDP</tag>
        <tag>sendto</tag>
        <tag>recvfrom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 编程概述]]></title>
    <url>%2FWindows-%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本文对windows编程中涉及的一些基本概念作一个简单的概述。 事件驱动Windows 应用程序基于事件驱动模型，windows应用程序启动后会一直等待事件的发生。常见的事件包括，键盘某个按键被按下，鼠标单击，窗口按钮被点击，窗口可见，等等。 消息与消息队列Windows 操作系统使用消息来通知应用程序事件的发生。当一个事件发生时，windows 会为该事件所对应的应用程序发送一条消息，然后，这条消息就被加入到应用程序的消息队列中。每个应用程序都有属于自己的消息队列，消息队列包含了windows操作系统向该应用程序发送的所有消息。 消息循环应用程序在一个消息循环里面不断地检查消息队列，如果消息队列里面存在消息，则将其取出来，并交由对应的窗口过程处理。消息循环的代码如下所示：1234while(::GetMessage(&amp;msg, 0, 0, 0)) &#123; ::TranslateMessage(&amp;msg); ::DispatchMessage(&amp;msg);&#125; 窗口过程窗口过程实质是一个函数，用于处理消息队列中的消息。每个窗口对应一个窗口过程（在创建窗口时指定窗口对应的窗口过程），但一个窗口过程可能对应于多个窗口，即多个窗口共用一个窗口过程。一个窗口过程中可能包含的消息处理代码如下：1234case WM_KEYDOWN: if (wParam == VK_ESCAPE) ::DestroyWindow(MainWindowHandle); return 0;]]></content>
      <categories>
        <category>Windows编程</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>窗口过程</tag>
        <tag>消息队列</tag>
        <tag>消息</tag>
        <tag>消息循环</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解UNIX的I/O模型]]></title>
    <url>%2F%E5%9B%BE%E8%A7%A3UNIX%E7%9A%84I-O%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[一、简述UNIX系统将所有的外部设备都看作一个文件来看待，所有打开的文件都通过文件描述符来引用。文件描述符是一个非负整数，它指向内核中的一个结构体。当打开一个现有文件或创建一个新文件时，内核向进程返回一个文件描述符。而对于一个socket的读写也会有相应的文件描述符，称为socketfd（socket描述符）。在UNIX系统中，I/O输入操作（例如标准输入或者套接字的输入）通常包含以下两个不同的阶段： 等待数据准备好 从内核向进程复制数据 例如对于套接字的输入，第一步是等待数据从网络中到达，当所等待的数据到达时，数据被复制到内核中的缓冲区。第二步则是把数据从内核缓冲区复制到应用进程的缓冲区。根据在这两个不同阶段处理的不同，可以将I/O模型划分为以下五种类型： 阻塞式I/O模型 非阻塞式I/O模型 I/O复用 信号驱动式I/O 异步I/O 二、I/O模型为简单起见，我们以UDP套接字中的recvfrom函数作为系统调用来说明I/O模型。recvfrom函数类似于标准的read函数，它的作用是从指定的套接字中读取数据报。recvfrom会从应用进程空间运行切换到内核空间中运行，一段时间后会再切换回来。有关recvfrom函数的介绍，可以参考本文的参考资料3。 2.1 阻塞式I/O模型阻塞式I/O模型可以说是最简单的I/O模型。 图1：阻塞式I/O模型 图1是阻塞式I/O模型的示意图，阅读此图须注意箭头的方向，沿着剪头方向顺时针阅读。图1中，应用进程调用recvfrom，然后切换到内核空间中运行，直到数据报到达且被复制到应用进程缓冲区中才返回。我们说进程从调用recvfrom开始到它返回的整段时间内是被阻塞的。recvfrom成功返回后，应用进程开始处理数据报。 2.2 非阻塞式I/O模型进程把一个套接字设置为非阻塞是指，在等待I/O数据时，进程并不阻塞，如果数据还没准备好，则直接返回一个错误。 图2：非阻塞式I/O模型 图2是非阻塞I/O模型的示图。在前两次调用recvfrom时由于数据报没准备好，因此内核马上返回一个系统调用错误。第3次调用recvfrom时，数据报已准备好，数据报被复制到应用进程的缓冲区，接着recvfrom成功返回。当一个应用进程像这样不断对一个非阻塞描述符循环调用recvfrom时，我们称之为轮询。应用进程会持续轮询内核，以确定某个操作是否就绪。轮询操作会消耗大量的CPU时间。 2.3 I/O复用模型我们常用的select和poll函数使用了I/O复用模型。我们以select为例说明I/O复用模型的特点。 图3：I/O复用模型 图3是I/O复用模型的示意图。当我们调用select函数时，将会阻塞于此函数，等待数据报套接字变为可读。当等待的多个套接字中的其中一个或者多个变得可读时，我们调用recvfrom把数据报复制到应用进程缓冲区。比较图3与图1，I/O复用模型好像没什么优势，而且应用进程为了获取数据报，还得增加了一个额外的select系统调用。不过I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。 2.4 信号驱动式I/O模型信号驱动I/O模型用得比较少，图4是该模型的示意图。 图4：信号驱动式I/O模型 为了使用该I/O模型，需要开启套接字的信号驱动I/O功能，并通过sigaction系统调用安装一个信号处理函数。sigaction函数立即返回，我们的进程继续工作，即进程没有被阻塞。当数据报准备好时，内核会为该进程产生一个SIGIO信号，这样我们可以在信号处理函数中调用recvfrom读取数据报，也可以在主循环中读取数据报。无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达期间不被阻塞。 2.5 异步I/O模型异步I/O模型的工作机制是，启动某个操作，并让内核在整个操作（包括等待数据和将数据从内核复制到用户空间）完成后通知应用进程。 图5：异步I/O模型 图5是异步I/O模型的示意图。我们调用aio_read函数，告诉内核，当整个I/O操作完成后通知我们。该系统调用立即返回，而在等待I/O完成期间，应用进程不会被阻塞。当I/O完成（包括数据从内样复制到用户进程）后，内核会产生一个信号通知应用进程，应用进程对数据报进行处理。异步I/O模型与信号驱动式I/O的区别在于：信号驱动式I/O在数据报准备好时就通知应用进程，应用进程还需要将数据报从内核复制到用户进程缓冲区；而异步I/O模型则是整个操作完成才通知应用进程，应用进程在整个操作期间都不会被阻塞。 2.6 各种I/O模型的比较图6：5种I/O模型的比较 从图6可以看到，前四种I/O模型的主要区别在于第一个阶段，它们的第二个阶段是一样的：在数据从内核复制到应用进程的缓冲区期间，进程会被阻塞于recvfrom系统调用。而异步I/O模型则是整个操作完成内核才通知应用进程。 三、同步I/O和异步I/OPOSIX标准将同步I/O和异步I/O定义为： 同步I/O操作：导致请求进程阻塞，直到I/O操作完成。 异步I/O操作：不导致请求进程阻塞。 根据上述两个定义，本文介绍的前面四种模型，包括阻塞式I/O，非阻塞式I/O，I/O复用和信号驱动式I/O模型都是同步I/O模型，因为其中真正的I/O操作（recvfrom）将阻塞进程。只有异步I/O模型才符合POSIX标准的异步I/O定义。 四、生活中的类比例子以生活中钓鱼为例子（例子参考了参考资料2），来说明各种I/O模型的不同，例子中的等待鱼上钩对应于上文中的等待数据，拉竿操则作对应于上文的将数据从内核复制到用户空间。 有A，B，C，D，E五个人在钓鱼。A使用了最古老的鱼竿，所以开始钓鱼后，就一直守着，直接鱼上钩了再拉竿；B由于着急想知道有没鱼上钩，所以隔一会就看一次鱼竿看有没鱼上钩，直到看到鱼上钩后，再拉竿；C同时使用了N支鱼竿来钩鱼，然后等着，只要有其中一支鱼竿有鱼上钩，就将对应的鱼竿拉起来；D的鱼竿比较高级，当有鱼上钩后，会发出警报提示，所以D开始钓鱼后不用一直守着，一旦鱼竿发出警报，D再回来拉竿即可；E为了更省事，直接雇个佣人给他钓鱼，当佣人钓起鱼后，再通知E去取鱼即可。 五、参考资料 Unix网络编程，卷1：套接字联网API，第三版，W. Richard Stevens著 http://blog.csdn.net/historyasamirror/article/details/5778378 http://pubs.opengroup.org/onlinepubs/009695399/functions/recvfrom.html]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>epoll</tag>
        <tag>select</tag>
        <tag>IO多路复用</tag>
        <tag>IO模型</tag>
        <tag>异步</tag>
        <tag>非阻塞</tag>
        <tag>同步</tag>
        <tag>阻塞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Protocol Buffers 体验日志]]></title>
    <url>%2FGoogle-Protocol-Buffers-%E4%BD%93%E9%AA%8C%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[最近在看RPC的相关的实现，其中google的 gRPC 默认使用 Protocol Buffers 来作为其接口描述语言（IDL，Interface Definition Language）以及使用 Protocol Buffers 实现消息的序列化与反序列化。因此，很有必要对 protocol buffers 作个初步的了解。下文以 protobuf 来简称 protocol buffer。 安装由于自己使用的是 Mac OS，因此，这里讲述 Mac OS 下如何安装 protobuf。可以在以下链接下载最新版本的 protobuf ，下载链接。可以直接下载其源代码，然后进行编译安装。 源代码下载后，进入其目录，然后执行：1234$ ./configure$ make$ make check$ sudo make install 编译好后，使用以下命令来测试是否已成功安装：12$ protoc --versionlibprotoc 3.2.0 说明已成功安装。 编写 .proto 文件.proto 文件定义了消息的格式，一个常见的.proto 文件如下所示： 12345message Person &#123; required int32 id = 1; required string name = 2; optional string email = 3;&#125; 其中，关键字 required 表示这个字段是必须的，optional 表示这个字段是可选的。int32 表示这个字段是 32 位整型类型，string 表示这个字段是字符串类型。 编写完 .proto 文件后，使用 protoc 来编译。例如，将上面的消息格式保存为 person.proto，进入这个文件所在的目录，执行以下命令： 1protoc -I=. --python_out=. person.proto (上面的 python_out 前面有两个 - ，该参数的意义是指定生写的 Python 版本的输出目录，具体参数的含义可以使用 protoc -h 来查看)这样，便在目录下生成 person_pb2.py 文件。 person_pb2.py 文件对 Person 类进行了定义。接下来，可以使用 Person 类提供的方法对来 Person 对象进行序列化和反序列化的操作。 序列化与反序列化我们编写 test.py 来测试 protobuf 的使用。将上面生成的 person_pb2.py 文件拷贝到 test.py 文件所在的目录。 123456789101112131415161718192021#!/usr/bin/env python# -*- coding:utf-8 -*-import person_pb2if __name__ == '__main__': # 构造对象 person = person_pb2.Person() person.id = 12345 person.name = 'leo' person.email = 'haolee@live.com' print person print '********************' # 序列化 serial_str = person.SerializeToString() print serial_str print '********************' # 反序列化 person2 = person_pb2.Person().FromString(serial_str) print person2.id, person2.name, person2.email print '********************' 输出：12345678id: 12345name: &quot;leo&quot;email: &quot;haolee@live.com&quot;********************�`leohaolee@live.com********************12345 leo haolee@live.com******************** 参考资料 https://github.com/google/protobuf/wiki https://github.com/google/protobuf https://github.com/google/protobuf/releases]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>Protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP的连接管理]]></title>
    <url>%2FTCP%E7%9A%84%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[简述TCP是面向连接的协议，TCP把连接作为最基本的抽象。每一条TCP连接唯一地被通信两端的两个端点所确定。那么，TCP连接的端点是什么呢？TCP连接的端点又叫套接字（socket），根据TCP协议的规定，端口号拼接到IP地址即构成了套接字，即 套接字 socket = （IP地址：端口号） 这样一来，TCP连接可以以下式子表示 TCP连接 ::= {socket1, socket2} = {（IP1: port1）,（IP2: port2）} 在面向连接通信中，连接的建立和释放是必不可少的过程。TCP连接的建立采用客户服务器方式，主动发起连接建立的应用进程叫做客户，而被动等待连接的应用进程叫做服务器。本文主要讲述TCP是如何管理连接的建立和连接的释放的。 TCP的连接建立图1：三次握手建立TCP连接 图1画出了TCP连接建立的过程。假定图中左端是客户A，右端是服务器B，一开始时，两端都处于CLOSED（关闭）状态。图中的方框分别是端点所处的状态。1）服务器进程准备好接受外来的连接，这通常是通过调用socket，bind，listen这三个函数来完成，我们称之为被动打开（passive open）。然后服务器进程就处于LISTEN状态，等待客户的连接请求，如有，则作出响应。2）客户通过调用connect发起主动打开（active open），向服务器发出连接请求报文段，请求中的首部的同步位SYN = 1，同时选择一个初始序号seq = x。TCP规定，SYN报文段不能携带数据，则要消耗一个序号。这时，TCP客户进入SYN-SEND（同步已发送）状态。 TCP规定，首部中序号字段值是本报文段所发送数据的第一个字节的序号。 3）服务器收到客户端连接请求后，必须确认（ACK）客户的SYN报文段。在确认报文段中，把SYN和ACK位都置为1，确认号为ack = x + 1，同时也为自己选择一个初始序号seq = y。请注意，这个报文段也不能携带数据，但同样要消耗掉一个序号。这时，TCP服务器进入SYN-RCVD（同步收到）状态。 TCP规定，若确认号 = N，则表明：到序号 N - 1为止的所有数据都已正确收到。 4）客户在收到服务器的确认后，还要向服务器进程给出确认。确认报文段的ACK置1，确认号ack = y + 1，而自己的序号seq = x + 1。TCP规定，这个报文段可以携带数据，也可以不携带数据，如果不携带数据，下一个数据报文段的序号仍是seq = x + 1。这时，TCP连接已经建立，客户进入ESTABLISHED（已建立连接）状态。5）服务器收到客户的确认后，也进入ESTABLISHED状态。在上述的建立连接的过程中，前后发送了三个报文段，因此TCP建立连接的过程也称之为三次握手（three-way handshake）。 为什么需要三次握手为什么客户在收到服务器的确认后，还要向服务器发送一次确认呢？这主要是为了防止已失效的连接请求报文段突然又传送到了服务器，因而发生错误。考虑一种情况，客户发出连接请求后，但因连接请求报文丢失而未收到确认。于是客户再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接。客户共发送了两个连接请求报文段，其中第一个丢失，第二个到达了服务器。没有“已失效的连接请求报文段”。现假定一种异常情况。即客户发出的第一个连接请求报文段并没有丢失，而是在某些网络结点长时间滞留了，以致延误到连接释放以后的某个时间才到达服务器。本来这是一个早已失效的报文段，但服务器收到此失效的连接请求后，就误认为是客户又一次发出一次新的连接请求。于是就向客户发出确认报文段，同意建立连接。假定不采用三次握手，那么只要服务器发出确认，新的连接就建立了。由于现在客户端并没有发出建立连接的请求，因此不会理睬服务器的确认，也不会向服务器发送数据。但服务器却以为新的连接已经建立了，并一直等待客户发送数据。服务器的许多资源就这样白浪费了。采用三次握手的办法可以防止上述现象的发生。例如刚才的情况下，客户不会向服务器的确认发出确认，由于服务器收不到确认，就知道客户并没有要求建立连接。 TCP的连接释放TCP建立一个连接需要三个报文段，释放一个连接却需要四个报文段。图2：TCP释放连接的过程 数据传输结束后，通信的双方可以释放连接。数据传输结束后的客户A和服务器B都处于ESTABLISHED状态，然后进入释放连接的过程。1）A的应用进程先发出释放连接报文段，并停止发送数据，主动关闭TCP连接。A把连接释放报文段首部FIN置1，其序号为seq = u。这时A进入FIN-WAIT-1（终止等待1）状态。2）B收到连接释放报文段后即发出确认确认号为ack = u + 1，而自己的序号为seq = v。然后B就进入CLOSE-WAIT（关闭等待）状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时的TCP连接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍接收。3）A收到来自B的确认后，就进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。4）若B已经没有要向A发送的数据，其应用进程就通知TCP释放 连接。这时B发出的连接释放报文段FIN = 1，还必须重复上次已发送过的确认号ack = u + 1。假定B的序号为w（在半关闭期间B可能又发送了一些数据）。这时B就进入了LAST-ACK（最后确认）状态，等待A的确认。5）A收到了的连接释放报文段后，必须对此发出确认。其确认号为ack = w + 1，而自己的序号为seq = u + 1。然后进入到TIME-WAIT（时间等待）状态。请注意，现在TCP连接还没有释放掉。必须经过时间等待计时器（TIME-WAIT timer）设置的时间 2MSL后，A才进入到CLOSED状态。时间MSL叫做最长报文段寿命（Maximum Segment Lifetime）。6）B只要收到A发出的确认，就进入CLOSED状态。我们注意到，B结束TCP连接的时间要比A早一些。由于释放TCP连接的过程需要发送四个报文段，因此释放连接的过程也称之为四次握手。 TIME_WAIT状态上述释放连接的过程中，A在TIME-WAIT状态必须等待2MSL，才进入CLOSED状态，上面也提到，这个MSL是报文段的最长寿命。那么MSL的真实含义是什么呢？MSL是任何IP数据报能够在网络中存活的最长时间。我们知道这个时间是有限的，因为每个数据报含有一个称为跳限（hop limit）的8位字段，它的最大值是255，即最大为255跳。尽管这是一个跳数限制而不是真正的时间限制，我们仍然假设：具有最大跳限的数据报在网络中存在的时间不可能超过MSL秒。任何TCP实现都必须为MSL选择一个值。RFC 1122的建议值为2分钟，对于现在的网络，MSL = 2分钟可能太长了，故一些实现采用30秒的值，这意味着，TIME-WAIT状态的持续时间在1分钟到4分钟之间。为什么客户在TIME-WAIT状态必须2MSL的时间呢？这有两个理由：1）可靠地实现TCP全双工连接的终止客户A最后一个ACK报文段可能丢失，这样服务器B处于LAST-ACK状态而收不到确认。接下来B会超时重传FIN + ACK报文段，而A就能在2MSL时间内收到这个重传的FIN + ACK报文段，并再重传一次确认，并重新启动2MSL计时器。最后，A和B都正常进入CLOSED状态。如果A在发送完最后一个ACK报文段后立即释放连接，那么就无法收到B重传的FIN + ACK报文段，因而也不会再发送一次确认报文段，这样B就无法按照正常步骤进入CLOSED状态。2）防止“已失效的连接请求报文段”出现在本连接中客户A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都会网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。 后记有这样一道面试题，对于应用程序来说，什么情况下会出现大量 TIME_WAIT 的状态？TIME_WAIT 出现的原因可以参考上面的详细解析，从上面的描述我们也可以知道，TIME_WAIT 的出现是一般是客户主动关闭 TCP 连接而出现的，即出现在客户端机器，服务端机器一般不会出现 TIME_WAIT 状态。那么，在什么情况下，客户端机器会大量出现关闭 TCP 连接呢？记得在网易的时候，我曾经为处理用户连接实时语音服务的日志开发过一个日志补全进程 audiolog。用户每天累积的实时语音日志达1千多万条，audiolog 进程在每天的固定时间（01:30）扫描语音日志，发现某些日志如果存在字段缺失的情况，audiolog 会通过发送 HTTP 请求查询并补全字段信息，audiolog会在短时间内扫描完所有的日志并运行完毕。我们知道 HTTP 是基于 TCP 的，这就导致短时间内 audiolog 断开大量的 TCP 连接，导致大量 TIME_WAIT 状态的出现。 图3：audiolog 机器出现 TIME_WAIT 状态统计 由图3可以看到，audiolog 所在的机器，在01:30这个时间点，出现了大量的 TIME_WAIT 状态，这个时间点正是 audiolog 进程运行的时间点，这跟上述分析出现大量 TIME_WAIT 状态的原因是致的。 上述的情况，都是在客户端出现 TIME_WAIT 的情况。另外，如果服务端主动关闭客户端非法请求或者处理长时间不跳跃连接，也会在服务端机器出现 TIME_WAIT 的状态。 参考资料 《计算机网络（第6版）》，谢希仁著 《UNIX网络编程 卷1：套接字联网API（第3版）》，W. Richard Stevens等著]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>三次握手</tag>
        <tag>四次挥手</tag>
        <tag>TIME_WAIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Sentinel 学习笔记]]></title>
    <url>%2FRedis-Sentinel-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[概述Redis Sentinel 是用来实现 Redis 高可用的一套解决方案。Redis Sentinel 由两个部分组成：由一个或者多个 Sentinel 实例组成 Sentinel 系统；由一个主 Redis 服务器（master redis）和多个从 Redis 服务器（slave redis）组成主从备份的 Redis 系统。Sentinel 系统本身是一个分布式的系统，它的作用是监视 redis 服务器，在 master redis 下线时，自动将某个 slave redis 上升为新的主服务器。Redis 系统由 master redis 处理客户端的命令请求，slave redis 作为主服务器的备份而存在。Redis Sentinel 的系统架构如图 1 所示。 图1：Redis Sentinel 架构图 假设某个时刻，原来的 master redis 进入下线状态，Sentinel 系统就会监控到这个 master redis 已下线，接下来，Sentinel 系统就会进行故障转移操作： 首先，Sentinel 系统会挑选出原来的其中一个 slave redis，让这个 slave redis 上升为 master redis。 之后，Sentinel 系统会向其他所有的 slave redis 发送新的复制命令，让它们成为新 master redis 的 slave redis。 另外，Sentinel 系统还会继续监控已下线的 redis，当它重新上线时，会将设置为新 master redis 的 slave redis。 需要指出的是，Redis Sentinel 只是一种主从备份的高可用方案，它的目的是支持 Redis 提供高可用的服务，但 Redis Sentinel 系统本身并没有对 Redis 的承载能力进行扩容。理论上说，如果 slave redis 并没有对外提供读写服务，Redis Sentinel 的负载能力与单点 Redis 的负载能力是一样的。有些同学使用了 Redis Sentinel 方案后，可以有效降低 Redis 单机版的负载问题，这种想法是不现实的。这是因为在 Redis Sentinel 系统中，还是只有一个 redis 服务器在提供服务，其负载能力还是受单个 redis 服务器的限制。如果需要对 redis 的承载能力进行扩容，可以使用其他的分布式 redis 解决方案，例如 Codis，或者 Twemproxy。 实验为了进一步了解 Redis Sentinel 系统如何实现系统高可用的原理，我们进行以下实验。 实验环境为了操作和描述的简单起见，我们搭建的 Redis Sentinel 系统，只包含一个 master redis 和一个 slave redis，以及两个 sentinel 实例。生产环境下，建议 slave redis 至少部署两个，sentinel 实例则至少三个。服务器部署环境如图 2 所示。 图2：Redis Sentinel 部署环境 操作系统：CentOS 6.6 Redis 版本：release 3.2.8 实验过程1. 搭建 Redis 系统在 redis 官网（https://redis.io/download）下载 redis后对其解压，然后修改 redis 目录下的配置文件。Master redis 使用的配置 redis.conf 如下（限于篇幅只列出关键的几个配置，其他配置使用默认值即可）：12345678# master redis 绑定的IP地址bind 192.168.174.137# 监听的端口，使用默认的 2679port 6379# 以后台进程运行daemonize yes# 日志输出位置，便查看实验结果logfile &quot;/home/lihao/redis-stable/logs/redis.log&quot; Slave redis 使用的配置 redis.conf 如下：12345678910# slave redis 绑定的IP地址bind 192.168.174.143# 监听的端口，使用默认的 2679port 6379# 以后台进程运行daemonize yes# 日志输出位置，便查看实验结果logfile &quot;/home/lihao/redis-stable/logs/redis.log&quot;# 作为 master redis 的 slave redisslaveof 192.168.174.137 6379 修改好配置后，先后启动 master redis 和 slave redis： 启动命令：1$ ./src/redis-server redis.conf 这样， 主从备份的 Redis 系统就搭建好了。 2. 搭建 Sentinel 系统第一个 sentinel 实例的配置 sentinel.conf 配置如下： 123456789# 绑定的IP地址bind 192.168.174.137# 监听的端口port 26379# 监控的 master redis 信息，其中的数字 2 表示判断# master redis 客观下线所需要的 sentinel 实例数量sentinel monitor mymaster 192.168.174.137 6379 2# 判断 master redis 主观下线的时长sentinel down-after-milliseconds mymaster 30000 第二个 sentinel 实例的配置如下： 123456789# 绑定的IP地下bind 192.168.174.143# 监听的端口port 26379# 监控的 master redis 信息，其中的数字 2 表示判断# master redis 客观下线所需要的 sentinel 实例数量sentinel monitor mymaster 192.168.174.137 6379 2# 判断 master redis 主观下线的时长sentinel down-after-milliseconds mymaster 30000 然后分别在两台机器上执行以下命令启动 sentinel 实例。1./src/redis-sentinel sentinel.conf 这样，Sentinel 系统也搭建完成了。 3. 测试 Redis 系统主从切换在机器 192.168.174.137 上执行以下命令，获取其主从角色信息：1$ ./src/redis-cli -h 192.168.174.137 info Replication 得到以下输出信息，表明 192.168.174.137 上的 redis 是 master 角色。1234# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.174.143,port=6379,state=online,offset=15,lag=1 机器 192.168.174.143 上的 redis 则是 slave 角色：1234# Replicationrole:slavemaster_host:192.168.174.137master_port:6379 对 192.168.174.137 master redis 执行下线命令操作：1192.168.174.137:6379&gt; shutdown 等待一会儿，可以看到 192.168.174.143 变成了 master 的角色。123# Replicationrole:masterconnected_slaves:0 192.168.174.137 的 redis 重新上线后，可以看到其已变成新的 master redis 的 slave redis。 1234# Replicationrole:slavemaster_host:192.168.174.143master_port:6379 实验小结从实验过程可以看到，Sentinel 系统会对已下线的 master redis 执行故障转移操作。这个故障操作实际包含三个步骤：（1）在已下线 master redis 属下的所有 slave redis 里面，挑选出一个 slave redis ，并将其转换为 master redis 角色。例如，上述实验中，将 192.168.174.143 的 redis 转换为 master 的角色。（2）让其他的所有的 slave redis 改为复制新的 master redis。（3）将已下线的 master redis 设置为新 master redis 的 slave redis。在上述实验中，192.168.174.137 上下线的 redis 在重新上线后，变成了 192.168.174.143 上 redis 的 slave redis。 总结Redis Sentinel 系统内部通过执行故障转移操作，保证 Redis 系统在 master redis 下线后，仍然可以继续提供对外服务，从而达到系统高可用的目的。 参考资料 Redis 设计与实现，黄健宏著，机械工业出版社，2015年 https://redis.io/topics/sentinel http://www.cnblogs.com/LiZhiW/p/4851631.html https://segmentfault.com/a/1190000002680804 http://blog.csdn.net/moyu_2012/article/details/47856949 http://www.oschina.net/p/codis/ https://github.com/twitter/twemproxy/]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>Sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper的Znode剖析]]></title>
    <url>%2FZooKeeper%E7%9A%84Znode%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[在ZooKeeper中，节点也称为znode。由于对于程序员来说，对zk的操作主要是对znode的操作，因此，有必要对znode进行深入的了解。ZooKeeper采用了类似文件系统的的数据模型，其节点构成了一个具有层级关系的树状结构。例如，图1展示了zk节点的层级树状结构。 图1：ZooKeeper的层级树状结构 图1中，根节点 / 包含了两个字节点 /module1，/module2，而节点 /module1 又包含了三个字节点 /module1/app1，/module1/app2，/module1/app3。在zk中，节点以绝对路径表示，不存在相对路径，且路径最后不能以 / 结尾（根节点除外）。 类型根据节点的存活时间，可以对节点划分为持久节点和临时节点。节点的类型在创建时就被确定下来，并且不能改变。持久节点的存活时间不依赖于客户端会话，只有客户端在显式执行删除节点操作时，节点才消失。临时节点的存活时间依赖于客户端会话，当会话结束，临时节点将会被自动删除（当然也可以手动删除临时节点）。利用临时节点的这一特性，我们可以使用临时节点来进行集群管理，包括发现服务的上下线等。ZooKeeper规定，临时节点不能拥有子节点。 持久节点使用命令create可以创建一个持久节点。 create /module1 module1 这样，便创建了一个持久节点/module1，且其数据为”module1”。 临时节点使用create命令，并加上-e参数，可以创建一个临时节点。 create -e /module1/app1 app1 这样，便创建了一个临时节点 /module1/app1，数据为”app1”。关闭会话，然后输入命令： get /module1/app1 可以看到有以下提示，说明临时节点已经被删除。 Node does not exist: /module1/app1 顺序节点ZooKeeper中还提供了一种顺序节点的节点类型。每次创建顺序节点时，zk都会在路径后面自动添加上10位的数字（计数器），例如 &lt; path &gt;0000000001，&lt; path &gt;0000000002，……这个计数器可以保证在同一个父节点下是唯一的。在zk内部使用了4个字节的有符号整形来表示这个计数器，也就是说当计数器的大小超过2147483647时，将会发生溢出。顺序节点为节点的一种特性，也就是，持久节点和临时节点都可以设置为顺序节点。这样一来，znode一共有4种类型：持久的、临时的，持久顺序的，临时顺序的。 使用命令create加上-s参数，可以创建顺序节点，例如， create -s /module1/app app 输出： Created /module1/app0000000001 便创建了一个持久顺序节点 /module1/app0000000001。如果再执行此命令，则会生成节点 /module1/app0000000002。如果在create -s再添加-e参数，则可以创建一个临时顺序节点。 节点的数据在创建节点时，可以指定节点中存储的数据。ZooKeeper保证读和写都是原子操作，且每次读写操作都是对数据的完整读取或完整写入，并不提供对数据进行部分读取或者写入的操作。以下命令创建一个节点/module1/app2，且其存储的数据为app2。 create /module1/app2 app2 ZooKeeper虽然提供了在节点存储数据的功能，但它并不将自己定位为一个通用的数据库，也就是说，你不应该在节点存储过多的数据。Zk规定节点的数据大小不能超过1M，但实际上我们在znode的数据量应该尽可能小，因为数据过大会导致zk的性能明显下降。如果确实需要存储大量的数据，一般解决方法是在另外的分布式数据库（例如redis）中保存这部分数据，然后在znode中我们只保留这个数据库中保存位置的索引即可。 节点的属性每个znode都包含了一系列的属性，通过命令get，我们可以获得节点的属性。 get /module1/app2app2cZxid = 0x20000000ectime = Thu Jun 30 20:41:55 HKT 2016mZxid = 0x20000000emtime = Thu Jun 30 20:41:55 HKT 2016pZxid = 0x20000000ecversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0 版本号对于每个znode来说，均存在三个版本号： dataVersion数据版本号，每次对节点进行set操作，dataVersion的值都会增加1（即使设置的是相同的数据）。 cversion子节点的版本号。当znode的子节点有变化时，cversion 的值就会增加1。 aclVersionACL的版本号，关于znode的ACL（Access Control List，访问控制），可以参考 参考资料1 有关ACL的描述。 以数据版本号来说明zk中版本号的作用。每一个znode都有一个数据版本号，它随着每次数据变化而自增。ZooKeeper提供的一些API例如setData和delete根据版本号有条件地执行。多个客户端对同一个znode进行操作时，版本号的使用就会显得尤为重要。例如，假设客户端C1对znode /config写入一些配置信息，如果另一个客户端C2同时更新了这个znode，此时C1的版本号已经过期，C1调用setData一定不会成功。这正是版本机制有效避免了数据更新时出现的先后顺序问题。在这个例子中，C1在写入数据时使用的版本号无法匹配，使得操作失败。图2描述了这个情况。 图2：使用版本号来阻止并行操作的不一致性 事务ID对于zk来说，每次的变化都会产生一个唯一的事务id，zxid（ZooKeeper Transaction Id）。通过zxid，可以确定更新操作的先后顺序。例如，如果zxid1小于zxid2，说明zxid1操作先于zxid2发生。需要指出的是，zxid对于整个zk都是唯一的，即使操作的是不同的znode。 cZxidZnode创建的事务id。 mZxidZnode被修改的事务id，即每次对znode的修改都会更新mZxid。 图3：Zxid在客户端重连中的作用 在集群模式下，客户端有多个服务器可以连接，当尝试连接到一个不同的服务器时，这个服务器的状态要与最后连接的服务器的状态要保持一致。Zk正是使用zxid来标识这个状态，图3描述了客户端在重连情况下zxid的作用。当客户端因超时与S1断开连接后，客户端开始尝试连接S2，但S2延迟于客户端所识别的状态。然而，S3的状态与客户端所识别的状态一致，所以客户端可以安全连接上S3。 时间戳包括znode的创建时间和修改时间，创建时间是znode创建时的时间，创建后就不会改变；修改时间在每次更新znode时都会发生变化。 以下命令创建了一个 /module2 节点。 create /module2 module2Created /module2 通过 get 命令，可以看到 /module2的 ctime和mtime均为Sat Jul 02 11:18:32 CST 2016。 get /module2module2cZxid = 0x2ctime = Sat Jul 02 11:18:32 CST 2016mZxid = 0x2mtime = Sat Jul 02 11:18:32 CST 2016pZxid = 0x2cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 0 修改 /module2，可以看到 ctime 没有发生变化，mtime已更新为最新的时间。 set /module2 module2_1cZxid = 0x2ctime = Sat Jul 02 11:18:32 CST 2016mZxid = 0x3mtime = Sat Jul 02 11:18:50 CST 2016pZxid = 0x2cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 0 参考资料 http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkDataModel http://java.globinch.com/enterprise-services/zookeeper/apache-zookeeper-explained-tutorial-cases-zookeeper-java-api-examples/# 《ZooKeeper分布式过程协同技术详解》，Flavio Junqueira等著，谢超等译]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>Znode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper命令简介]]></title>
    <url>%2FZooKeeper%E5%91%BD%E4%BB%A4%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[ZooKeeper的客户端包括Java版本和C语言版本。使用Java版本连接zk的命令以下： 1bin/zkCli.sh -server ip:port 执行此命令，客户端成功连接上zk，会有类似以下的输出，其中，包括“Welcome to ZooKeeper!”的欢迎语，以及其他一些连接的信息等。 1234567891011121314151617lihaodeMacBook-Pro:bin lihao$ ./zkCli.sh -server 127.0.0.1:2182Connecting to 127.0.0.1:21822016-06-29 07:51:39,679 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT2016-06-29 07:51:39,682 [myid:] - INFO [main:Environment@100] - Client environment:host.name=192.168.31.1072016-06-29 07:51:39,682 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_772016-06-29 07:51:39,684 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation……2016-06-29 07:51:39,686 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=127.0.0.1:2182 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@531d72caWelcome to ZooKeeper!2016-06-29 07:51:39,715 [myid:] - INFO [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1032] - Opening socket connection to server 127.0.0.1/127.0.0.1:2182. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2016-06-29 07:51:39,797 [myid:] - INFO [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@876] - Socket connection established to 127.0.0.1/127.0.0.1:2182, initiating session[zk: 127.0.0.1:2182(CONNECTING) 0] 2016-06-29 07:51:39,830 [myid:] - INFO [main-SendThread(127.0.0.1:2182):ClientCnxn$SendThread@1299] - Session establishment complete on server 127.0.0.1/127.0.0.1:2182, sessionid = 0x155996605050000, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null 连接成功后，便可以使用命令与zk服务进行交互。 helphelp命令会输出zk支持的所有命令。1234567891011121314151617181920212223[zk: 127.0.0.1:2182(CONNECTED) 0] helpZooKeeper -server host:port cmd argsstat path [watch]set path data [version]ls path [watch]delquota [-n|-b] pathls2 path [watch]setAcl path aclsetquota -n|-b val pathhistory redo cmdnoprintwatches on|offdelete path [version]sync pathlistquota pathrmr pathget path [watch]create [-s] [-e] path data acladdauth scheme authquit getAcl pathclose connect host:port ls查看指定路径下包含的节点12[zk: localhost:2181(CONNECTED) 2] ls /[zookeeper] create创建一个节点，例如： 12[zk: localhost:2181(CONNECTED) 3] create /zk mydataCreated /zk 以上命令创建一个/zk节点，且其内容为 “myData” get显示指定路径下节点的信息，例如，我们检查一下上面的/zk节点最否创建成功12345678910111213[zk: localhost:2181(CONNECTED) 4] get /zkmydatacZxid = 0xb59ctime = Thu Jun 30 11:13:24 CST 2016mZxid = 0xb59mtime = Thu Jun 30 11:13:24 CST 2016pZxid = 0xb59cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 可以看到/zk节点的内容为”myData”，且输出包含了znode的其他信息。有关各个字段的具体含义，请参见了本博客对znode的介绍。 set设置节点的内容，例如：12345[zk: localhost:2181(CONNECTED) 6] set /zk &quot;anotherData&quot;……[zk: localhost:2181(CONNECTED) 7] get /zk&quot;anotherData&quot;…… delete删除一个节点，例如：123[zk: localhost:2181(CONNECTED) 8] delete /zk[zk: localhost:2181(CONNECTED) 9] get /zkNode does not exist: /zk 以上就是zk客户端最常用的几个命令，从这几个命令我们也可以看到zk提供的API设计的简单。 四字母命令ZooKeeper提供了多个由4个字母构成的命令，可以使用nc或者telnet来使用这些命令。例如： 1telnet 127.0.0.1 2181 成功连接zk后，输入conf会看到以下输出12345678clientPort=2181dataDir=D:\Soft\zookeeper-3.4.6\data\version-2dataLogDir=D:\Soft\zookeeper-3.4.6\data\version-2tickTime=2000maxClientCnxns=60minSessionTimeout=4000maxSessionTimeout=40000serverId=0 或者使用nc来向zk发送4字母命令，例如：1echo conf | nc 192.168.229.161 2181 其他常用的四字母命令如下表格所示： 表格：ZooKeeper提供的四字母命令 命令 描述 conf zk服务配置的详细信息 stat 客户端与zk连接的简要信息 srvr zk服务的详细信息 cons 客户端与zk连接的详细信息 mntr zk服务目前的性能状况 wchs watch的简要信息 wchc watch的详细信息，客户端 -&gt; watch的映射，线上环境要小心使用 wchp watch的详细信息, znode -&gt; 客户端的映射，线上环境要小心使用 例如，mntr 命令的输出：12345678910111213141516171819echo mntr | nc 192.168.229.161 2181zk_version 3.4.6-1569965, built on 02/20/2014 09:09 GMTzk_avg_latency 0zk_max_latency 565zk_min_latency 0zk_packets_received 95353zk_packets_sent 95713zk_num_alive_connections 3zk_outstanding_requests 0zk_server_state leaderzk_znode_count 20zk_watch_count 12zk_ephemerals_count 9zk_approximate_data_size 1465zk_open_file_descriptor_count 37zk_max_file_descriptor_count 65535zk_followers 2 - 只有leader进程才有此项输出zk_synced_followers 2 - 只有leader进程才有此项输出zk_pending_syncs 0 - 只有leader进程才有此项输出 参考资料 http://www.programering.com/a/MTOxUjMwATI.html https://zookeeper.apache.org/doc/r3.4.8/zookeeperStarted.html]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper的安装与部署]]></title>
    <url>%2FZooKeeper%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[本文讲述如何安装和部署ZooKeeper。 一、系统要求ZooKeeper可以运行在多种系统平台上面，表1展示了zk支持的系统平台，以及在该平台上是否支持开发环境或者生产环境。 表1：ZooKeeper支持的运行平台 系统 开发环境 生产环境 Linux 支持 支持 Solaris 支持 支持 FreeBSD 支持 支持 Windows 支持 不支持 MacOS 支持 不支持 ZooKeeper是用Java编写的，运行在Java环境上，因此，在部署zk的机器上需要安装Java运行环境。为了正常运行zk，我们需要JRE1.6或者以上的版本。对于集群模式下的ZooKeeper部署，3个ZooKeeper服务进程是建议的最小进程数量，而且不同的服务进程建议部署在不同的物理机器上面，以减少机器宕机带来的风险，以实现ZooKeeper集群的高可用。ZooKeeper对于机器的硬件配置没有太大的要求。例如，在Yahoo!内部，ZooKeeper部署的机器其配置通常如下：双核处理器，2GB内存，80GB硬盘。 二、下载可以从 https://zookeeper.apache.org/releases.html 下载ZooKeeper，目前最新的稳定版本为 3.4.8 版本，用户可以自行选择一个速度较快的镜像来下载即可。 三、目录下载并解压ZooKeeper软件压缩包后，可以看到zk包含以下的文件和目录： 图1：ZooKeeper软件的文件和目录 bin目录zk的可执行脚本目录，包括zk服务进程，zk客户端，等脚本。其中，.sh是Linux环境下的脚本，.cmd是Windows环境下的脚本。 conf目录配置文件目录。zoo_sample.cfg为样例配置文件，需要修改为自己的名称，一般为zoo.cfg。log4j.properties为日志配置文件。 libzk依赖的包。 contrib目录一些用于操作zk的工具包。 recipes目录zk某些用法的代码示例 四、单机模式ZooKeeper的安装包括单机模式安装，以及集群模式安装。 单机模式较简单，是指只部署一个zk进程，客户端直接与该zk进程进行通信。在开发测试环境下，通过来说没有较多的物理资源，因此我们常使用单机模式。当然在单台物理机上也可以部署集群模式，但这会增加单台物理机的资源消耗。故在开发环境中，我们一般使用单机模式。但是要注意，生产环境下不可用单机模式，这是由于无论从系统可靠性还是读写性能，单机模式都不能满足生产的需求。 4.1 运行配置上面提到，conf目录下提供了配置的样例zoo_sample.cfg，要将zk运行起来，需要将其名称修改为zoo.cfg。打开zoo.cfg，可以看到默认的一些配置。 tickTime时长单位为毫秒，为zk使用的基本时间度量单位。例如，1 tickTime是客户端与zk服务端的心跳时间，2 tickTime是客户端会话的超时时间。tickTime的默认值为2000毫秒，更低的tickTime值可以更快地发现超时问题，但也会导致更高的网络流量（心跳消息）和更高的CPU使用率（会话的跟踪处理）。 clientPortzk服务进程监听的TCP端口，默认情况下，服务端会监听2181端口。 dataDir无默认配置，必须配置，用于配置存储快照文件的目录。如果没有配置dataLogDir，那么事务日志也会存储在此目录。 4.2 启动在Windows环境下，直接双击zkServer.cmd即可。在Linux环境下，进入bin目录，执行命令1./zkServer.sh start 这个命令使得zk服务进程在后台进行。如果想在前台中运行以便查看服务器进程的输出日志，可以通过以下命令运行：1./zkServer.sh start-foreground 执行此命令，可以看到大量详细信息的输出，以便允许查看服务器发生了什么。 使用文本编辑器打开zkServer.cmd或者zkServer.sh文件，可以看到其会调用zkEnv.cmd或者zkEnv.sh脚本。zkEnv脚本的作用是设置zk运行的一些环境变量，例如配置文件的位置和名称等。 4.3 连接如果是连接同一台主机上的zk进程，那么直接运行bin/目录下的zkCli.cmd（Windows环境下）或者zkCli.sh（Linux环境下），即可连接上zk。直接执行zkCli.cmd或者zkCli.sh命令默认以主机号 127.0.0.1，端口号 2181 来连接zk，如果要连接不同机器上的zk，可以使用 -server 参数，例如： 1bin/zkCli.sh -server 192.168.0.1:2181 五、集群模式单机模式的zk进程虽然便于开发与测试，但并不适合在生产环境使用。在生产环境下，我们需要使用集群模式来对zk进行部署。 注意在集群模式下，建议至少部署3个zk进程，或者部署奇数个zk进程。如果只部署2个zk进程，当其中一个zk进程挂掉后，剩下的一个进程并不能构成一个quorum的大多数。因此，部署2个进程甚至比单机模式更不可靠，因为2个进程其中一个不可用的可能性比一个进程不可用的可能性还大。 5. 1 运行配置在集群模式下，所有的zk进程可以使用相同的配置文件（是指各个zk进程部署在不同的机器上面），例如如下配置： 12345678tickTime=2000dataDir=/home/myname/zookeeperclientPort=2181initLimit=5syncLimit=2server.1=192.168.229.160:2888:3888server.2=192.168.229.161:2888:3888server.3=192.168.229.162:2888:3888 initLimitZooKeeper集群模式下包含多个zk进程，其中一个进程为leader，余下的进程为follower。当follower最初与leader建立连接时，它们之间会传输相当多的数据，尤其是follower的数据落后leader很多。initLimit配置follower与leader之间建立连接后进行同步的最长时间。 syncLimit配置follower和leader之间发送消息，请求和应答的最大时间长度。 tickTimetickTime则是上述两个超时配置的基本单位，例如对于initLimit，其配置值为5，说明其超时时间为 2000ms * 5 = 10秒。 server.id=host:port1:port2其中id为一个数字，表示zk进程的id，这个id也是dataDir目录下myid文件的内容。host是该zk进程所在的IP地址，port1表示follower和leader交换消息所使用的端口，port2表示选举leader所使用的端口。 dataDir其配置的含义跟单机模式下的含义类似，不同的是集群模式下还有一个myid文件。myid文件的内容只有一行，且内容只能为1 - 255之间的数字，这个数字亦即上面介绍server.id中的id，表示zk进程的id。 注意如果仅为了测试部署集群模式而在同一台机器上部署zk进程，server.id=host:port1:port2配置中的port参数必须不同。但是，为了减少机器宕机的风险，强烈建议在部署集群模式时，将zk进程部署不同的物理机器上面。 5.2 启动假如我们打算在三台不同的机器 192.168.229.160，192.168.229.161，192.168.229.162上各部署一个zk进程，以构成一个zk集群。三个zk进程均使用相同的 zoo.cfg 配置： 12345678tickTime=2000dataDir=/home/myname/zookeeperclientPort=2181initLimit=5syncLimit=2server.1=192.168.229.160:2888:3888server.2=192.168.229.161:2888:3888server.3=192.168.229.162:2888:3888 在三台机器dataDir目录（ /home/myname/zookeeper 目录）下，分别生成一个myid文件，其内容分别为1，2，3。然后分别在这三台机器上启动zk进程，这样我们便将zk集群启动了起来。 5.3 连接可以使用以下命令来连接一个zk集群： 1bin/zkCli.sh -server 192.168.229.160:2181,192.168.229.161:2181,192.168.229.162:2181 成功连接后，可以看到如下输出： 12345678910112016-06-28 19:29:18,074 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=192.168.229.160:2181,192.168.229.161:2181,192.168.229.162:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@770537e4Welcome to ZooKeeper!2016-06-28 19:29:18,146 [myid:] - INFO [main-SendThread(192.168.229.162:2181):ClientCnxn$SendThread@975] - Opening socket connection to server 192.168.229.162/192.168.229.162:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2016-06-28 19:29:18,161 [myid:] - INFO [main-SendThread(192.168.229.162:2181):ClientCnxn$SendThread@852] - Socket connection established to 192.168.229.162/192.168.229.162:2181, initiating session2016-06-28 19:29:18,199 [myid:] - INFO [main-SendThread(192.168.229.162:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server 192.168.229.162/192.168.229.162:2181, sessionid = 0x3557c39d2810029, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 192.168.229.160:2181,192.168.229.161:2181,192.168.229.162:2181(CONNECTED) 0] 图2：客户端连接zk集群的输出日志 从日志输出可以看到，客户端连接的是192.168.229.162:2181进程（连接上哪台机器的zk进程是随机的），客户端已成功连接上zk集群。 参考资料 http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html 《ZooKeeper分布式系统开发实战》课程，主讲人：玺感 《ZooKeeper分布式过程协同技术详解》，Flavio Junqueira等著，谢超等译 百度百科有关quorum的解释，http://baike.baidu.com/link?url=pqWrzgH-_VhMLnscR1iRTpPjovfyhxG-8Qs9HxGutiGi5bhnA_lX_pmabLQ-3MiDeigcHRFMYSbFg90RAYVAta 《Zookeeper 安装和配置》，http://coolxing.iteye.com/blog/1871009]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper概览]]></title>
    <url>%2FZooKeeper%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[一、简介ZooKeeper是一个高性能，分布式的，开源分布式应用协调服务。它提供了简单原始的功能，分布式应用可以基于它实现更高级的服务，比如同步，集群管理，命名空间，配置维护等。ZooKeeper使用了我们熟悉的文件树状结构作为自己的数据模型，因此是易于使用的。 我们知道，由于资源竞争等因素的影响，维护分布式服务的正常运行是一项极具挑战的事情。图1形象地说明了道路交通中没有信号灯的指挥协调导致出现交通阻塞混乱的情况出现。对于分布式系统来说，如果没有一个协调者的角色对各个分布式应用进行协调，分布式系统同样会出现一团糟的情况。 图1：混乱的交通 ZooKeeper背后的动机则是为了简化分布式应用的开发流程，并提供更加敏捷健壮的方案，以此来协调分布式系统中的各个应用服务。可以使用交通信号灯来形象比喻zk在分布式系统中的作用。 图2：交通信号灯对交通进行协调 二、特点总体来说，ZooKeeper具有以下的特点。 1. 简单ZooKeeper使用了类似文件系统的数据模型，即每个节点按照层级的关系组成了ZooKeeper的命名空间。一个ZooKeeper的命名空间示例如下： 图: ZooKeeper的层级命名空间 上图包含了五个节点（在ZooKeeper中称为znode，znode并不等于机器，实际中我们可以用znode表示分布式应用的一个进程），可以用它的路径来表示这些节点，例如/app1，/app1/p_2，等。znode节点可以包含数据，也可以不包含。如果一个znode包含数据，那么数据被存储为字节数组（byte array）。字节数组的格式取决于我们自己的解析，使用可以使用Protocol Buffers，MessagePack等序列化。 为了操作znode，你仅需要使用以下的API。 create /path data创建一个名为/path的znode节点，并包含数据data delete /path删除名为/path的znode exists /path检查是否否在名为/path的znode getData /path返回名为/path的znode的数据 setData /path data设置名为/path的znode的数据为data getChildren /path返回/path节点下所有子节点列表 2. 多点图: ZooKeeper服务架构 作为分布式应用的协调者，ZooKeeper要保证自己本身的高可用特性。为了达到这个目的，ZooKeeper跟其他分布式应用一样，被设计成多点的服务，以避免单点故障。客户端连接其中一个ZooKeeper进程，不同的ZooKeeper进程会进行同步的操作。 3. 有序ZooKeeper使用一个数字来标识一个更新操作，以反映ZooKeeper事务的顺序特性。基于这一特性，ZooKeeper可以实现更新高级的抽象操作，例如同步等。 4. 快速无论是读写操作，ZooKeeper可以实现快速响应。尤其对于读操作，ZooKeeper可以达到更快的响应速度。 三、保证ZooKeeper简单且快速响应，它的目的是为构建复杂的分布式应用而服务，例如为分布式系统提供同步管理，集群管理等。为了达到这一目的，ZooKeeper提供以下的保证： 顺序一致性ZooKeeper提供了顺序保障，这意味着同一个会话中的请求会以FIFO（先进先出）顺序执行。通常，一个客户端只打开一个会话，因此客户端请求将全部以FIFO顺序执行。如果客户端拥有多个并发的会话，FIFO顺序在多个会话之间未必能够保持。而即使同一个客户端中连贯的会话并不重叠，也未必能够保证FIFO顺序。 原子性一个操作或者成功，或者失败，不会出现部分成功的结果 单一系统映像虽然ZooKeeper本身以多点的形式运行，但对于客户端来说，无论连接的是哪个server进程，看到的是同样的视图。 可靠性一旦更新操作被执行，更新的操作就会被持久化，直到被下一次更新覆盖。 时效性客户端看到的系统视图在一定的时间范围内总是最新的。 参考资料 https://zookeeper.apache.org/doc/trunk/zookeeperOver.html 《ZooKeeper分布式过程协同技术详解》，Flavio Junqueira等著，谢超等译]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何理解Nginx, WSGI, Flask之间的关系]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Nginx-WSGI-Flask%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[概览之前对 Nginx，WSGI（或者 uWSGI，uwsgi），Flask(或者 Django），这几者的关系一存存在疑惑。通过查阅了些资料，总算把它们的关系理清了。总括来说，客户端从发送一个 HTTP 请求到 Flask 处理请求，分别经过了 web服务器层，WSGI层，web框架层，这三个层次。不同的层次其作用也不同，下面简要介绍各层的作用。 图1：web服务器，web框架与 WSGI 的三层关系 Web服务器层对于传统的客户端 - 服务器架构，其请求的处理过程是，客户端向服务器发送请求，服务器接收请求并处理请求，然后给客户端返回响应。在这个过程中，服务器的作用是： 接收请求 处理请求 返回响应 Web服务器是一类特殊的服务器，其作用是主要是接收 HTTP 请求并返回响应。提起 web服务器大家都不会陌生，常见的 web服务器有 Nginx，Apache，IIS等。在上图1的三层结构中，web服务器是最先接收用户请求的，并将响应结果返回给用户。 Web框架层Web框架的作用主要是方便我们开发 web应用程序，HTTP请求的动态数据就是由 web框架层来提供的。常见的 web框架有Flask，Django等，我们以 Flask 框架为例子，展示 web框架的作用： 123456789from flask import Flaskapp = Flask(__name__)@app.route('/hello')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run(host='0.0.0.0', port=8080) 以上简单的几行代码，就创建了一个 web应用程序对象 app。app 监听机器所有 ip 的 8080 端口，接受用户的请求连接。我们知道，HTTP 协议使用 URL 来定位资源，上面的程序会将路径 /hello 的请求交由 hello_world 方法处理，hello_world 返回 ‘Hello World!’ 字符串。对于 web框架的使用者来说，他们并不关心如何接收 HTTP 请求，也不关心如何将请求路由到具体方法处理并将响应结果返回给用户。Web框架的使用者在大部分情况下，只需要关心如何实现业务的逻辑即可。 WSGI层WSGI 不是服务器，也不是用于与程序交互的API，更不是真实的代码，WSGI 只是一种接口,它只适用于 Python 语言，其全称为 Web Server Gateway Interface，定义了 web服务器和 web应用之间的接口规范。也就是说，只要 web服务器和 web应用都遵守WSGI协议，那么 web服务器和 web应用就可以随意的组合。下面的代码展示了 web服务器是如何与 web应用组合在一起的。 123def application(env, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return [b"Hello World"] 方法 application由 web服务器调用，参数env，start_response 由 web服务器实现并传入。其中，env是一个字典，包含了类似 HTTP_HOST，HOST_USER_AGENT，SERVER_PROTOCO 等环境变量。start_response则是一个方法，该方法接受两个参数，分别是status，response_headers。application方法的主要作用是，设置 http 响应的状态码和 Content-Type 等头部信息，并返回响应的具体结果。 上述代码就是一个完整的 WSGI 应用，当一个支持 WSGI 的 web服务器接收到客户端的请求后，便会调用这个 application 方法。WSGI 层并不需要关心env，start_response 这两个变量是如何实现的，就像在 application 里面所做的，直接使用这两个变量即可。 值得指出的是，WSGI 是一种协议，需要区分几个相近的名词： uwsgi同 wsgi 一样也是一种协议，uWSGI服务器正是使用了 uwsgi 协议 uWSGI实现了 uwsgi 和 WSGI 两种协议的web服务器。注意 uWSGI 本质上也是一种 web服务器，处于上面描述的三层结构中的 web服务器层。 CGI通用网关接口，并不限于 Python 语言，定义了 web服务器是如何向客户端提供动态的内容。例如，规定了客户端如何将参数传递给 web服务器，web服务器如何将参数传递给 web应用，web应用如何将它的输出如何发送给客户端，等等。生产环境下的 web应用都不使用 CGI 了，CGI进程（类似 Python 解释器）针对每个请求创建，用完就抛弃，效率低下。WSGI 正是为了替代 CGI 而出现的。 说到这，我们基本理清了 WSGI 在 web服务器与 web框架之间作用：WSGI 就像一条纽带，将 web服务器与 web框架连接起来。回到本文的题目，Nginx 属于一种 web服务器，Flask属于一种 web框架，因此，WSGI 与 Nginx、Flask 的作用就不明而喻了。 最后以 Nginx，WSGI，Flask 之间的对话结束本文。Nginx：Hey，WSGI，我刚收到了一个请求，我需要你作些准备，然后由Flask来处理这个请求。WSGI：OK，Nginx。我会设置好环境变量，然后将这个请求传递给Flask处理。Flask：Thanks WSGI！给我一些时间，我将会把请求的响应返回给你。WSGI：Alright，那我等你。Flask：Okay，我完成了，这里是请求的响应结果，请求把结果传递给Nginx。WSGI：Good job！Nginx，这里是响应结果，已经按照要求给你传递回来了。Nginx：Cool，我收到了，我把响应结果返回给客户端。大家合作愉快~ 参考资料： http://hackerxu.com/2015/05/10/flask011.html https://www.quora.com/What-are-good-ways-to-understand-WSGI-flup-fastcgi-CGI-and-Django https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface http://flask.pocoo.org/docs/0.10/quickstart/ http://stackoverflow.com/questions/219110/how-python-web-frameworks-wsgi-and-cgi-fit-together https://www.python.org/dev/peps/pep-0333/ 《Python核心编程（第3版）》Wesley Chun著]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>WSGI</tag>
        <tag>uWSGI</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC 基本原理与 Apach Thrift 初体验]]></title>
    <url>%2FRPC-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8E-Apach-Thrift-%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[概述RPC（Remote Procedure Call，远程过程调用）是一个计算机通信协议，此协议允许进程间通信。简单来说，当机器 A 上的进程调用机器 B 上的进程时，A 上的调用进程被挂起，而 B 上的被调用进程开始执行。调用方可以通过参数将信息传送给被调用方，然后可以通过被调用方传回的结果得到返回。RPC 框架屏蔽了底层传输方式（TCP/UDP）、序列化和反序列化（XML/JSON/二进制）等内容，使用框架只需要知道被调用者的地址和接口就可以了，无须额外地为这些底层内部编程。目前主流的 RPC 框架有如下几种。 Thrift：Facebook 开源的跨语言框架 gRPC：Google 基于 HTTP/2 和 Protobuf 的能用框架 Avro：Hadoop 的子项目 本文讲述 RPC 的基本原理并以 Thrift 框架为例说明 RPC 的使用。 基本原理图1：客户端与服务器之间的 RPC 通信过程 图1描述了客户端与服务器 RPC 通信的基本过程，具体来说，包括几下步骤：（1）客户过程以正常方式调用客户桩（client stub，一段代码）；（2）客户桩生成一个消息，然后调用本地操作系统；（3）客户端操作系统将消息发送给远程操作系统；（4）远程操作系统将消息交给服务器桩（server stub，一段代码）；（5）服务器桩将参数提取出来，然后调用服务器过程；（6）服务器执行要求的操作，操作完成后将结果返回给服务器桩；（7）服务器桩将结果打包成一个消息，然后调用本地操作系统；（8）服务器操作系统将含有结果的消息发送回客户端操作系统；（9）客户端操作系统将消息交给客户桩；（10）客户桩将结果从从消息中提取出来，返回给调用它的客户过程。所有这些步骤的效果是，将客户过程对客户桩发出的本地调用转换成对服务器过程的本地调用，而客户端和服务器都不会意识到有中间步骤的存在。这个时候，你可能会想，既然是调用另一台机器的服务，使用 RESTful API 也可以实现啊，为什么要选择 RPC 呢？我们可以从两个方面对比： 资源粒度。RPC 就像本地方法调用，RESTful API 每一次添加接口都可能需要额外地组织开放接口的数据，这相当于在应用视图中再写了一次方法调用，而且它还需要维护开发接口的资源粒度、权限等。 流量消耗。RESTful API 在应用层使用 HTTP 协议，哪怕使用轻型、高效、传输效率高的 JSON 也会消耗较大的流量，而 RPC 传输既可以使用 TCP 也可以使用 UDP，而且协议一般使用二制度编码，大大降低了数据的大小，减少流量消耗。 对接异构第三方服务时，通常使用 HTPP/RESTful 等公有协议，对于内部的服务调用，应用选择性能更高的二进制私有协议。 Thrift 架构说完 RPC 的一般性原理，我们再来看目前流行的 RPC 框架—— Apache Thrift。Apache Thrift 最初是 Facebook 实现的一种支持多种编程语言、高效的远程服务器调用框架，它于 2008 年进入 Apache 开源项目。Apache Thrift 采用接口描述语言（IDL）定义 RPC 接口和数据类型，通过编译器生成不同语言的代码（支持 C++，Java，Python，Ruby等），其数据传输采用二进制格式，相对 XML 和 JSON 来说体积更小，对于高并发、大数据量和多语言的环境更有优势。在 Facebook，Apache Thrift 正是使用于其内部服务的通信，其稳定性和高性能已在生产环境中得到证明的。Apache Thrift 的架构如图2所示。 图2：Apache Thrift 客户/服务器架构 由图2可以看到，Apache Thrift 仍然是基于图1所示的 RPC 通信的原理。图2黄色部分是用户实现的业务逻辑，接下来两层是 Apache Thrift 根据 IDL 生成的客户端和服务端代码（其中红色部分用来实现数据的读写操作），对应于图1中的 client stub 和 server stub。TProtocol 用来对数据进行序列化与反序列化，具体方法包括二进制，JSON 或者 Apache Thrift 定义的格式。TTransport 提供数据传输功能，使用 Apache Thrift 可以方便地定义一个服务并选择不同的传输协议。 Thrift 使用废话少说，我们来看下 Thrift 是如何使用的。 安装 Thrift我们以 Mac OS 为例，说明如何安装 Thrift ，安装过程具体可以参考 http://thrift.apache.org/docs/install/os_x 。（1）安装 boost（2）安装 libevent（3）安装 thrift安装过程还算顺利，如果出现 openssl 头文件找不到的情况，可以通过给 make 命令添加参数的方式解决，例如：1make CPPFLAGS=&quot;-I/usr/local/Cellar/openssl/1.0.2h_1/include&quot; LDFLAGS=&quot;-L/usr/local/Cellar/openssl/1.0.2h_1/lib&quot; 安装完成后，使用命令来测试一下安装是否成功：1$ thrift -version 可以看到输出： Thrift version 0.10.0 说明已正确安装。 定义 IDL 文件安装好 Thrift 后，我们便可以编写 IDL 文件。Thrift 使用 .thrift 文件来定义接口和数据类型。下面是一个简单的 .thrift 文件示例：12345typedef i32 intservice MultiplicationService&#123; int multiply(1:int n1, 2:int n2),&#125; 为了简单起见，我们只定义了一个接口。解析一下 .thrift 文件的语法： typedef 用来定义 Thrift 类型名称的别名。Thrift 中拥有自己的类型系统，其中 i32 为32位有符号整形。 service 用来定义 Thrift 的服务接品，在上面我们定义一个 MultiplicationService，其中包含了一个接口：multiply。接口的第一个字段表示接口返回的类型比如上面返回一个 i32 类型。接口的参数需要定义其类型和顺序。 编写好 .thrift 文件后，将上面的文件保存为 multiplication.thrift，然后，便可以使用 thrift 命令来对其进行编译。1thrift --gen py multiplication.thrift 执行此命令后，会在当前目录生成一个 gen-py 的目录，进入 gen-py 目录，可以看到其中包含以下的文件： 图3：编译 .thrift 文件后生成的 Python 代码 生成的 MultiplicationService.py 包含了 RPC 通信所需的客户端和服务端代码，以及数据 read/write 操作代码。 实现服务端我们在 gen-py 相同的目录下编写服务端与客户端的实现代码。首先是服务端代码 multiserver.py。 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python# -*- coding:utf-8 -*-import syssys.path.append('gen-py')from multiplication import MultiplicationServicefrom multiplication.ttypes import *from thrift.transport import TSocketfrom thrift.transport import TTransportfrom thrift.protocol import TBinaryProtocolfrom thrift.server import TServerclass MultiplicationHandler(object): # multiplication.thrift 文件中已对 `multiploy` 进行了定义，参数为两个整型 # 返回值为一整型 def multiply(self, n1, n2): print('multify n1 * n2, n1: %s, n2: %s, result: %s' % (n1, n2, n1 * n2)) return n1 * n2if __name__ == '__main__': handler = MultiplicationHandler() # Processor 用来从连接中读取数据，将处理授权给 handler（自己实现）， # 最后将结果写到连接上 processor = MultiplicationService.Processor(handler) # 服务端使用 9090 端口， transport 是网络读写抽象层，为到来的连接创建传输对象 transport = TSocket.TServerSocket(port=9090) tfactory = TTransport.TBufferedTransportFactory() pfactory = TBinaryProtocol.TBinaryProtocolFactory() server = TServer.TSimpleServer(processor, transport, tfactory, pfactory) print('Starting the server...') server.serve() print('done.') multiserver.py 首先对 multiplication.thrift 文件中的 multiply 方法进行了实现，这样当服务端接收到客户端请求后会调用此方法进行处理。Processor 类则主要完成两个事情：从连接中读取数据，往连接中写入数据。当然中间会将数据交给 handler 处理。服务端启动后，会监听 9090 端口，接受客户端的连接。 实现客户端我们在 multiclient.py 实现客户端的代码。 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python# -*- coding:utf-8 -*-import syssys.path.append('gen-py')from multiplication import MultiplicationServicefrom multiplication.ttypes import *from thrift import Thriftfrom thrift.transport import TSocketfrom thrift.transport import TTransportfrom thrift.protocol import TBinaryProtocoltry: # 同样使用 9090 端口，使用阻塞式 I/O 进行传输，是最常见的模式 transport = TSocket.TSocket('localhost', 9090) transport = TTransport.TBufferedTransport(transport) # 封装协议，使用二进制编码格式进行传输 protocol = TBinaryProtocol.TBinaryProtocol(transport) # 创建一个 client client = MultiplicationService.Client(protocol) # 打开连接 transport.open() n1 = 5 n2 = 7 product = client.multiply(n1, n2) print '%s * %s = %s' % (n1, n2, product) # 关闭连接 transport.close()except Thrift.TException, tx: print '%s' % (tx.message) 在上面的代码中，我们创建了一个 Client 对象，Client 类的代码由 .thrift 生成。然后我们调用 multiply 发送 RPC 请求，并取得返回值。 测试打开一个终端，执行命令，运行服务端程序：1python multiserver.py 打开另一个终端，执行命令，运行客户端程序：1python multiclient.py 此时，服务端终端输出： multify n1 * n2, n1: 5, n2: 7, result: 35 客户端终端输出： 5 * 7 = 35 说明客户端已正常发送 RPC 请求，并由服务端处理，客户端成功接收返回。 总结本文简述了 RPC 的基本原理与 Apache Thrift 的使用步骤。通过此文章，希望大家对于 RPC 和 Apache Thrift 有个初步的了解。对于 Apache Thrift 更深入的分析，可以参考本人的系列文章。 参考资料 http://thrift.apache.org Python Web开发实践，董伟明著，中国工信出版集团，电子工业出版社 https://www.cs.rutgers.edu/~pxk/417/notes/03-rpc.html https://www.zhihu.com/question/25536695 https://en.wikipedia.org/wiki/Apache_Thrift http://thrift.apache.org/docs/install/os_x http://thrift-tutorial.readthedocs.io/en/latest/usage-example.html http://www.thrift.pl http://techlabs.emag.ro/introduction-to-apache-thrift/ https://thrift.apache.org/static/files/thrift-20070401.pdf http://jnb.ociweb.com/jnb/jnbJun2009.html]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>RPC</tag>
        <tag>Thrift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细说 C++ Traits Classes]]></title>
    <url>%2F%E7%BB%86%E8%AF%B4-C-Traits-Classes%2F</url>
    <content type="text"><![CDATA[最近在看侯捷的《STL源码剖析》，看到第三章有关 traits 的介绍，被搞得一头雾水，看了半天不知所云。为了彻底了解这个技法的原理，硬着头皮上网查了资料，并结合 Scott Meyers 的 《Effective C++》，总算是把 traits 的原理搞明白了:) 什么是 traits？我们先来看下 C++ 之父的回答： Think of a trait as a small object whose main purpose is to carry information used by another object or algorithm to determine “policy” or “implementation details”. - Bjarne Stroustrup 在 C++ 中，traits 习惯上总是被实现为 struct ，但它们往往被称为 traits classes。Traits classes 的作用主要是用来为使用者提供类型信息。 模板特化（Template Specialization）为了清晰理解 traits 的原理，我们先来看 traits 使用的关键技术 —— 模板的特化与偏特化。 我们先来看下一函数模板的通用定义： 1234template&lt;typename T&gt;struct my_is_void &#123; static const bool value = false;&#125;; 然后，针对 void 类型，我们有以以下的特化版本： 1234template&lt;&gt;struct my_is_void&lt;void&gt; &#123; static const bool value = true;&#125;; 测试代码如下： 1234my_is_void&lt;bool&gt; t1;cout &lt;&lt; t1.value &lt;&lt; endl; // 输出0my_is_void&lt;void&gt; t2;cout &lt;&lt; t2.value &lt;&lt; endl; // 输出1 当声明 my_is_void&lt;void&gt; t2; 时，使用的是特化版本，故其 value 值为 1。 偏特化（Patial Spcialization）模板特化时，可以只指定一部分而非所有模板参数，或者是参数的一部分而非全部特性，这叫做模板的偏特化。一个类模板的偏特化本身是一个模板，使用它时用户还必须为那些在特例化版本中未指定的模板参数提供实参。 我们以另一个例子来说明模板的偏特化。先来看通用的原始模板。 1234template&lt;typename T&gt;struct my_is_pointer &#123; static const bool value = false;&#125;; 我们对模板参数T进行限制，要求其为一个指针的类型： 1234template&lt;typename T&gt;struct my_is_pointer&lt;T*&gt; &#123; static const bool value = true;&#125;; 测试： 1234my_is_pointer&lt;int&gt; p1;cout &lt;&lt; p1.value &lt;&lt; endl; // 输出 0，使用原始模板my_is_pointer&lt;int*&gt; p2;cout &lt;&lt; p2.value &lt;&lt; endl; // 输出 1，使偏特化模板，因为指定了 int * 类型的参数 typename 关键字提问一个问题，以下模板的声明中， class 和 typename 有什么不同？ 12template&lt;class T&gt; class Test;template&lt;typename T&gt; class Test; 答案：没有不同。然而，C++ 并不总是把 class 和 typename 视为等价。有时候我们一定得使用 typename。默认情况下，C++ 语言假定通过作用域运算符访问的名字不是类型。因此，如果我们希望使用一个模板类型参数的类型成员，就必须显式告诉编译器该名字是一个类型。我们通过使用关键字 typename 来实现这一点： 12345678template&lt;typename T&gt;typename T::value_type top(const T &amp;c)&#123; if (!c.empty()) return c.back(); else return typename T::value_type();&#125; top 函数期待一个容器类型的实参，它使用 typename 指明其返回类型，并在 c 中没有元素时生成一个初始值的元素，并返回给调用者。 12345vector&lt;int&gt; vec;vec.push_back(1);vec.push_back(2);vec.push_back(3);cout &lt;&lt; top&lt;vector&lt;int&gt; &gt;(vec) &lt;&lt; endl; 在这里我们只需要记住一点，当我们希望通知编译器一个名字表示类型时，必须使用关键字 typename，而不能使用 class。 实现 Traits Classes说完了背景知识，我们正式进入 traits 的关键地带。 我们知道，在 STL 中，容器与算法是分开的，彼此独立设计，容器与算法之间通过迭代器联系在一起。那么，算法是如何从迭代器类中萃取出容器元素的类型的？没错，这正是我们要说的 traits classes 的功能。迭代器所指对象的类型，称为该迭代器的 value_type。我们来简单模拟一个迭代器 traits classes 的实现。 1234template&lt;class IterT&gt;struct my_iterator_traits &#123; typedef typename IterT::value_type value_type;&#125;; my_iterator_traits 其实就是个类模板，其中包含一个类型的声明。有上面 typename 的基础，相信大家不难理解 typedef typename IterT::value_type value_type; 的含义：将迭代器的value_type 通过 typedef 为 value_type。 对于my_iterator_traits，我们再声明一个偏特化版本。 1234template&lt;class IterT&gt;struct my_iterator_traits&lt;IterT*&gt; &#123; typedef IterT value_type;&#125;; 即如果 my_iterator_traits 的实参为指针类型时，直接使用指针所指元素类型作为 value_type。 为了测试 my_iterator_traits 能否正确萃取迭代器元素的类型，我们先编写以下的测试函数。 1234567891011void fun(int a) &#123; cout &lt;&lt; "fun(int) is called" &lt;&lt; endl;&#125;void fun(double a) &#123; cout &lt;&lt; "fun(double) is called" &lt;&lt; endl;&#125;void fun(char a) &#123; cout &lt;&lt; "fun(char) is called" &lt;&lt; endl;&#125; 我们通过函数重载的方式，来测试元素的类型。 测试代码如下： 123456my_iterator_traits&lt;vector&lt;int&gt;::iterator&gt;::value_type a;fun(a); // 输出 fun(int) is calledmy_iterator_traits&lt;vector&lt;double&gt;::iterator&gt;::value_type b;fun(b); // 输出 fun(double) is calledmy_iterator_traits&lt;char*&gt;::value_type c;fun(c); // 输出 fun(char) is called 为了便于理解，我们这里贴出 vector 迭代器声明代码的简化版本： 12345678910template &lt;class T, ...&gt;class vector &#123;public: class iterator &#123; public: typedef T value_type; ... &#125;; ...&#125;; 我们来解释 my_iterator_traits&lt;vector&lt;int&gt;::iterator&gt;::value_type a; 语句的含义。vector&lt;int&gt;::iterator 为vector&lt;int&gt; 的迭代器，该迭代器包含了 value_type 的声明，由 vector 的代码可以知道该迭代器的value_type 即为 int 类型。接着，my_iterator_traits&lt;vector&lt;int&gt;::iterator&gt; 会采用 my_iterator_traits 的通用版本，即 my_iterator_traits&lt;vector&lt;int&gt;::iterator&gt;::value_type 使用 typename IterT::value_type 这一类型声明，这里 IterT 为 vector&lt;int&gt;::iterator，故整个语句萃取出来的类型为 int 类型。 对 double 类型的 vector 迭代器的萃取也是类似的过程。 而 my_iterator_traits&lt;char*&gt;::value_type 则使用 my_iterator_traits 的偏特化版本，直接返回 char 类型。 由此看来，通过 my_iterator_traits ，我们正确萃取出了迭代器所指元素的类型。 总结一下我们设计并实现一个 traits class 的过程：1）确认若干我们希望将来可取得的类型相关信息，例如，对于上面的迭代器，我们希望取得迭代器所指元素的类型；2）为该信息选择一个名称，例如，上面我们起名为 value_type；3）提供一个 template 和一组特化版本（例如，我们上面的 my_iterator_traits），内容包含我们希望支持的类型相关信息。 参考资料 https://accu.org/index.php/journals/442 Effective C++，第三版，Scott Meyers 著，侯捷译 STL 源码剖析，侯捷著 http://www.bogotobogo.com/cplusplus/template_specialization_traits.php http://www.bogotobogo.com/cplusplus/template_specialization_function_class.php]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>traits classes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[体验 gRPC 那些事儿]]></title>
    <url>%2F%E4%BD%93%E9%AA%8C-gRPC-%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF%2F</url>
    <content type="text"><![CDATA[概述我们来看 google 对于 gRPC 的定义： A high performance, open-source universal RPC framework 即 gRPC 是一个开源的高性能通过 RPC 框架。具体来说，它具有以下特点： 使用 protocol buffers 来作为序列化和反序列化，以及接口定义语言 （IDL） 跨语言，跨平台，gRPC支持多种平台和多种语言，这应该是 gRPC 框架最大的优势 易于使用，安装编译环境和运行环境 基于 HTTP/2 ，提供双向传输和认证机制 对于 gRPC 的应用场景，google 给出了一些常见的应用情景，个人则认为更适合于内部服务的使用。 关于 protobuf，多说一下。gRPC 将服务接口的定义写在 .proto 文件中，.proto 文件遵循 protobuf 的规范。使用 gRPC 提供的命令行工具，可以对 .proto 文件进行编译成相关语言的源代码，例如 c++ 语言或者 python 语言的服务端和客户端代码 。 安装以 python 版本的 gRPC 为例，说明 gRPC 的安装。 1） 将 pip 的版本升级到最新版本1pip install --upgrade pip 2）安装 grpcio 包1pip install grpcio 3）安装 gRPC 的 python 工具包1pip install grpcio-tools 4）下载 gRPC 的官方例子12git clone https://github.com/grpc/grpccd grpc/examples/python/helloworld 下载的例子在 gPRC 源代码的 examples 目录中。 使用环境已安装完毕，接下来便可以运行代码了。进入 examples/python/helloworld 目录。运行服务端程序，服务端监听 50051端口。1python greeter_server.py 打开另一个终端，运行客户端程序，1python greeter_client.py 可以看到客户端程序的输出 Greeter client received: Hello, you! 说明客户端通过 gRPC 成功从服务端获取了数据。 分析正常使用 gRPC 的流程是，我们需要首先编写接口和消息的声明文件 .proto，然后进行编译才可以得到相应接口和消息的源文件。上面我们省略了编译的操作，是由于 gRPC 的源代码中已经将 examples 目录下的例子都编译好了。 打开 examples/protos/helloworld.proto，可以看到原来的接口和消息声明为：123456789101112131415// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;// The request message containing the user&apos;s name.message HelloRequest &#123; string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123; string message = 1;&#125; 该.proto 文件包括两部分的声明，分别是接口的声明和消息的声明。原来已经有一个接口 SayHello，假设现在我们需要增加一个接口 SayHelloAgain，新接口的参数和返回值使用原有的类型，那么新的 .proto 文件如下： 1234567// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125; // Sends another greeting rpc SayHelloAgain (HelloRequest) returns (HelloReply) &#123;&#125;&#125; 编写或者更新 .proto 文件后，需要对其进行编译。上面我们安装了 grpcio-tools 工具，可以对 .proto 文件编译。 12cd examples/python/helloworldpython -m grpc_tools.protoc -I../../protos --python_out=. --grpc_python_out=. ../../protos/helloworld.proto 执行命令后会生成新的 helloworld_pb2.py 文件和新的 helloworld_pb2_grpc.py 文件。helloworld_pb2.py 文件包含生成的请求类和响应类，而 helloworld_pb2_grpc.py 文件则包含生成的服务端骨架（skeleton）代码和客户端桩（stub）代码。 为了使用新的接口 SayHelloAgain ，需要在我们的程序代码增加新的测试代码。服务端程序： 123456class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloReply(message='Hello, %s!' % request.name) def SayHelloAgain(self, request, context): return helloworld_pb2.HelloReply(message='Hello again, %s!' % request.name) 客户端程序：12345678def run(): channel = grpc.insecure_channel('localhost:50051') stub = helloworld_pb2_grpc.GreeterStub(channel) response = stub.SayHello(helloworld_pb2.HelloRequest(name='you')) print("Greeter client received: " + response.message) response = stub.SayHelloAgain(helloworld_pb2.HelloRequest(name='you')) print("Greeter client received: " + response.message) 最后我们再次执行新的服务端和客户端程序。可以看到，客户端终端中，增加了一行输出： Greeter client received: Hello, you!Greeter client received: Hello again, you! 说明我们成功添加了新的接口。 参考资料 http://www.grpc.io https://http2.github.io/ http://www.grpc.io/docs/quickstart/python.html http://www.integralist.co.uk/posts/grpc.html]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>gRPC</tag>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git远程仓库]]></title>
    <url>%2FGit%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Git 支持在本地仓库进行程序项目的版本管理，这个本地的 git 仓库在程序项目的文件夹内（也就是 “.git” 文件夹内）。随着程序项目的推进，我们会把不同阶段的文件内容加入到这个 git 仓库中。这个仓库由我们直接操控，我们将它称之为“本地 git 仓库”。除了“本地 git 仓库”外，如果需要与他人共同开发这个程序项目，还需要一个“远程 git 仓库”。程序项目，本地 git 仓库和远程 git 仓库之间的关系如图1所示。图1：程序项目，本地仓库，远程仓库之间的关系 远程 git 仓库并不一定存储在另一台计算机中。很多人知道 git 支持 HTTP/HTTPS，SSH等协议，以允许本地 git 仓库与存在于另一台计算机中的远程 git 仓库之间传送数据。但实际上，远程 git 仓库也可以跟本地 git 仓库处于同一台电脑中，使用本地协议来进行数据的传送。 创建远程 git 仓库以下命令可以创建一个远程 git 仓库：1git init --bare git仓库文件夹名称 即在git init命令加上--bare选项。这是因为远程 git 仓库也称为bare类型的仓库。例如以下命令会创建一个名为 lee.git 的远程仓库：1git init --bare lee.git 执行以上命令后，可以看到当前目录下多了一个 lee.git 目录，打开 lee.git 目录，可以看到目录的结构：图2：bare类型的git仓库 可以看到 lee.git 目录下并没有 .git 这个子文件夹。 从远程仓库复制出本地仓库使用 git clone 命令，可以从远程仓库复制出一个本地仓库。例如如下命令： 1git clone ./lee.git local 这样就从 lee.git 复制出一个名为 local 的本地仓库。进入 local 这个目录，可以看到 .git 子文件夹，也就是说，这个复制得到的 git 仓库是一般的 git 仓库，并不是 bare 类型，我们可以在这上面进行程序项目的开发。 本地仓库和远程仓库的同步在本地仓库的配置文件 config 中，有一个 origin 的属性，用来记录本地仓库与远程仓库之间的关系。使用git config -l可以列出 git 的配置信息： …（其他属性）remote.origin.url=/Users/lihao/Code/Git/./lee.git/remote.origin.fetch=+refs/heads/:refs/remotes/origin/…（其他属性） 如果我们对本地仓库 local 进行了修改，现在需要推送到远程仓库，可以使用以下命令：1git push origin master 即把 master 分支的最新数据，推送到远程仓库存储。需要指出的是，执行这个命令，不会在配置文件中记录本地仓库的分支和远程仓库分支之间的关系。如果加上--set-upstream选项（或者使用短选项-u）如下，git 就会在配置文件中记录本地仓库的分支和远程仓库的分支之间的关系。1git push --set-upstream origin master 执行此命令后，再使用git config -l | grep branch可以看到以下结果，这个对应关系会被下次git push使用。 branch.master.remote=originbranch.master.merge=refs/heads/master 第一行配置指定当前这个分支对应的远程仓库；第二行配置指定当前这个分支所对应的分支名称。 使用git branch -a，可以看到和 origin 相关的分支： *masterremotes/origin/master remotes 开头的分支是用来跟踪远程仓库的状态。由于远程仓库可能已被他人修改，因此，我们需要将远程仓库的最新状态同步到本地仓库。使用命令git pull可以更新本地仓库状态：1git pull 具体来说，git pull命令会执行两个操作： git fetch从远程仓库取回当前所在分支的最新数据。完成这项操作后，可以将本地计算机上的远程仓库的分支状态更新至最新。 git merge把远程仓库的分支合并到本地仓库的分支。执行第2个操作可能会导致冲突，如果出现冲突，需要先解决冲突，才能将本地仓库的修改通过git push到远程仓库。 远程仓库的 remote 操作添加远程仓库前面我们提到的本地仓库与远程仓库的操作，都是假设先有远程仓库，再复制一个本地仓库，这也是团队开发的一个标准模式。但有时程序项目并不一开始就由团队共同开发，而是先由某一个程序开发人员开头，而后才逐步过渡到团队开发的模式。使用git clone --bare 程序项目文件夹名称 远程仓库路径命令，可以从本地仓库复制出一下远程仓库。例如使用以下命令，可以从 local 本地仓库创建一个 server.git 远程仓库。 1git clone --bare local server.git 但是，本地仓库 local 并不会自动记录其远程仓库的属性，为此，先进入 local 本地仓库目录，然后使用以下命令来添加远程仓库：1git remote add origin ../server.git 这样，就为本地仓库添加了一个远程仓库的属性，其远程仓库的名称为 origin。但是，使用git remote add命令，只是设置了仓库之间的对应关系，还没创建远程仓库的追踪分支。如果使用git branch -a命令，只会看到本地仓库的分支，并没有任何远程仓库的追踪分支。为此，还需要执行1git remote update 让 git 在本地仓库创建追踪分支。 git remote add命令有两点需要注意： 远程仓库的名称不一定叫做 origin ，叫其他名称也可以； 远程仓库并不是唯一的，一个本地仓库可以对应多个远程仓库属性。 关于第2点，可以参考下图3。假设 origin 这个远程仓库原来就已经存在，现在我们需要创建另一个远程仓库，并把 master 和 bug 分支推送到这个新的远程仓库中。 图3：本地仓库对应多个远程仓库的例子 要完成这项工作，需要按以下顺序执行。首先，创建一个新的远程仓库。1git init --bare lee2.git 这样便创建一个名称 lee2.git 的仓库。接下来，在本地计算地按顺序执行：123git remote add new-repo ../lee2.gitgit push new-repo mastergit push new-repo bug git remote add new-repo ../lee2.git 命令假设我们在本地仓库的目录执行该命令，且远程仓库的路径跟本地仓库路径处于同一级目录，new-repo为我们自己取的名称，也可以起其他的名称。下面的两个git push命令会将 master 和 bug 分支推送到新创建的仓库，并自动创建远程仓库的两个追踪分支。这时，如果使用git branch -a命令，可以显示以下的追踪分支信息： …（其他分支）remotes/new-repo/masterremotes/new-repo/bug 删除远程仓库与git remote add相反的是git remote rm命令。例如，现在我们需要删除上面添加的new-repo远程仓库：1git remote rm new-repo 删除远程仓库后，本地所有属于它的追踪分支也会一起消失。如果需要还原回来，只需再使用git remote add和git remote update命令即可。 其他远程仓库操作更改远程仓库名称：1git remote rename 旧名称 新名称 更新远程仓库地址：1git remote set-url 远程仓库名称 新的地址 显示和远程仓库相关的设置：1git remote -v 删除远程仓库分支：1git push 远程仓库的名称 --delete 分支名称 参考资料 完全学会Git GitHub Git Server的24堂课, 孙宏明著, 清华大学出版社, 2016年 Git Pro, https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB Java使用指南]]></title>
    <url>%2FMongoDB-Java%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[MongoDB是当今非常流行的一款NoSQL数据库，本文介绍如何使用MongoDB的Java驱动来操作MongoDB。 一、引入MongoDB Java Driver包如果需要操作MongoDB的Java项目是一个Maven项目，可以在依赖中加上以下的配置。1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;2.13.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 或者通过直接下载jar包的方式使用，下载地址：mongo-java-driver-2.13.2.jar。详细的如何引入MongoDB jar包的方法可以参考官方文档。 二、连接MongoDB可以使用MongoClient来连接MongoDB，MongoClient的使用方式如下：12MongoClient mongoClient = new MongoClient("localhost", 27017);DB db = mongoClient.getDB("mydb"); 上面的代码连接了localhost:27017上MongoDB服务，并指定使用mydb数据库。连接后便可以对这个数据库作进一步的操作。需要指出的是，MongoClient是线程安全的，可以在多程程环境中共享同一个MongoClient。通常来说，一个应用程序中，只需要生成一个全局的MongoClient实例，然后在程序的其他地方使用这个实例即可。 三、认证可以使用多种方式对连接进行认证，下面介绍两种方式。 1. 方式一：MongoCredentialMongoCredential类的createCredential方法可以指定认证的用户名，密码，以及使用的数据库，并返回一个MongoCredential对象。其方法的声明如下： 1static MongoCredential createCredential(String userName, String database, char[] password) 例如， 1MongoCredential credential = MongoCredential.createCredential("user", "mydb", "password".toCharArray(); 上面创建了一个用户名为user，密码为password，数据库为mydb的MongoCredential对象。将生成MongoCredential的对象作为MongoClient构造函数的参数。由于MongoClient构造函数的为List&lt;MongoCredential&gt;类型，所以需要先构造成一个List再传递。完整的认证的例子如下： 1234MongoCredential credential = MongoCredential.createCredential("user", "mydb", "password".toCharArray()); ServerAddress serverAddress = new ServerAddress("localhost", 27017); MongoClient mongoClient = new MongoClient(serverAddress, Arrays.asList(credential)); DB db = mongoClient.getDB("mydb"); 2. 方式二：MongoClientURI亦可以使用MongoClientURI完成MongoDB的认证，它代表了一个URI对象。MongoClientURI的构造函数接受一个String类型的字符串，这个字符串的格式如下： mongodb://[username:password@]host1[:port1][,host2[:port2],…[,hostN[:portN]]][/[database][?options]] 生成的MongoClientURI对象作为MongoClient构造函数的参数，完整的认证例子如下： 1234String sURI = String.format("mongodb://%s:%s@%s:%d/%s", "user", "password", "localhost", 27017, "mydb"); MongoClientURI uri = new MongoClientURI(sURI); MongoClient mongoClient = new MongoClient(uri); DB db = mongoClient.getDB("mydb"); 四、获取一个集合1DBCollection coll = db.getCollection("mycol"); 然后可以对指定的集合进行操作，例如，插入，删除，查找，更新文档等。 五、插入文档例如，一个文档以Json来表示如下， { “name”: “mongo”, “info”: { “ver”: “3.0” } } 现在需要插入到集合mycol中。为了插入到集合中，可以使用BasicDBObject构造一个文档。 12BasicDBObject doc = new BasicDBObject("name", "mongo").append("info", new BasicDBObject("ver", "3.0"));coll.insert(doc); 六、查找文档1. 通过findOne查找一个符合条件文档通过findOne可以查找一个符合条件的文档。例如，对于上面的mycol集合，执行以下命令： 12DBObject myDoc = coll.findOne();System.out.println(myDoc); 将输出mycol集合中的第一个文档。也可以通过指定findOne的查找参数，来查找符合查找条件的一个文档。 2. 通过find查找所有符合条件的文档find用来查找符合条件的文档，它返回一个DBCursor对象，通过遍历DBCursor对象，可以获得所有符合查找条件的文档。为了说明和测试，我们先插入一批以下格式的文档 { “i”: value } 123for (int i=0; i &lt; 100; i++) &#123; coll.insert(new BasicDBObject("i", i));&#125; find的使用示例如下： 12345678DBCursor cursor = coll.find();try &#123; while(cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125;&#125; finally &#123; cursor.close();&#125; 会输出mycol集合中所有的文档。也可以指定查找的条件，例如： 1234567891011BasicDBObject query = new BasicDBObject("i", 71);DBCursor cursor = coll.find(query);try &#123; while(cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125;&#125; finally &#123; cursor.close();&#125; 对于查找条件中包括$操作符的情形，例如以下一条mongo shell命令： db.coll.find({i: {$gte: 50}}); 可以使用DBObject生成查找条件， 1234567891011// find all where i &gt;= 50BasicDBObject query = new BasicDBObject("i", new BasicDBObject("$gte", 50));DBCursor cursor = coll.find(query);try &#123; while (cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125;&#125; finally &#123; cursor.close();&#125; 七、更新文档123BasicDBObject query = new BasicDBObject("i", 70);BasicDBObject up = new BasicDBObject("$set", new BasicDBObject("i", 100));coll.update(query, up); 上面的语句将i为70的文档更新i的值等于100。与我们常用的更新文档的mongo语句一样，DBCollection还包含了save，findAndModify等更新文档的方法，其使用方法在此不再赘述，可以参考API说明文档即可。 八、删除文档可以通过生成一个DBObject对象来删除指定的文档，例如：12BasicDBObject query = new BasicDBObject("i", 71);coll.remove(query); 上面的语句删除i为71的文档。 九、参考资料 http://mongodb.github.io/mongo-java-driver/2.13/getting-started/installation-guide/ http://mongodb.github.io/mongo-java-driver/2.13/getting-started/quick-tour/ https://github.com/mongodb/mongo-java-driver/blob/2.13.x/src/examples/example/QuickTour.java http://mongodb.github.io/mongo-java-driver/3.0/driver/reference/connecting/authenticating/ http://api.mongodb.org/java/3.0/?com/mongodb/MongoClientURI.html http://api.mongodb.org/java/2.13/]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Requests库简明使用教程]]></title>
    <url>%2FPython-Requests%E5%BA%93%E7%AE%80%E6%98%8E%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Requests是一常用的http请求库，它使用python语言编写，可以方便地发送http请求，以及方便地处理响应结果。 一、安装1.1 使用pip进行安装要安装requests，最方便快捷的方法是使用pip进行安装。 pip install requests 如果还没有安装pip，这个链接 Properly Installing Python 详细介绍了在各种平台下如何安装python以及setuptools，pip，virtualenv等常用的python工具，可以参考其中的步骤来进行安装。如果是在Windows平台下使用requests，也可以参考 《Windows下安装Django》 中有关 pip 安装的介绍来安装pip。 1.2 使用源代码安装可以在github上下载最新的requests的源代码 git clone git://github.com/kennethreitz/requests.git 下载成功后，使用以下命令进行安装 python setup.py install 二、使用2.1 发送http请求为了使用requests，需要首先将requests库import进来：1import requests 然后，可以使用requests来发送http请求，例如发送get请求：1r = requests.get('http://httpbin.org/ip') 执行此行代码，获得一个Response对象r，从r可以获取http请求的响应结果。 如果要发送post请求，则1r = requests.post('http://httpbin.org/post', data=&#123;'name': 'leo'&#125;) 2.2 构造url我们常常将http请求的参数以url的query string的形式进行发送，传统的做法是我们使用拼凑的方式构造这个url。例如我们需要构造以下这个url： http://httpbin.org/get?key1=value1&amp;key2=value2 使用reqeuets，你可以方便地构造这个url，而不用手工拼凑。你只需要将这些参数和值构造一个字典，然后将这个字典传给params参数即可： 123d = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get('http://httpbin.org/get', params=d)print r.url 输出 http://httpbin.org/get?key2=value2&amp;key1=value1 你可以看到，key1和key2这两个参数已被正确附加到url的query string中。 2.3 HTTP响应正文一个http响应的格式通常如下： 响应行响应报头响应正文 下面是请求 http://httpbin.org/ip 的http响应结果： HTTP/1.1 200 OKServer: nginxDate: Thu, 07 Jul 2016 02:53:49 GMTContent-Type: application/jsonContent-Length: 31Connection: keep-aliveAccess-Control-Allow-Origin: *Access-Control-Allow-Credentials: true[空行]{ “origin”: “43.230.90.94”} 注意，响应正文与响应头部之间有一空行间隔。Requests已将http响应封装成Response对象，从Response对象可获取响应正文的内容。 2.3.1 响应正文文本使用Response.text，可以获取响应的正文文本内容。 12r = requests.get('http://httpbin.org/ip')print r.text 输出 { “origin”: “218.107.63.234”} Requests会自动对响应正文进行解码：如果Response.encoding为空，那么requests会猜测响应的编码方式，然后进行解码。如果你可以确定响应的编码方式，也可以先对Response.encoding进行设置，然后再通过Response.text获取响应正文。 2.3.2 二进制响应正文使用Response.content可以获取响应正文的二进制字节内容。 1234567from PIL import Imagefrom StringIO import StringIOr = requests.get('http://img.blog.csdn.net/20150719230252020')image = Image.open(StringIO(r.content))print image.size # 输出(680, 510)image.show() # Windows平台下可以打开图像 上面的代码使用Response.content构造了一幅图像，然后输出其尺寸并打开该图像。 2.3.3 Json响应正文如果响应正文是一json串，可以使用Response.json()方法对响应正文进行json decode操作，并返回一个字典。1234r = requests.get('http://httpbin.org/ip')d = r.json()print dprint d['origin'] 输出： {u’origin’: u’43.230.90.94’}u’43.230.90.94’ 如果响应正文不是一个json串，则会报错。 2.4 响应状态Response对象status_code属性标识http请求响应的状态码： 12r = requests.get('http://httpbin.org/get')print r.status_code 如果状态码是40X或者50X，那么可以使用Response.raise_for_status()抛出一下异常： 12r = requests.get('http://httpbin.org/status/404')print r.status_code 404 响应返回404，故使用以下语句会抛出异常：1r.raise_for_status() Traceback (most recent call last): raise HTTPError(http_error_msg, response=self)requests.exceptions.HTTPError: 404 Client Error: NOT FOUND for url:http://httpbin.org/status/404 如果是返回200，则raise_for_status()并不会抛出异常。123r = requests.get('http://httpbin.org/status/200')print r.status_coder.raise_for_status() 2.5 响应的头部Response对象的headers属性是一个字典，可以获得http响应结果的头部的相关信息：12r = requests.get('http://httpbin.org/headers')print r.headers 输出： {‘Content-Length’: ‘157’, ‘Server’: ‘nginx’, ‘Connection’: ‘keep-alive’, ‘Access-Control-Allow-Credentials’: ‘true’, ‘Date’: ‘Wed, 06 Jul 2016 15:53:36 GMT’, ‘Access-Control-Allow-Origin’: ‘*’, ‘Content-Type’: ‘application/json’} 2.6 定制请求的头部Requests支持定制http请求的头部。为此，我们只需要构造一个字典，然后传给requests.get()的headers参数即可。 1234url = 'http://httpbin.org/headers'headers = &#123;'user-agent': 'my-app/0.0.1'&#125;r = requests.get(url, headers=headers)print r.text http://httpbin.org/headers 这个链接可以输出请求的头部，由于我们修改了请求头的user-agent字段，所以会访问这个链接会返回： { “headers”: { “Accept”: “/“, “Accept-Encoding”: “gzip, deflate”, “Host”: “httpbin.org”, “User-Agent”: “my-app/0.0.1” }} 2.7 发送POST请求一个http请求包括三个部分，为别为请求行，请求报头，消息主体，类似以下这样： 请求行请求报头消息主体 HTTP协议规定post提交的数据必须放在消息主体中，但是协议并没有规定必须使用什么编码方式。服务端通过是根据请求头中的Content-Type字段来获知请求中的消息主体是用何种方式进行编码，再对消息主体进行解析。具体的编码方式包括： application/x-www-form-urlencoded最常见post提交数据的方式，以form表单形式提交数据。 application/json以json串提交数据。 multipart/form-data一般使用来上传文件。 2.7.1 以form形式发送post请求Reqeusts支持以form表单形式发送post请求，只需要将请求的参数构造成一个字典，然后传给requests.post()的data参数即可。 1234url = 'http://httpbin.org/post'd = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.post(url, data=d)print r.text 输出： { “args”: {}, “data”: “”, “files”: {}, “form”: { “key1”: “value1”, “key2”: “value2” }, “headers”: {…… “Content-Type”: “application/x-www-form-urlencoded”,…… }, “json”: null,……} 可以看到，请求头中的Content-Type字段已设置为application/x-www-form-urlencoded，且d = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}以form表单的形式提交到服务端，服务端返回的form字段即是提交的数据。 2.7.2 以json形式发送post请求可以将一json串传给requests.post()的data参数， 1234url = 'http://httpbin.org/post's = json.dumps(&#123;'key1': 'value1', 'key2': 'value2'&#125;)r = requests.post(url, data=s)print r.text 输出： { “args”: {}, “data”: “{\”key2\”: \”value2\”, \”key1\”: \”value1\”}”, “files”: {}, “form”: {}, “headers”: {…… “Content-Type”: “application/json”,…… }, “json”: { “key1”: “value1”, “key2”: “value2” },……} 可以看到，请求头的Content-Type设置为application/json，并将s这个json串提交到服务端中。 2.7.3 以multipart形式发送post请求Requests也支持以multipart形式发送post请求，只需将一文件传给requests.post()的files参数即可。1234url = 'http://httpbin.org/post'files = &#123;'file': open('report.txt', 'rb')&#125;r = requests.post(url, files=files)print r.text 输出： { “args”: {}, “data”: “”, “files”: { “file”: “Hello world!” }, “form”: {}, “headers”: {…… “Content-Type”: “multipart/form-data; boundary=467e443f4c3d403c8559e2ebd009bf4a”,…… }, “json”: null,……} 文本文件report.txt的内容只有一行：Hello world!，从请求的响应结果可以看到数据已上传到服务端中。 2.8 Cookie设置使用requests，可以轻松获取响应的cookies，和设置请求的cookies。 2.8.1 获取响应的cookiesr.cookies是响应cookies的字典，通过r.cookies可访问响应带上的cookies。12r = requests.get(url)print r.cookies['example_cookie_name'] 2.8.2 发送带cookies的请求1234url = 'http://httpbin.org/cookies'cookies = &#123;'cookies_are': 'working'&#125;r = requests.get(url, cookies=cookies)print r.text 输出： { “cookies”: { “cookies_are”: “working” }} 2.9 请求的超时设置Requests允许对一个http请求设置超时的时间，只需要在requests.get()或者requests.post()方法的timeout参数设置一个值（单位为秒）即可。12url = 'http://httpbin.org/get'r = requests.get(url, timeout=0.001) 将会抛出一个超时异常： raise ConnectTimeout(e, request=request)requests.exceptions.ConnectTimeout: HTTPConnectionPool(host=’httpbin.org’, port=80): Max retries exceeded with url: /get (Caused by ConnectTimeoutError(, ‘Connection to httpbin.org timed out. (connect timeout=0.001)’)) 2.10 异常在发送http请求时，由于各种原因，requests可能会请求失败而抛出异常。常见的异常包括： ConnectionError由于网络原因，无法建立连接。 HTTPError如果响应的状态码不为200，Response.raise_for_status()会抛出HTTPError 异常。 Timeout超时异常。 TooManyRedirects若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。 所有requests抛出的异常都继承自 requests.exceptions.RequestException类。 三、小结从以上介绍可以看到，requests在处理httpy请求和响应时可谓简单而直观，这也印证了requests的开发哲学： Beautiful is better than ugly.(美丽优于丑陋) Explicit is better than implicit.(清楚优于含糊) Simple is better than complex.(简单优于复杂) Complex is better than complicated.(复杂优于繁琐) Readability counts.(重要的是可读性) 共勉。 四、参考资料 http://docs.python-requests.org/en/master/ http://docs.python-requests.org/en/latest/user/quickstart/ http://docs.python-requests.org/en/latest/user/install/#install http://docs.python-guide.org/en/latest/starting/installation/ http://stackoverflow.com/questions/5725430/http-test-server-that-accepts-get-post-calls http://www.tutorialspoint.com/http/http_responses.htm http://www.aikaiyuan.com/6324.html http://cn.python-requests.org/zh_CN/latest/user/quickstart.html http://www.pythonforbeginners.com/requests/using-requests-in-python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡（Load Balancing）学习笔记三——负载均衡算法]]></title>
    <url>%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%EF%BC%88Load-Balancing%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89%E2%80%94%E2%80%94%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文讲述实现负载均衡的常用算法。 轮询法（Round Robin）轮询法是负载均衡中最常用的算法，它容易理解也容易实现。轮询法是指负载均衡服务器（load balancer）将客户端请求按顺序轮流分配到后端服务器上，以达到负载均衡的目的。假设现在有6个客户端请求，2台后端服务器。当第一个请求到达负载均衡服务器时，负载均衡服务器会将这个请求分派到后端服务器1；当第二个请求到害时，负载均衡服务器会将这个请求分派到后端服务器2。然后第三个请求到达，由于只有两台后端服务器，故请求3会被分派到后端服务器1。依次类推，其示意图如图1所示。 图1：轮询法负载均衡 加权轮询法（Weighted Round Robin）简单的轮询法并不考虑后端机器的性能和负载差异。给性能高、负载低的机器配置较高的权重，让其处理较多的请求；而性能低、负载高的机器，配置较低的权重，让其处理较少的请求。加权轮询法可以很好地处理这一问题，它将请求顺序且按照权重分派到后端服务器。假设有6个客户端请求，2台后端服务器。后端服务器1被赋予权值5，后端服务器2被赋予赋予权值1。这样一来，客户端请求1，2，3，4，5都被分派到服务器1处理；客户端请求6被分派到服务器2处理。接下来，请求7，8，9，10，11被分派到服务器1，请求12被分派到服务器2，依次类推。这个请求分派的过程可以用图2来表示。 图2：加权轮询法负载均衡 最小连接数法（Least Connections）即使后端机器的性能和负载一样，不同客户端请求复杂度不一样导致处理时间也不一样。最小连接数法根据后端服务器当前的连接数情况，动态地选取其中积压连接数最小的一台服务器来处理当前的请求，尽可能提高后端服务器的利用效率，合理地将请求分流到每一台服务器。为什么根据连接数可以合理地利用服务器处理请求呢？考虑一个客户端请求的处理逻辑较复杂，需要服务器的处理时间较长，由于客户端需要等待服务器的响应，故需要保持与服务器的连接，这样一来，客户端就需要与服务器保持较长时间的连接。假设客户端请求1，2，3，4，5已被分派给服务器1和服务器2，其分派的情况如图3所示： 图3：最小连接法示意图（1） 此时，服务器上的请求1，请求3已处理完毕，与客户端的连接已断开。而请求2，4，5还在服务器上处理，即服务器还保持与这些请求的客户端的连接。如果再把请求分派到服务器2，则会导致服务器的请求更多，服务器2的负载更高。如果考虑服务器的连接数，当前服务器1的连接数为1，服务器2的连接数为2，将请求分派到服务器1，则负载相对均衡。采用最小连接数法的分派方法如图4所示： 图4：最小连接法示意图（2） 随机法（Random）随机法也很简单，就是随机选择一台后端服务器进行请求的处理。由于每次服务器被挑中的概率都一样，客户端的请求可以被均匀地分派到所有的后端服务器上。 源地址哈希法（Source Hashing）源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。如果后端服务器是一缓存系统，当后端服务器增加或者减少时，采用简单的哈希取模的方法，会使得命中率大大降低，这个问题可以采用一致性哈希的方法来解决。关于一致性哈希的介绍，可以参考文件 一致性Hash(Consistent Hashing)原理剖析。 参考资料 http://www.jscape.com/blog/load-balancing-algorithms 大型网站技术架构——核心原理与安全分析，李智慧著，电子工业出版社 http://www.cnblogs.com/SmartLee/p/5161415.html http://blog.csdn.net/lihao21/article/details/54193868]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡（Load Balancing）学习笔记(二)]]></title>
    <url>%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%EF%BC%88Load-Balancing%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[概述文章负载均衡（Load Balancing）学习笔记(一) 讲述了负载均衡的一般性原理，本文继续介绍常见的实现负载均衡的方法。 HTTP重定向HTTP重定向服务器是一台普通的Web服务器，用户的请求先到达重定向服务器，这台服务器会挑选一台后端服务器的地址（例如使用轮询的方式），并将该地址写入HTTP重定向响应结果中（以响应状态码302返回）返回给用户。用户将根据这个新的地址重新发送请求到选中的服务器上。选中的服务器会处理用户请求，并将结果返回给用户。HTTP重定向的处理流程如图1所示。图1：HTTP重定向实现负载均衡 通过重定向服务器的处理，用户请求被分配到不同的后端服务器上进行处理，实现了负载均衡的目的。 优点HTTP重定向负载均衡的方法实现上比较简单。 缺点 增加了用户的时延，因为访问请求需要进行两次往返 重定向服务器没有将后端服务器的负载差异考虑进去，有些后端服务器可能在相当繁忙时仍然接收到较多的请求 重定向服务器的并发处理能力制约着整个系统的并发处理能力 如果重定向服务器出现故障，站点就会瘫痪 由于存在这些缺点，HTTP重定向通常都会与其他一种或多种负载均衡技术结合使用。 DNS负载均衡DNS负载均衡的实现原理是在DNS服务器中为同一个主机名配置多个IP地址，在应答DNS查询时，DNS服务器对于每个查询将以DNS记录的IP地址按顺序返回不同的解析结果，将客户端的访问引导到不同的机器上去，使得不同客户端访问不同的服务器，从而达到负载均衡的目的。 图2：DNS实现负载均衡 优点 由于DNS系统本身是一个分布式系统，相对来说它不存在性能和吞吐能力的限制，故使用DNS来做负载均衡并不需要担心负载均衡服务器的处理能力 可以根据用户IP进行智能解析，DNS服务器可以在所有可用的A记录中寻找离用户最近的一台服务器为用户提供服务 ##缺点 DNS服务器并不知道后端服务器的情况，如果后端服务器宕机，而用户的请求继续被分配到这个后端服务器，会导致无法响应用户的请求 DNS服务器依然没有将后端服务器的负载差异考虑进去 DNS服务器存在缓存，当需要更新请求的分配时，新增一个IP或者删除一个IP并不能立即生效 反向代理反向代理服务器的工作主要是转发HTTP请求，因此它工作在HTTP层，也就是OSI七层结构中的应用层（第七层），所以基于反向代理的负载均衡也称为七层负载均衡。利用反向代理服务器进行负载均衡，如图3所示。 图3：反向代理实现负载均衡 反向代理服务处于后端服务器的前面，由于需要直接受用户的请求，故反向代理服务器需要一个外网IP。而后端服务器并不直接对外提供访问，因此后端服务器并不需要外网IP。反向代理服务器通过内网IP与后端服务器进行通信。图3中，浏览器的请求到达反向代理服务器114.113.200.84，反向代理服务器收到请求后，根据负载均衡算法计算得到一台真实的后端服务器地址10.0.0.1，并将请求转发到这台后端服务器。10.0.0.1处理请求后将响应返回给反向代理服务器，再由反向代理服务器将该响应返回给用户。常见的反向代理服务器包括Nginx，Apache，Haproxy等。 优点 部署比较简单，使用常见的反向代理服务器软件即可部署反向代理服务器 调度策略丰富，例如，可以为不同的后端机器设置不同的分配权重， 这样处理能力高的机器可以分配到较多的任务，达到能者多劳的目的 反向代理服务器可以让用户在一次会话周期内的所有请求始终分配到一台特定的后端服务器（粘滞会话） 缺点由于反向代理工作在HTTP层，所有请求都需要经过反向代理服务器的处理，后端服务器的响应结果也必须经过反向代理服务器传送给用户，故这种负载均衡方式要求反向代理的并发处理能力较高。 IP负载均衡我们已了解前面几种负载均衡的方法，这些负载均衡都是工作在HTTP层，那么，能否在HTTP层以下来实现负载均衡呢？答案是肯定的。事实上，在数据链路层（第二层），网络层（第三层），以及传输层（第四层）都可以实现不同机制的负载均衡。但与HTTP层机制不同，这些负载均衡调度的工作必须由Linux内核来完成，即网络数据包在从内核缓冲区进入用户进程空间前，已完成转发的工作。在网络层通过修改请求目的地址进行负载均衡的流程如图4所示。 图4：IP负载均衡 用户请求到达负载均衡服务器114.113.200.84，负载均衡服务器在操作系统内核获取网络数据包，根据负载均衡算法计算得到一台真实的后端服务器10.0.0.1，然后将数据包的目地址改为10.0.0.1，不需要用户进程处理。后端服务器10.0.0.1处理完成后，响应数据包返回到负载均衡服务器，负载均衡服务器再将数据包源地址修改为自身的IP地址（114.113.200.84）发送给用户。 优点IP负载均衡在内核进程完成数据分发，处理性能得到了很大的提高。 缺点由于所有请求和响应都要经过负载均衡服务器，系统的最大吞吐量仍然受到负载均衡服务器网卡带宽的限制。对于提供下载服务或者视频服务等需要传输大量数据站点，IP负载均衡的方式是难以满足需求的。 数据链路层负载均衡数据链路层负载均衡通过修改数据帧的MAC地址来实现负载均衡的目的。数据链路层是OSI网络模型的第二层，由于数据链路层负载均衡的方法走的是MAC层的协议，因此需要负载均衡服务器和后端服务器处在同一个二层（同一个广播域）之中。数据链路层负载均衡的工作流程如图所示： 图5：数据链路层实现负载均衡 图5所示的数据传输方式又称作三角传输模式，负载均衡数据分发过程中不修改IP址，只修改MAC地址，通过配置后端服务器与负载均衡服务器具有相同的IP地址，从而达到不修改数据包的源IP地址和目的IP地址就可以进行数据分发的目的。由于后端服务器的IP和数据请求目的IP一致，不需要通过负载均衡服务器进行地址转换，可以将响应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。图5中，用户请求到达负载均衡服务器114.113.200.84，负载均衡服务器将请求数据的目的MAC地址必为00.0c.29.d2，并不修改数据包目的IP地址，由于后端服务器与负载均衡服务器并有相同的IP地址，因此数据可以正常传输到达00.0c.29.d2对应的服务器，该服务器处理完成后将响应数据直接返回给用户，不需要经过负载均衡服务器。 在Linux平台上最好的数据链路层负载均衡的开源产品是LVS（Linux Virtual Server）。LVS有三种运行模式，分别为NAT模式，TUN模式，DR模式。DR模式正是运行在数据链路层，通过修改数据帧的MAC地址来实现负载均衡的。 优点 数据链路层负载均衡工作在Linux内核进程，性能很高 后端服务器的响应不需要再次经过负载均衡服务器，解决了负载均衡服务器网卡流量瓶颈的问题 缺点可配置性较高，并不支持复杂的调度策略。 参考资料 大型网站技术架构——核心原理与安全分析，李智慧著，电子工业出版社 构建高性能Web站点，郭欣著，中国工信出版集团，电子工业出版社 HTTP权威指南，David Gourley等著，人民邮电出版社 https://segmentfault.com/a/1190000002578457 http://baike.baidu.com/item/DNS负载均衡 http://blog.csdn.net/asqi1/article/details/41478111]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>LVS</tag>
        <tag>重定向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Redis SETNX 命令实现分布式锁]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Redis-SETNX-%E5%91%BD%E4%BB%A4%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[使用Redis的 SETNX 命令可以实现分布式锁，下文介绍其实现方法。 SETNX命令简介命令格式 SETNX key value 将 key 的值设为 value，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是SET if Not eXists的简写。 返回值返回整数，具体为 1，当 key 的值被设置 0，当 key 的值没被设置 例子 redis&gt; SETNX mykey “hello”(integer) 1redis&gt; SETNX mykey “hello”(integer) 0redis&gt; GET mykey“hello”redis&gt; 使用SETNX实现分布式锁多个进程执行以下Redis命令： SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt; 如果 SETNX 返回1，说明该进程获得锁，SETNX将键 lock.foo 的值设置为锁的超时时间（当前时间 + 锁的有效时间）。如果 SETNX 返回0，说明其他进程已经获得了锁，进程不能进入临界区。进程可以在一个循环中不断地尝试 SETNX 操作，以获得锁。 解决死锁考虑一种情况，如果进程获得锁后，断开了与 Redis 的连接（可能是进程挂掉，或者网络中断），如果没有有效的释放锁的机制，那么其他进程都会处于一直等待的状态，即出现“死锁”。 上面在使用 SETNX 获得锁时，我们将键 lock.foo 的值设置为锁的有效时间，进程获得锁后，其他进程还会不断的检测锁是否已超时，如果超时，那么等待的进程也将有机会获得锁。 然而，锁超时时，我们不能简单地使用 DEL 命令删除键 lock.foo 以释放锁。考虑以下情况，进程P1已经首先获得了锁 lock.foo，然后进程P1挂掉了。进程P2，P3正在不断地检测锁是否已释放或者已超时，执行流程如下： P2和P3进程读取键 lock.foo 的值，检测锁是否已超时（通过比较当前时间和键 lock.foo 的值来判断是否超时） P2和P3进程发现锁 lock.foo 已超时 P2执行 DEL lock.foo命令 P2执行 SETNX lock.foo命令，并返回1，即P2获得锁 P3执行 DEL lock.foo命令将P2刚刚设置的键 lock.foo 删除（这步是由于P3刚才已检测到锁已超时） P3执行 SETNX lock.foo命令，并返回1，即P3获得锁 P2和P3同时获得了锁 从上面的情况可以得知，在检测到锁超时后，进程不能直接简单地执行 DEL 删除键的操作以获得锁。 为了解决上述算法可能出现的多个进程同时获得锁的问题，我们再来看以下的算法。我们同样假设进程P1已经首先获得了锁 lock.foo，然后进程P1挂掉了。接下来的情况： 进程P4执行 SETNX lock.foo 以尝试获取锁 由于进程P1已获得了锁，所以P4执行 SETNX lock.foo 返回0，即获取锁失败 P4执行 GET lock.foo 来检测锁是否已超时，如果没超时，则等待一段时间，再次检测 如果P4检测到锁已超时，即当前的时间大于键 lock.foo 的值，P4会执行以下操作GETSET lock.foo &lt;current Unix timestamp + lock timeout + 1&gt; 由于 GETSET 操作在设置键的值的同时，还会返回键的旧值，通过比较键 lock.foo 的旧值是否小于当前时间，可以判断进程是否已获得锁 假如另一个进程P5也检测到锁已超时，并在P4之前执行了 GETSET 操作，那么P4的 GETSET 操作返回的是一个大于当前时间的时间戳，这样P4就不会获得锁而继续等待。注意到，即使P4接下来将键 lock.foo 的值设置了比P5设置的更大的值也没影响。 另外，值得注意的是，在进程释放锁，即执行 DEL lock.foo 操作前，需要先判断锁是否已超时。如果锁已超时，那么锁可能已由其他进程获得，这时直接执行 DEL lock.foo 操作会导致把其他进程已获得的锁释放掉。 程序代码用以下python代码来实现上述的使用 SETNX 命令作分布式锁的算法。 12345678910111213141516171819202122LOCK_TIMEOUT = 3lock = 0lock_timeout = 0lock_key = 'lock.foo'# 获取锁while lock != 1: now = int(time.time()) lock_timeout = now + LOCK_TIMEOUT + 1 lock = redis_client.setnx(lock_key, lock_timeout) if lock == 1 or (now &gt; int(redis_client.get(lock_key))) and now &gt; int(redis_client.getset(lock_key, lock_timeout)): break else: time.sleep(0.001)# 已获得锁do_job()# 释放锁now = int(time.time())if now &lt; lock_timeout: redis_client.delete(lock_key) 参考资料 http://redis.io/commands/setnx http://redis.io/topics/distlock http://redis.io/commands/getset http://redis.readthedocs.org/en/latest/string/setnx.html http://my.oschina.net/u/1995545/blog/366381]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++虚函数表剖析]]></title>
    <url>%2FC-%E8%99%9A%E5%87%BD%E6%95%B0%E8%A1%A8%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[关键词：虚函数，虚表，虚表指针，动态绑定，多态 一、概述为了实现C++的多态，C++使用了一种动态绑定的技术。这个技术的核心是虚函数表（下文简称虚表）。本文介绍虚函数表是如何实现动态绑定的。 二、类的虚表每个包含了虚函数的类都包含一个虚表。我们知道，当一个类（A）继承另一个类（B）时，类A会继承类B的函数的调用权。所以如果一个基类包含了虚函数，那么其继承类也可调用这些虚函数，换句话说，一个类继承了包含虚函数的基类，那么这个类也拥有自己的虚表。 我们来看以下的代码。类A包含虚函数vfunc1，vfunc2，由于类A包含虚函数，故类A拥有一个虚表。123456789class A &#123;public: virtual void vfunc1(); virtual void vfunc2(); void func1(); void func2();private: int m_data1, m_data2;&#125;; 类A的虚表如图1所示。图1：类A的虚表示意图 虚表是一个指针数组，其元素是虚函数的指针，每个元素对应一个虚函数的函数指针。需要指出的是，普通的函数即非虚函数，其调用并不需要经过虚表，所以虚表的元素并不包括普通函数的函数指针。虚表内的条目，即虚函数指针的赋值发生在编译器的编译阶段，也就是说在代码的编译阶段，虚表就可以构造出来了。 三、虚表指针虚表是属于类的，而不是属于某个具体的对象，一个类只需要一个虚表即可。同一个类的所有对象都使用同一个虚表。为了指定对象的虚表，对象内部包含一个虚表的指针，来指向自己所使用的虚表。为了让每个包含虚表的类的对象都拥有一个虚表指针，编译器在类中添加了一个指针，*__vptr，用来指向虚表。这样，当类的对象在创建时便拥有了这个指针，且这个指针的值会自动被设置为指向类的虚表。 图2：对象与它的虚表 上面指出，一个继承类的基类如果包含虚函数，那个这个继承类也有拥有自己的虚表，故这个继承类的对象也包含一个虚表指针，用来指向它的虚表。 四、动态绑定说到这里，大家一定会好奇C++是如何利用虚表和虚表指针来实现动态绑定的。我们先看下面的代码。12345678910111213141516171819202122232425class A &#123;public: virtual void vfunc1(); virtual void vfunc2(); void func1(); void func2();private: int m_data1, m_data2;&#125;;class B : public A &#123;public: virtual void vfunc1(); void func1();private: int m_data3;&#125;;class C: public B &#123;public: virtual void vfunc2(); void func2();private: int m_data1, m_data4;&#125;; 类A是基类，类B继承类A，类C又继承类B。类A，类B，类C，其对象模型如下图3所示。 图3：类A，类B，类C的对象模型 由于这三个类都有虚函数，故编译器为每个类都创建了一个虚表，即类A的虚表（A vtbl），类B的虚表（B vtbl），类C的虚表（C vtbl）。类A，类B，类C的对象都拥有一个虚表指针，*__vptr，用来指向自己所属类的虚表。类A包括两个虚函数，故A vtbl包含两个指针，分别指向A::vfunc1()和A::vfunc2()。类B继承于类A，故类B可以调用类A的函数，但由于类B重写了B::vfunc1()函数，故B vtbl的两个指针分别指向B::vfunc1()和A::vfunc2()。类C继承于类B，故类C可以调用类B的函数，但由于类C重写了C::vfunc2()函数，故C vtbl的两个指针分别指向B::vfunc1()（指向继承的最近的一个类的函数）和C::vfunc2()。虽然图3看起来有点复杂，但是只要抓住“对象的虚表指针用来指向自己所属类的虚表，虚表中的指针会指向其继承的最近的一个类的虚函数”这个特点，便可以快速将这几个类的对象模型在自己的脑海中描绘出来。 非虚函数的调用不用经过虚表，故不需要虚表中的指针指向这些函数。 假设我们定义一个类B的对象bObject。由于bObject是类B的一个对象，故bObject包含一个虚表指针，指向类B的虚表。1234int main() &#123; B bObject;&#125; 现在，我们声明一个类A的指针p来指向对象bObject。虽然p是基类的指针只能指向基类的部分，但是虚表指针亦属于基类部分，所以p可以访问到对象bObject的虚表指针。bObject的虚表指针指向类B的虚表，所以p可以访问到B vtbl。如图3所示。12345int main() &#123; B bObject; A *p = &amp; bObject;&#125; 当我们使用p来调用vfunc1()函数时，会发生什么现象？123456int main() &#123; B bObject; A *p = &amp; bObject; p-&gt;vfunc1();&#125; 程序在执行p-&gt;vfunc1()时，会发现p是个指针，且调用的函数是虚函数，接下来便会进行以下的步骤。首先，根据虚表指针p-&gt;__vptr来访问对象bObject对应的虚表。虽然指针p是基类A*类型，但是*__vptr也是基类的一部分，所以可以通过p-&gt;__vptr可以访问到对象对应的虚表。然后，在虚表中查找所调用的函数对应的条目。由于虚表在编译阶段就可以构造出来了，所以可以根据所调用的函数定位到虚表中的对应条目。对于p-&gt;vfunc1()的调用，B vtbl的第一项即是vfunc1对应的条目。最后，根据虚表中找到的函数指针，调用函数。从图3可以看到，B vtbl的第一项指向B::vfunc1()，所以p-&gt;vfunc1()实质会调用B::vfunc1()函数。 如果p指向类A的对象，情况又是怎么样？123456int main() &#123; A aObject; A *p = &amp;aObject; p-&gt;vfunc1();&#125; 当aObject在创建时，它的虚表指针__vptr已设置为指向A vtbl，这样p-&gt;__vptr就指向A vtbl。vfunc1在A vtbl对应在条目指向了A::vfunc1()函数，所以p-&gt;vfunc1()实质会调用A::vfunc1()函数。 可以把以上三个调用函数的步骤用以下表达式来表示： 1(*(p-&gt;__vptr)[n])(p) 可以看到，通过使用这些虚函数表，即使使用的是基类的指针来调用函数，也可以达到正确调用运行中实际对象的虚函数。我们把经过虚表调用虚函数的过程称为动态绑定，其表现出来的现象称为运行时多态。动态绑定区别于传统的函数调用，传统的函数调用我们称之为静态绑定，即函数的调用在编译阶段就可以确定下来了。 那么，什么时候会执行函数的动态绑定？这需要符合以下三个条件。 通过指针来调用函数 指针upcast向上转型（继承类向基类的转换称为upcast，关于什么是upcast，可以参考本文的参考资料） 调用的是虚函数 如果一个函数调用符合以上三个条件，编译器就会把该函数调用编译成动态绑定，其函数的调用过程走的是上述通过虚表的机制。 五、总结封装，继承，多态是面向对象设计的三个特征，而多态可以说是面向对象设计的关键。C++通过虚函数表，实现了虚函数与对象的动态绑定，从而构建了C++面向对象程序设计的基石。 参考资料 《C++ Primer》第三版，中文版，潘爱民等译 http://www.learncpp.com/cpp-tutorial/125-the-virtual-table/ 侯捷《C++最佳编程实践》视频，极客班，2015 Upcasting and Downcasting, http://www.bogotobogo.com/cplusplus/upcasting_downcasting.php 附录 示例代码]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>虚函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡（Load Balancing）学习笔记(一)]]></title>
    <url>%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%EF%BC%88Load-Balancing%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[概述在分布式系统中，负载均衡（Load Balancing）是一种将任务分派到多个服务端进程的方法。例如，将一个HTTP请求派发到实际的Web服务器中执行的过程就涉及负载均衡的实现。一个HTTP请求到达Web服务器，这中间涉及多个过程，也存在多种不同负载均衡的方法。本文讲述负载均衡的基本原理与派发策略，下图1是负载均衡的基本原理图，图1中客户端的请求请求经过达负载均衡器（Load Balancer）的分派，被指定的服务器进程进行处理。图1：负载均衡基本原理 实现负载均衡主要有两个目的。第一个目的是将任务的处理负载均摊到不同的进程，以减少单一进程的负载，以达到处理能力水平扩容的目的。第二个目的则是提高容错能力。我们知道，在线上正式环境中，机器宕机或者进程异常导致服务不可用是常有的现象。在实现负载均衡的系统中，多个服务器进程提供同样的服务，一个进程不可用的情况下，任务会被负载均衡器派发到其他可用的进程，以达到高可用的目的。在多台不同的服务器中部署相同的服务进程，通过负载均衡对外提供服务，这组进程也称为“集群”（cluster）。 负载均衡实现策略常见的负载均衡实现策略有以下几种： 均匀派发（Even Task Distribution Scheme） 加权派发（Weighted Task Distribution Scheme） 粘滞会话（Sticky Session Scheme） 均匀任务队列派发（Even Size Task Queue Distribution Scheme） 单一队列（Autonomous Queue Scheme） 均匀派发（Even Task Distribution Scheme）均匀派发是实现负载均衡最简单的策略，均衡派发的意思是指任务将均匀地派发到所有的服务器进程。在实现时，可以使用随机派发或者轮流派发（Round Robin）。 图2：均匀派发策略 均匀派发策略假设集群内所有进程具有相同的处理能力，且任务处理用时相同。但实际上，由于进程部署环境的不同，其处理能力一般不同，任务处理时间也不尽相同。因此均匀派发的策略并不能很好地将任务负载均滩到各个进程中。 DNS负载均衡我们知道，DNS提供域名解析服务，当我们访问某个站点时，实际上首先需要通过该站点域名的DNS服务器来获取指向该域名的IP地址，在这过程中，DNS服务器完成了域名到IP地址的映射。由于这一映射可以是一对多的关系，因此DNS服务器可以充当负载均衡器的作用，DNS服务器在派发IP地址时，正是使用轮流派发的方式来实现的。 加权派发（Weighted Task Distribution Scheme）加权派发策略在派发任务时，会赋予服务器进程一个权值，即不同的进程会接受不同数量的任务，具体数量为权值确定。 图3：加权派发策略 例如，三个进程的处理任务的能力比率为3:3:2，那么可以赋予这三个进程3:3:2的权值，即每8个任务中，3个发派给第一个进程，3个发派给第二个进程，2个分派给第三个进程。加权派发策略考虑了进程处理能力的不同，因此更接近实际的应用。可是，加权派发策略也没有考虑任务处理的要求。 粘滞会话（Sticky Session Scheme）前面两种负载均衡策略并没有考虑任务之间的依赖关系，在实际中，后面的任务处理常常会依赖于前面的任务。例如，对于同一个登录的用户的请求，用户购买的请求依赖于用户登录的请求，如果用户的登录信息保存在进程1中，那么，如果购买请求被分派到进程2或者进程3，那么购买请求将不能正确处理。这种请求间的依赖关系也称为粘滞会话（Sticky Session），负载均衡策略需要考虑粘滞会话的情况。 图4：粘滞会话策略 粘滞会话的派发策略要求属于同一个会话的任务将会被分派到同一个进程中。虽然这可以正确处理任务，但是却带来任务派发不均匀的问题，因为一些会话可能包含更多的任务，一些会话包含更少的任务。粘滞会话的另一种处理策略是使用数据库或者缓存，将所有会话数据存储到数据库或者缓存中。集群内所有进程都可以通过访问数据库或者缓存来获取会话数据，进程内存都不保存会话数据，这样，负载均衡器便可以使用前面介绍的策略来派发任务。 均匀任务队列派发（Even Size Task Queue Distribution Scheme）均匀任务队列派发策略跟加权派发策略类似，都考虑了进程的处理能力，不过其实现方式不同。在均匀队列派发策略下，负载均衡器为每个进程都创建一个大小相等的任务队列，这些任务队列包含了对应进程需要处理的任务。任务处理快的进程，其队列也会减少得快，这样负载均衡器会派发更多的任务给这个进程；相应地，任务处理慢的进程，其队列也会减少得慢，这样负载均衡器会派发更少的任务给这个进程。因此，通过这些任务队列，负载均衡器在派发任务时将进程处理任务的能力因素考虑了进去。 图5：均匀任务队列派发策略 单一队列（Autonomous Queue Scheme）与上面的均匀队列策略一样，单一队列策略也使用了队列来实现负载均衡。不同的是，单一队列策略只使用了一个队列。图6是单一队列策略的原理图。 图6：单一队列策略 单一队列策略中，实际上并没有负载均衡器的存在。所有的服务器进程从队列中取出任务执行，如果某个进程出现宕机的情况，那么其他进程仍然可以继续执行任务。这样一来，任务队列并不需要知道服务进程的情况，只需要服务进程知道自己的任务队列，并不断执行任务即可。单一队列策略实际上也考虑到进程的处理能力，进程处理任务得越快，其从队列取出任务的速度也越快。 总结由于负载均衡为系统提供了水平扩展的能力以及提高了系统的高可用性，因此，负载均衡在分布式系统中的作用可谓十分重要。在实际使用中，我们可以充分利用一些已有的负载均衡硬件或者软件为我们实现负载均衡。硬件方面有F5，A10，软件方面有Nginx，HAProxy，LVS等。即使是自己实现，也可以考虑现有的开源软件，比如任务队列可以使用RabbitMQ，等。与其重复造轮子，不如先站在巨人的肩膀上:) 参考资料 http://tutorials.jenkov.com/software-architecture/load-balancing.html http://www.oschina.net/news/77156/load-balance http://nginx.org/en/docs/http/load_balancing.html 构建高性能Web站点，第十二章，郭欣著，2012年6月，第二版]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性Hash(Consistent Hashing)原理剖析]]></title>
    <url>%2F%E4%B8%80%E8%87%B4%E6%80%A7Hash-Consistent-Hashing-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[引入在业务开发中，我们常把数据持久化到数据库中。如果需要读取这些数据，除了直接从数据库中读取外，为了减轻数据库的访问压力以及提高访问速度，我们更多地引入缓存来对数据进行存取。读取数据的过程一般为： 图1：加入缓存的数据读取过程 对于分布式缓存，不同机器上存储不同对象的数据。为了实现这些缓存机器的负载均衡，可以使用式子1来定位对象缓存的存储机器： m = hash(o) mod n ——式子1 其中，o为对象的名称，n为机器的数量，m为机器的编号，hash为一hash函数。图2中的负载均衡器（load balancer）正是使用式子1来将客户端对不同对象的请求分派到不同的机器上执行，例如，对于对象o，经过式子1的计算，得到m的值为3，那么所有对对象o的读取和存储的请求都被发往机器3执行。 图2：如何利用Hash取模实现负载均衡 式子1在大部分时候都可以工作得很好，然而，当机器需要扩容或者机器出现宕机的情况下，事情就比较棘手了。 当机器扩容，需要增加一台缓存机器时，负载均衡器使用的式子变成： m = hash(o) mod (n + 1) ——式子2 当机器宕机，机器数量减少一台时，负载均衡器使用的式子变成： m = hash(o) mod (n - 1) ——式子3 我们以机器扩容的情况为例，说明简单的取模方法会导致什么问题。假设机器由3台变成4台，对象o1由式子1计算得到的m值为2，由式子2计算得到的m值却可能为0，1，2，3（一个 3t + 2的整数对4取模，其值可能为0，1，2，3，读者可以自行验证），大约有75%（3/4)的可能性出现缓存访问不命中的现象。随着机器集群规模的扩大，这个比例线性上升。当99台机器再加入1台机器时，不命中的概率是99%（99/100）。这样的结果显然是不能接受的，因为这会导致数据库访问的压力陡增，严重情况，还可能导致数据库宕机。 一致性hash算法正是为了解决此类问题的方法，它可以保证当机器增加或者减少时，对缓存访问命中的概率影响减至很小。下面我们来详细说一下一致性hash算法的具体过程。 一致性Hash环一致性hash算法通过一个叫作一致性hash环的数据结构实现。这个环的起点是0，终点是2^32 - 1，并且起点与终点连接，环的中间的整数按逆时针分布，故这个环的整数分布范围是[0, 2^32-1]，如下图3所示： 图3：一致性Hash环 将对象放置到Hash环假设现在我们有4个对象，分别为o1，o2，o3，o4，使用hash函数计算这4个对象的hash值（范围为0 ~ 2^32-1）: hash(o1) = m1hash(o2) = m2hash(o3) = m3hash(o4) = m4 把m1，m2，m3，m4这4个值放置到hash环上，得到如下图4： 图4：放置了对象的一致性Hash环 将机器放置到Hash环使用同样的hash函数，我们将机器也放置到hash环上。假设我们有三台缓存机器，分别为 c1，c2，c3，使用hash函数计算这3台机器的hash值： hash(c1) = t1hash(c2) = t2hash(c3) = t3 把t1，t2，t3 这3个值放置到hash环上，得到如下图5： 图5：放置了机器的一致性Hash环 为对象选择机器将对象和机器都放置到同一个hash环后，在hash环上顺时针查找距离这个对象的hash值最近的机器，即是这个对象所属的机器。例如，对于对象o2，顺序针找到最近的机器是c1，故机器c1会缓存对象o2。而机器c2则缓存o3，o4，机器c3则缓存对象o1。 图6：在一致性Hash环上为对象选择机器 处理机器增减的情况对于线上的业务，增加或者减少一台机器的部署是常有的事情。例如，增加机器c4的部署并将机器c4加入到hash环的机器c3与c2之间。这时，只有机器c3与c4之间的对象需要重新分配新的机器。对于我们的例子，只有对象o4被重新分配到了c4，其他对象仍在原有机器上。如图7所示： 图7：增加机器后的一致性Hash环的结构 如上文前面所述，使用简单的求模方法，当新添加机器后会导致大部分缓存失效的情况，使用一致性hash算法后这种情况则会得到大大的改善。前面提到3台机器变成4台机器后，缓存命中率只有25%（不命中率75%）。而使用一致性hash算法，理想情况下缓存命中率则有75%，而且，随着机器规模的增加，命中率会进一步提高，99台机器增加一台后，命中率达到99%，这大大减轻了增加缓存机器带来的数据库访问的压力。 再例如，将机器c1下线（当然，也有可能是机器c1宕机），这时，只有原有被分配到机器c1对象需要被重新分配到新的机器。对于我们的例子，只有对象o2被重新分配到机器c3，其他对象仍在原有机器上。如图8所示： 图8：减少机器后的一致性Hash环的结构 虚拟节点上面提到的过程基本上就是一致性hash的基本原理了，不过还有一个小小的问题。新加入的机器c4只分担了机器c2的负载，机器c1与c3的负载并没有因为机器c4的加入而减少负载压力。如果4台机器的性能是一样的，那么这种结果并不是我们想要的。为此，我们引入虚拟节点来解决负载不均衡的问题。将每台物理机器虚拟为一组虚拟机器，将虚拟机器放置到hash环上，如果需要确定对象的机器，先确定对象的虚拟机器，再由虚拟机器确定物理机器。说得有点复杂，其实过程也很简单。 还是使用上面的例子，假如开始时存在缓存机器c1，c2，c3，对于每个缓存机器，都有3个虚拟节点对应，其一致性hash环结构如图9所示： 图9：机器c1，c2，c3的一致性Hash环结构 假设对于对象o1，其对应的虚拟节点为c11，而虚拟节点c11对象缓存机器c1，故对象o1被分配到机器c1中。 新加入缓存机器c4，其对应的虚拟节点为c41，c42，c43，将这三个虚拟节点添加到hash环中，得到的hash环结构如图10所示： 图10：机器c1，c2，c3，c4的一致性Hash环结构 新加入的缓存机器c4对应一组虚拟节点c41，c42，c43，加入到hash环后，影响的虚拟节点包括c31，c22，c11（顺时针查找到第一个节点），而这3个虚拟节点分别对应机器c3，c2，c1。即新加入的一台机器，同时影响到原有的3台机器。理想情况下，新加入的机器平等地分担了原有机器的负载，这正是虚拟节点带来的好处。而且新加入机器c4后，只影响25%（1/4）对象分配，也就是说，命中率仍然有75%，这跟没有使用虚拟节点的一致性hash算法得到的结果是相同的。 总结一致性hash算法解决了分布式环境下机器增加或者减少时，简单的取模运算无法获取较高命中率的问题。通过虚拟节点的使用，一致性hash算法可以均匀分担机器的负载，使得这一算法更具现实的意义。正因如此，一致性hash算法被广泛应用于分布式系统中。 参考资料 https://en.wikipedia.org/wiki/Consistent_hashing https://www.codeproject.com/articles/56138/consistent-hashing 《大型网站技术架构——核心原理与安全分析》，李智慧著，电子工业出版社]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>一致性哈希</tag>
        <tag>Consistent Hashing</tag>
      </tags>
  </entry>
</search>